[
    {
        "id": 1,
        "question": "What is a database in the context of software systems?",
        "answer": "A database is an organized, persistent collection of logically related data designed to meet the information needs of an organization. It represents some aspect of the real world (the miniworld) and is built to be shared by multiple users and applications. Unlike a simple file system, a database minimizes redundancy and allows for efficient querying and data management. For example, a social media platform's database stores user profiles, posts, comments, and connections in separate but interrelated tables, enabling complex features like news feeds and search."
    },
    {
        "id": 2,
        "question": "What is a DBMS and what are its primary functions?",
        "answer": "A Database Management System (DBMS) is a software suite that provides a systematic and scalable way to create, retrieve, update, and manage data in a database. It acts as an intermediary between the database and end-users/application programs. Its core functions include: data storage and retrieval, concurrency control to manage simultaneous access, security and authorization, integrity enforcement to ensure data accuracy, and backup and recovery. Examples include MySQL, PostgreSQL, Oracle, and MongoDB."
    },
    {
        "id": 3,
        "question": "How does a Database System differ from just a database?",
        "answer": "The term 'Database System' or 'Database Management System (DBMS)' refers to the entire software application, which includes both the database itself (the stored data) and the DBMS software that manipulates it. It's the complete ecosystem: the data, the hardware it resides on, the software to manage it, and the personnel who use and administer it. You can't have a functional database system without the DBMS software."
    },
    {
        "id": 4,
        "question": "What are the key advantages of using a DBMS over traditional file systems?",
        "answer": "DBMS offers significant advantages over file processing systems: 1. **Reduced Data Redundancy:** Data is stored in one place, minimizing duplication. 2. **Improved Data Integrity:** Rules and constraints ensure data is accurate and consistent. 3. **Enhanced Data Sharing and Security:** Multiple users can access data concurrently with controlled permissions. 4. **Data Independence:** Application programs are insulated from changes in how the data is stored. 5. **Powerful Data Retrieval:** SQL provides a efficient, standardized language for complex queries. 6. **Backup and Recovery:** Built-in mechanisms protect data from system failures."
    },
    {
        "id": 5,
        "question": "What were the main limitations of traditional file processing systems that led to the DBMS?",
        "answer": "File processing systems suffered from several critical flaws: 1. **Data Redundancy and Inconsistency:** The same data was often stored in multiple files, leading to wasted space and potential inconsistencies. 2. **Difficulty in Accessing Data:** Writing new queries to extract specific data was often complex and required deep knowledge of the file structure. 3. **Data Isolation:** Data was scattered across various files in different formats, making it hard to get a unified view. 4. **Integrity Problems:** It was hard to apply and enforce business rules (e.g., 'account balance > 0') across the entire system. 5. **Atomicity Problems:** Difficulty ensuring operations like bank transfers completed entirely or not at all. 6. **Concurrent Access Anomalies:** Uncontrolled simultaneous access could lead to incorrect data updates."
    },
    {
        "id": 6,
        "question": "Can you describe the three-schema architecture for data abstraction in DBMS?",
        "answer": "The three-schema architecture defines different levels of abstraction to achieve data independence: 1. **Internal Level (Physical Schema):** Describes *how* the data is physically stored on the storage device (files, indices, storage structures). 2. **Conceptual Level (Logical Schema):** Describes *what* data is stored and the relationships between them. It defines the structure of the entire database for the community of users (entities, attributes, relationships, constraints). 3. **External Level (View Schema):** Describes the data as it is seen by specific end-user groups or applications. It is a tailored subset of the conceptual schema, hiding irrelevant data."
    },
    {
        "id": 7,
        "question": "What are the two primary integrity rules in the relational model?",
        "answer": "The relational model is governed by two key integrity rules: 1. **Entity Integrity:** This rule states that no attribute that is part of the primary key in a base relation (table) can have a NULL value. This ensures every row can be uniquely identified. 2. **Referential Integrity:** This rule states that if a relation has a foreign key, then every value of that foreign key must either be NULL or must match the value of the primary key in the relation it references. This maintains consistency between related tables."
    },
    {
        "id": 8,
        "question": "What is the difference between the intension and extension of a database?",
        "answer": "This distinction separates the database's blueprint from its current state: *   **Intension (Database Schema):** This is the constant, logical design of the database. It's the overall structure, including table definitions, attributes, data types, constraints, and relationships. It changes infrequently, only when the design is modified. *   **Extension (Database Instance):** This is the dynamic, time-dependent collection of data stored in the database at a particular moment. It's the set of all tuples (rows) in all the tables. The extension changes every time data is inserted, updated, or deleted."
    },
    {
        "id": 9,
        "question": "What was System R and why was it historically significant?",
        "answer": "System R was a groundbreaking research project at IBM's San Jose Research Laboratory in the 1970s. It was one of the first systems to demonstrate that a relational database management system could be implemented with practical performance. Its two major subsystems were: 1. **Research Storage (RS):** Managed the low-level storage of data on disk. 2. **System Relational Data System (RDS):** Provided the higher-level relational interface, including the pioneering SQL query language (then called SEQUEL). System R proved the viability of relational databases and directly influenced the development of commercial RDBMS like IBM's DB2 and SQL/DS."
    },
    {
        "id": 10,
        "question": "How did the data model of System R differ from the pure relational model defined by Codd?",
        "answer": "As a pioneering prototype, System R did not implement the full relational model as originally defined: 1. **No Domain Support:** It did not support the concept of domains (distinct data types with constraints), using simple data types instead. 2. **Optional Uniqueness:** Enforcement of candidate key uniqueness was not mandatory. 3. **Optional Entity Integrity:** The rule that primary keys cannot be null was not strictly enforced. 4. **No Referential Integrity:** The system did not automatically enforce foreign key constraints. These simplifications were practical choices for the initial implementation but were addressed in later commercial systems."
    },
    {
        "id": 11,
        "question": "What is meant by data independence in a DBMS?",
        "answer": "Data independence is the immunity of application programs to changes in the definition and organization of data. It is a fundamental benefit of the three-schema architecture. There are two types: 1. **Physical Data Independence:** The ability to modify the physical schema (storage structures, indexing) without needing to change the logical or conceptual schema. Applications are shielded from changes like switching from one storage engine to another. 2. **Logical Data Independence:** The ability to modify the conceptual schema (adding a new column, splitting a table) without having to change existing external schemas (views) or application programs, as long as the data they use remains available."
    },
    {
        "id": 12,
        "question": "What is a database view and how does it promote data independence?",
        "answer": "A view is a virtual table derived from one or more base tables. It does not store data itself but represents a stored query. Views are crucial for logical data independence because they create an abstraction layer. Users and applications can interact with views instead of base tables. If the underlying base tables need to be restructured (e.g., a table is split), the view's definition can be updated to reconstruct the original virtual table for the user. This means the application's interface to the data remains unchanged, insulating it from changes in the database's logical structure."
    },
    {
        "id": 13,
        "question": "What is a data model?",
        "answer": "A data model is a collection of conceptual tools used to describe the structure of a database. It provides a framework for defining data elements, their relationships, the semantics (meaning) of the data, and the consistency constraints that apply to the data. Think of it as a blueprint. Common data models include the Relational Model (tables), Entity-Relationship Model (entities and relationships), and Document Model (JSON-like documents)."
    },
    {
        "id": 14,
        "question": "How does the Entity-Relationship (E-R) Model represent data?",
        "answer": "The Entity-Relationship Model is a high-level, conceptual data model based on perceiving the real world as a set of basic objects called **entities** and the **relationships** among these objects. *   **Entities:** Represent real-world objects (e.g., a `Student`, a `Course`, an `Employee`). *   **Attributes:** Properties that describe an entity (e.g., `StudentID`, `CourseName`, `Salary`). *   **Relationships:** Associations between entities (e.g., a Student *is enrolled in* a Course). It is primarily used for database design to visually map out requirements before implementation."
    },
    {
        "id": 15,
        "question": "What are the core concepts of the Object-Oriented Data Model?",
        "answer": "The Object-Oriented Data Model represents data as **objects**, similar to those in object-oriented programming. Its core concepts are: 1. **Objects:** Contain both data (in **instance variables** or attributes) and behavior (**methods** or functions that operate on the data). 2. **Classes:** A collection of similar objects that share the same attributes and methods. A class is a blueprint for creating objects. 3. **Inheritance:** A class (subclass) can inherit attributes and methods from a more general class (superclass), promoting code reusability. This model is useful for representing complex data with rich behavior, like in CAD or multimedia systems."
    },
    {
        "id": 16,
        "question": "In database design, what is an entity?",
        "answer": "An entity is a distinct, identifiable object or concept in the real world that we want to store information about. It has an independent existence and can be uniquely identified. Entities are represented as tables in a relational database. *   **Example:** In a hospital management system, `PATIENT`, `DOCTOR`, `APPOINTMENT`, and `MEDICATION` could all be entities."
    },
    {
        "id": 17,
        "question": "What defines an entity type?",
        "answer": "An entity type defines a category or a blueprint for a set of entities that share the same properties or attributes. It is the structure or schema. *   **Example:** The entity type `CAR` defines the structure for all car entities. Its attributes could be `VIN`, `Make`, `Model`, `Year`, and `Color`. Every individual car (e.g., the specific car with VIN '123ABC') is an instance of the `CAR` entity type."
    },
    {
        "id": 18,
        "question": "How is an entity set different from an entity type?",
        "answer": "This is the critical distinction between a *definition* and a *collection*: *   **Entity Type:** The *definition* or category (e.g., `STUDENT`). *   **Entity Set:** The actual *collection* of all entities of a particular type that exist in the database at a given time (e.g., the set of all student rows in the STUDENT table: {Alice, Bob, Charlie...}). The entity set is the extension of the entity type."
    },
    {
        "id": 19,
        "question": "What is the extension of an entity type?",
        "answer": "The extension of an entity type is simply another name for the **entity set**—the collection of all entities of that specific type present in the database at a specific moment in time. It is dynamic and changes as data is added or removed. If `PRODUCT` is the entity type, the extension is the complete, current list of all products in the database."
    },
    {
        "id": 20,
        "question": "What characterizes a weak entity set?",
        "answer": "A weak entity set is an entity set that does not have a sufficient set of attributes to form a primary key on its own. Its existence is dependent on another entity, called the **owner** or **identifying** entity. It must relate to the owner entity via a **identifying relationship**. The primary key of a weak entity is formed by combining its **partial key** (a discriminator) with the primary key of its owner entity. *   **Example:** An `DEPENDENT` entity (e.g., a child) for an `EMPLOYEE`. `DependentName` might be a partial key, but the full primary key for `DEPENDENT` would be {`EmployeeID` (from EMPLOYEE), `DependentName`}."
    },
    {
        "id": 21,
        "question": "What is an attribute in the context of a database entity?",
        "answer": "An attribute is a defining property or characteristic of an entity or a relationship. It describes the details we want to store about an entity. Each attribute has a name and a domain, which defines the set of possible values it can hold (e.g., INTEGER, VARCHAR(100), DATE). *   **Example:** For an entity type `PRODUCT`, possible attributes include `ProductID`, `Name`, `Description`, `Price`, and `StockQuantity`."
    },
    {
        "id": 22,
        "question": "Can you explain the difference between a relation schema and a relation instance?",
        "answer": "This is the structure vs. content distinction for a table: *   **Relation Schema:** This is the table's structure or blueprint. It is defined by the relation name and a fixed set of attributes. It is static and rarely changes. For example: `Student(StudentID, Name, Major, GPA)`. *   **Relation Instance:** This is the actual set of data (the rows or tuples) that populates the table at a given moment. It is dynamic and changes frequently with data manipulation operations (INSERT, UPDATE, DELETE). The instance is the current 'state' of the relation."
    },
    {
        "id": 23,
        "question": "What is meant by the 'degree' of a relation?",
        "answer": "The degree of a relation is the number of attributes (columns) in its relation schema. It's a measure of how 'wide' the table is. *   **Example:** A relation schema `Customer(CustomerID, FirstName, LastName, Email)` has a degree of 4. A relation schema `LogEntry(Timestamp, Message)` has a degree of 2."
    },
    {
        "id": 24,
        "question": "In data modeling, what is a relationship?",
        "answer": "A relationship is an association between two or more entities. It represents a business rule or a interaction between the entities. Relationships are crucial for connecting data across different tables. *   **Example:** In a library database, there is a relationship between the `Member` entity and the `Book` entity. This relationship, which we might call `BORROWS`, records which member has borrowed which book."
    },
    {
        "id": 25,
        "question": "What is a relationship set?",
        "answer": "A relationship set is a collection of relationships of the same type. It groups together all the individual instances of a specific relationship that exist in the database. *   **Example:** If the relationship is `WORKS_FOR` between `Employee` and `Department`, then the relationship set is the complete list of all (Employee, Department) pairs that currently exist, such as {(Alice, Sales), (Bob, Engineering), (Charlie, Sales)}."
    },
    {
        "id": 26,
        "question": "How is a relationship type defined?",
        "answer": "A relationship type defines the nature of the association between entity types. It is the schema for a relationship. It specifies the entity types that participate in the relationship and the name of the relationship itself. *   **Example:** The relationship type `Enrollment` defines an association between the `Student` entity type and the `Course` entity type. It establishes that students can be enrolled in courses."
    },
    {
        "id": 27,
        "question": "What is the degree of a relationship type?",
        "answer": "The degree of a relationship type is the number of entity types that participate in the relationship. *   **Binary Relationship:** Degree 2. This is the most common type (e.g., `Employee` WORKS_FOR `Department`). *   **Ternary Relationship:** Degree 3 (e.g., a `Doctor` prescribes a `Medication` to a `Patient`). *   **N-ary Relationship:** Degree n, for relationships involving more than two entity types."
    },
    {
        "id": 28,
        "question": "What is the purpose of the Data Definition Language (DDL)?",
        "answer": "Data Definition Language (DDL) is a subset of SQL used to define, modify, and delete the structure of database objects, but not the data within them. It is used by database designers and administrators to create the database's schema. Common DDL commands are `CREATE` (to create tables, indexes, views), `ALTER` (to modify existing structures), and `DROP` (to delete objects). DDL statements implicitly commit transactions."
    },
    {
        "id": 29,
        "question": "What role does a View Definition Language (VDL) play?",
        "answer": "View Definition Language (VDL) is the component of a DBMS used to specify user views and their mappings to the conceptual schema. In modern SQL-based systems, this functionality is integrated into the standard DDL using the `CREATE VIEW` statement. This statement allows the definition of a virtual table (a view) as a query on other base tables or views, effectively customizing how different users perceive the database."
    },
    {
        "id": 30,
        "question": "What is the function of the Storage Definition Language (SDL)?",
        "answer": "Storage Definition Language (SDL) is used to specify the internal schema of the database. It defines how the data is physically stored on the storage device. This includes specifying file organizations, indexing techniques, storage media, and the mapping between the conceptual and internal levels. SDL is typically used by database administrators to tune the database for performance and is often hidden from regular users and application developers."
    },
    {
        "id": 31,
        "question": "How does Data Storage-Definition Language relate to physical storage?",
        "answer": "Data Storage-Definition Language is a specific type of DDL focused exclusively on the physical layer. It is used to define the low-level storage structures and access methods that the database system will use. This includes commands to create table spaces, data files, and to control physical properties like page size, buffer pools, and encryption at rest. It provides the crucial link between the logical data model and its physical implementation on disk."
    },
    {
        "id": 32,
        "question": "What are the two main types of Data Manipulation Language (DML)?",
        "answer": "DML is used for managing data within database objects. The two main types are: 1. **Procedural DML (Low-Level):** Requires the user to specify *what* data is needed and *how* to get it. The user must express the logic to navigate the database (e.g., using loops and pointers). Relational Algebra is a procedural language. 2. **Non-Procedural DML (High-Level/Declarative):** Requires the user to specify *only what* data is needed, without describing how to retrieve it. The DBMS's query optimizer determines the most efficient execution path. SQL's `SELECT`, `INSERT`, `UPDATE`, `DELETE` are non-procedural."
    },
    {
        "id": 33,
        "question": "What is the role of a DML Compiler?",
        "answer": "The DML Compiler is a crucial component of the DBMS query processing engine. Its primary role is to translate high-level, non-procedural DML statements (like SQL queries) into a sequence of low-level instructions (often called a query plan or access plan). These low-level instructions are in a form that the query evaluation engine can understand and execute efficiently against the physical storage system."
    },
    {
        "id": 34,
        "question": "What does the Query Evaluation Engine do?",
        "answer": "The Query Evaluation Engine is the component that executes the low-level instructions generated by the DML compiler. It is responsible for carrying out the actual operations required to fulfill a data request. This includes reading data from storage, performing operations like sorting and joining, applying filters, and returning the final result set to the user or application. It interacts directly with the storage manager."
    },
    {
        "id": 35,
        "question": "What is the function of a DDL Interpreter?",
        "answer": "The DDL Interpreter processes Data Definition Language (DDL) statements. It parses DDL commands (like `CREATE TABLE`, `ALTER VIEW`) and executes them. Its key function is to record the definitions of database objects (their metadata) in the system catalog or data dictionary. This metadata includes table schemas, constraints, indexes, and privileges, which is essential for the DBMS to understand and manage the database structure."
    },
    {
        "id": 36,
        "question": "What is a Record-at-a-time DML operation?",
        "answer": "Record-at-a-time DML is a low-level, procedural style of data manipulation where operations are performed on a single record (row) at a time. The application code must typically open a cursor, loop through a set of records, and process each one individually. This approach gives the programmer fine-grained control but is more complex and less efficient for set-based operations. It is characteristic of navigational database models and older programming interfaces."
    },
    {
        "id": 37,
        "question": "What is a Set-at-a-time DML operation?",
        "answer": "Set-at-a-time DML is a high-level, declarative style of data manipulation where operations are performed on entire sets of records simultaneously. A single DML statement (like a SQL `UPDATE` or `DELETE` with a `WHERE` clause) can identify and modify multiple rows in one go. This is a fundamental strength of the relational model, leading to more concise code and allowing the DBMS to optimize execution for high performance."
    },
    {
        "id": 38,
        "question": "What is Relational Algebra?",
        "answer": "Relational Algebra is a formal, procedural query language for the relational model. It provides a set of operations that take one or two relations (tables) as input and produce a new relation as output. Its operations form the theoretical foundation for SQL. Core operations include **Select (σ)** to filter rows, **Project (π)** to select columns, **Union (∪)**, **Set Difference (-)**, **Cartesian Product (×)**, and various **Join** operations. It specifies *how* a query should be executed."
    },
    {
        "id": 39,
        "question": "What is Relational Calculus?",
        "answer": "Relational Calculus is a formal, non-procedural query language for the relational model. It is based on first-order predicate logic. Instead of specifying *how* to retrieve data, it describes *what* data is desired by stating the desired properties of the result set. There are two variants: Tuple Relational Calculus and Domain Relational Calculus. SQL's `SELECT...WHERE` clause is heavily influenced by relational calculus, as you declare the conditions the result must satisfy."
    },
    {
        "id": 40,
        "question": "What is the key difference between Tuple and Domain Relational Calculus?",
        "answer": "The key difference lies in what the variables in the formulas represent: *   **Tuple Relational Calculus (TRC):** Variables range over tuples (rows) from relations. A query specifies the tuples we want based on a condition involving their attribute values. It is closer to how SQL is written. *   **Domain Relational Calculus (DRC):** Variables range over values from the domains of attributes. A query specifies the constraints on the values we want to appear in the result. It operates at a more atomic level than TRC."
    },
    {
        "id": 41,
        "question": "What is database normalization and what are its goals?",
        "answer": "Normalization is a systematic process of decomposing (breaking down) complex database tables into smaller, simpler tables to eliminate data redundancy and avoid data anomalies. The primary goals are: 1. **Minimize Data Redundancy:** Store each piece of data only once to save space and prevent update anomalies. 2. **Eliminate Anomalies:** Prevent inconsistencies that can occur during data insertion, deletion, and updating. 3. **Simplify Data Integrity:** Make it easier to enforce integrity constraints. The process involves analyzing tables based on their functional dependencies and primary keys."
    },
    {
        "id": 42,
        "question": "What is a Functional Dependency (FD) in a database?",
        "answer": "A Functional Dependency is a constraint between two sets of attributes in a relation. It is denoted as X → Y, meaning that the set of attributes X functionally determines the set of attributes Y. This implies that for any two tuples (rows) in the relation, if they have the same values for X, they must also have the same values for Y. X is called the determinant. Functional dependencies are derived from the real-world meaning of the data and are fundamental to the process of normalization."
    },
    {
        "id": 43,
        "question": "What is the Lossless Join (Non-Additive Join) property in decomposition?",
        "answer": "The Lossless Join property is a critical characteristic of a valid database decomposition. It guarantees that when the decomposed relations (tables) are joined back together using a natural join operation, the result is exactly the original relation—no more and no fewer tuples. A decomposition that does not have this property is considered faulty because it creates spurious (fake) tuples that were not in the original data, leading to incorrect information."
    },
    {
        "id": 44,
        "question": "What defines the First Normal Form (1NF)?",
        "answer": "A relation is in First Normal Form (1NF) if and only if every attribute (column) contains only atomic (indivisible) values. This means: 1. **No Multi-Valued Attributes:** Each cell must contain a single value, not a list or set of values. 2. **No Composite Attributes:** Attributes should not be broken down into smaller sub-parts within the same column. 3. **A Fixed Set of Columns:** All rows must have the same number of columns. 1NF is the most basic requirement for a relational table."
    },
    {
        "id": 45,
        "question": "What is a Full Functional Dependency?",
        "answer": "A functional dependency X → Y is a **full functional dependency** if the removal of any attribute A from the determinant X means that the dependency no longer holds. In other words, Y is functionally dependent on the entire key X, and not on any proper subset of X. This concept is central to the definition of Second Normal Form (2NF). *   **Example:** In {StudentID, CourseID} → Grade, if Grade depends on both the student and the course (i.e., you can't determine the grade with just StudentID or just CourseID), then it is a full functional dependency."
    },
    {
        "id": 46,
        "question": "What are the conditions for a relation to be in Second Normal Form (2NF)?",
        "answer": "A relation is in Second Normal Form (2NF) if it meets two criteria: 1. It is already in First Normal Form (1NF). 2. **No Partial Dependency:** Every non-prime attribute (an attribute not part of any candidate key) must be fully functionally dependent on the entire primary key. This means no non-prime attribute should be dependent on only a part of a composite primary key. Relations with a single-column primary key are automatically in 2NF if they are in 1NF."
    },
    {
        "id": 47,
        "question": "What are the conditions for a relation to be in Third Normal Form (3NF)?",
        "answer": "A relation is in Third Normal Form (3NF) if it meets two criteria: 1. It is in Second Normal Form (2NF). 2. **No Transitive Dependency:** No non-prime attribute is transitively dependent on the primary key. Transitive dependency occurs when a non-prime attribute depends on another non-prime attribute, which in turn depends on the primary key (e.g., PK → A → B). In 3NF, non-prime attributes must depend directly on the primary key. A common definition states that for every functional dependency X → A, either X is a superkey or A is a prime attribute."
    },
    {
        "id": 48,
        "question": "What is Boyce-Codd Normal Form (BCNF) and how is it stronger than 3NF?",
        "answer": "Boyce-Codd Normal Form (BCNF) is a stronger version of 3NF. A relation is in BCNF if for every non-trivial functional dependency X → Y, the determinant X must be a superkey (a superset of a candidate key). BCNF addresses rare anomalies that can remain in a 3NF relation when there are multiple overlapping candidate keys. In simpler terms, BCNF ensures that the only determinants (things on the left-hand side of a functional dependency) in the table are candidate keys. Every relation in BCNF is also in 3NF, but not vice-versa."
    },
    {
        "id": 49,
        "question": "What is Fourth Normal Form (4NF) and what problem does it solve?",
        "answer": "Fourth Normal Form (4NF) deals with dependencies beyond functional dependencies, specifically Multi-Valued Dependencies (MVDs). A relation is in 4NF if it is in BCNF and for every non-trivial multi-valued dependency X →→ Y, X must be a superkey. A multi-valued dependency exists when for a single value of X, there is a set of values for Y, and this set is independent of other attributes. 4NF eliminates redundancy caused by independent multi-valued facts about an entity. For example, it separates a table storing employees, their children, and their skills into two separate tables."
    },
    {
        "id": 50,
        "question": "What is Fifth Normal Form (5NF) or Project-Join Normal Form (PJNF)?",
        "answer": "Fifth Normal Form (5NF) or Project-Join Normal Form (PJNF) is the highest level of normalization based on join dependencies. A relation is in 5NF if it is in 4NF and every join dependency in the relation is implied by its candidate keys. A join dependency means that the relation can be recreated by joining its projections (subsets of columns) without losing information. 5NF deals with very complex, subtle cases of redundancy that are unlikely to be intentionally designed into a schema. It is more of theoretical interest than practical use."
    },
    {
        "id": 51,
        "question": "What is Domain-Key Normal Form (DKNF)?",
        "answer": "Domain-Key Normal Form (DKNF) is a theoretical ideal normal form. A relation is in DKNF if every constraint on the relation is a logical consequence of the definitions of its domains and keys. Constraints include functional dependencies, multi-valued dependencies, and join dependencies. DKNF ensures that no insertion or deletion anomalies exist. Achieving DKNF is often not practical as it requires all business rules to be expressed solely through domain and key constraints. It represents a 'perfect' database design."
    },
    {
        "id": 52,
        "question": "Can you explain different types of keys used in database design?",
        "answer": "Various keys serve different purposes in uniquely identifying and linking data: 1. **Partial Key:** A set of attributes that uniquely identifies a weak entity relative to its owner entity. Also called a discriminator. 2. **Alternate Key:** A candidate key that is not chosen as the primary key. A table can have multiple alternate keys. 3. **Artificial Key (Surrogate Key):** A system-generated, meaningless numeric identifier (e.g., an auto-increment number) created solely to act as the primary key. It has no business meaning. 4. **Compound Key (Composite Key):** A primary key that consists of two or more attributes. 5. **Natural Key:** A candidate key that has business meaning and is used to identify an entity in the real world (e.g., SocialSecurityNumber, ISBN)."
    },
    {
        "id": 53,
        "question": "What is database indexing and what are common types of indexes?",
        "answer": "Indexing is a database optimization technique that creates a separate, smaller data structure (an index) to allow faster retrieval of records from a table. An index works like a book's index, providing a sorted list of values and pointers to their location in the table. Common types include: 1. **B-Tree Index:** The most common type, efficient for equality and range queries. 2. **Hash Index:** Excellent for exact-match queries but useless for ranges. 3. **Bitmap Index:** Ideal for columns with a low cardinality (few distinct values), like gender or status flags. 4. **Clustered Index:** Determines the physical order of data storage in a table. A table can have only one. 5. **Non-Clustered Index:** Creates a separate sorted structure with pointers to the data. A table can have many."
    },
    {
        "id": 54,
        "question": "What is the system catalog and what is its common name?",
        "answer": "The system catalog is a collection of special tables within a database that contain metadata, which is 'data about the data'. It stores comprehensive information about every database object, including tables, columns, data types, constraints, indexes, views, privileges, and users. This catalog is maintained automatically by the DBMS itself. It is more commonly known as the **Data Dictionary**. The DBMS constantly consults the data dictionary to parse queries, enforce constraints, and manage security."
    },
    {
        "id": 55,
        "question": "What is query optimization in a DBMS?",
        "answer": "Query optimization is the process performed by the DBMS where it analyzes a declarative query (like a SQL `SELECT` statement) and selects the most efficient execution plan (or query plan) from among many possible alternatives. The goal is to minimize the total estimated cost of executing the query, which is typically measured in terms of disk I/O, CPU usage, and memory consumption. The optimizer uses statistics about the data (e.g., table sizes, index availability) and sophisticated algorithms to choose the best way to access and join tables."
    },
    {
        "id": 56,
        "question": "What does the 'Durability' property of a transaction mean?",
        "answer": "Durability is the 'D' in the ACID properties of a transaction. It guarantees that once a transaction has been committed, its effects are permanent and will persist even in the event of a system failure (e.g., power outage, crash). The DBMS ensures durability typically by writing the transaction's changes to non-volatile storage (like a hard disk or SSD) in a transaction log *before* the commit operation is reported as successful to the user. After a crash, the DBMS uses this log to recover and restore all committed transactions."
    },
    {
        "id": 57,
        "question": "What is the difference between Atomicity and Aggregation in databases?",
        "answer": "These are two distinct concepts: *   **Atomicity:** This is the 'A' in ACID. It is a transaction property that ensures a transaction is treated as an indivisible unit of work. It must execute entirely ('all') or not at all ('nothing'). If any part of the transaction fails, the entire transaction is rolled back, leaving the database unchanged. *   **Aggregation:** This is a conceptual data modeling concept. It represents a relationship between a relationship and an entity (or between multiple relationships). It is used to model a 'has-a' relationship where one entity is composed of others, or when a relationship itself has attributes that need to be tracked."
    },
    {
        "id": 58,
        "question": "What is a Phantom Deadlock in distributed systems?",
        "answer": "A Phantom Deadlock is a false positive in deadlock detection that can occur in distributed database systems. It happens due to delays in propagating local state information (like lock ownership and wait-for graphs) across different nodes in the network. The distributed deadlock detection algorithm might incorrectly infer a cycle in the global wait-for graph based on outdated information, identifying a deadlock that does not actually exist. This can lead to the unnecessary aborting of a transaction."
    },
    {
        "id": 59,
        "question": "What is a database checkpoint and why is it important?",
        "answer": "A checkpoint is a mechanism where the DBMS periodically forces all modified buffers (dirty pages) in memory to be written to disk. It creates a consistent snapshot of the database state at a point in time. Checkpoints are crucial for recovery: during restart after a crash, the DBMS only needs to redo (replay) transactions committed after the last checkpoint and undo (roll back) transactions that were active at the time of the crash. This significantly reduces recovery time compared to processing the entire transaction log."
    },
    {
        "id": 60,
        "question": "What are the phases of transaction recovery?",
        "answer": "After a system crash, the DBMS goes through a recovery process with distinct phases: 1. **Analysis Phase:** The transaction log is scanned to identify the state of all transactions at the time of the crash. It determines which transactions need to be redone (committed) and which need to be undone (not committed). 2. **Redo Phase:** This phase reapplies all the updates of committed transactions. It starts from the oldest log record of a transaction that was not yet written to disk at the time of the crash and moves forward, ensuring all committed changes are durable. 3. **Undo Phase:** This phase rolls back (reverses) the updates of any transactions that were active but not committed at the time of the crash, restoring the database to a consistent state."
    },
    {
        "id": 61,
        "question": "What is a flat file database?",
        "answer": "A flat file database is a simple database that stores all its data in a single table, often in a plain text file (like a CSV or TSV file). Unlike relational databases, it lacks programmatic access languages (like SQL) and has no capability to establish relationships or enforce referential integrity between different files. While user-friendly for very small, simple datasets, it suffers severely from data redundancy, inconsistency, and the other limitations of file processing systems as the data grows."
    },
    {
        "id": 62,
        "question": "What is a transparent DBMS?",
        "answer": "A transparent DBMS is one that hides its physical storage details and internal implementation complexities from the users and application programs. Users interact with the database through a logical, high-level interface (like SQL) without needing to know how or where the data is physically stored, what indexes are used, or how queries are optimized. This transparency is a key benefit of the DBMS, providing physical and logical data independence."
    },
    {
        "id": 63,
        "question": "What is a database query?",
        "answer": "A query is a request for data or for manipulation of data from a database. It is a user command, written in a database query language, that instructs the DBMS to perform a specific operation. Queries can be used to retrieve, insert, update, or delete data. The most common standard is the Structured Query Language (SQL), which includes Data Manipulation Language (DML) commands like `SELECT`, `INSERT`, `UPDATE`, and `DELETE`."
    },
    {
        "id": 64,
        "question": "What is a correlated subquery in SQL?",
        "answer": "A correlated subquery is an inner subquery that is executed repeatedly, once for each row processed by the outer main query. It is 'correlated' because it references a column from the outer query within its `WHERE` clause. The result of the inner query depends on the value of the current row being evaluated in the outer query. This is different from a regular (non-correlated) subquery, which is executed only once, independently of the outer query. Correlated subqueries can be less performant than joins but are powerful for solving certain types of problems."
    },
    {
        "id": 65,
        "question": "What are the primitive operations common to all record management systems?",
        "answer": "At their most fundamental level, all record management systems, from simple flat files to complex DBMS, must support three basic primitive operations on data: 1. **Addition:** Inserting new records into the dataset. 2. **Deletion:** Removing existing records from the dataset. 3. **Modification (Update):** Changing the values within existing records. All other complex operations, like querying and reporting, are built upon these three core actions."
    },
    {
        "id": 66,
        "question": "What are the unary operations in Relational Algebra?",
        "answer": "Unary operations are those that operate on a single relation (table). The two fundamental unary operations in Relational Algebra are: 1. **Selection (σ):** This operation filters rows from a relation based on a given condition or predicate. It chooses a horizontal subset of a table. 2. **Projection (π):** This operation selects specific columns from a relation, eliminating all others. It also removes duplicate rows from the result. It chooses a vertical subset of a table."
    },
    {
        "id": 67,
        "question": "Are the results of the PRODUCT and JOIN operations the same?",
        "answer": "No, the results are fundamentally different. *   **PRODUCT (Cartesian Product):** This operation returns all possible combinations of rows from the two involved relations. If table A has 'm' rows and table B has 'n' rows, the product A × B will have m * n rows. It does not require or use any logical relationship between the tables. *   **JOIN:** This operation combines rows from two relations based on a related column (a join condition) between them. It is essentially a Cartesian Product followed by a Selection operation to filter only the meaningful combinations. The result of a join is always a subset of the Cartesian Product."
    },
    {
        "id": 68,
        "question": "What is the RDBMS Kernel?",
        "answer": "The RDBMS Kernel, often just called the kernel, is the core heart of the database management system. It is the central software component that resides in memory and handles the most critical tasks. Its functions include: parsing and optimizing SQL statements, managing memory buffers and caching, controlling transaction management (ACID properties), handling locking and concurrency control, enforcing security and authorization, and interacting directly with the data storage layer. Think of it as the database's operating system."
    },
    {
        "id": 69,
        "question": "What are the major subsystems of an RDBMS?",
        "answer": "A full-featured RDBMS is composed of several integrated subsystems that work together: 1. **Query Processor:** Handles parsing, optimization, and execution of queries. 2. **Storage Manager:** Manages disk space, data files, and data structures like indexes. 3. **Transaction Manager:** Ensures ACID properties (Atomicity, Consistency, Isolation, Durability). 4. **Buffer Manager:** Handles the transfer of data between disk and main memory. 5. **Lock Manager:** Controls concurrent access to data items. 6. **Log Manager:** Records all changes for recovery and auditing. 7. **Security and Authorization Subsystem:** Manages users, roles, and permissions. 8. **Network Communication Subsystem:** Handles communication with database clients."
    },
    {
        "id": 70,
        "question": "Which part of the RDBMS manages the data dictionary and how?",
        "answer": "The data dictionary is exclusively managed and maintained by the RDBMS kernel itself. It is stored as a set of special system tables within the database. The kernel is the only software component with the privilege to directly modify these tables. When a user issues a DDL statement (like `CREATE TABLE`), the kernel's DDL interpreter processes the command and updates the corresponding metadata entries in the data dictionary tables. All other components of the RDBMS and all user queries constantly read from the data dictionary to function correctly."
    },
    {
        "id": 71,
        "question": "What is the purpose of the information stored in the data dictionary?",
        "answer": "The data dictionary's metadata serves as the central nervous system for the DBMS. Its primary purposes are: 1. **Validation:** It validates the existence of database objects (tables, columns, users) when they are referenced in SQL statements. 2. **Access Control:** It stores security information, determining which users have what permissions on which objects. 3. **Mapping and Translation:** It provides the essential mapping between the logical schema (table and column names) and the physical schema (file locations, storage details), enabling data independence. 4. **Query Optimization:** The optimizer uses statistics and structural information from the data dictionary to choose efficient query execution plans."
    },
    {
        "id": 72,
        "question": "How do you communicate with a relational database management system?",
        "answer": "The primary and standard method to communicate with an RDBMS is through the Structured Query Language (SQL). Applications and users send SQL statements to the DBMS. These statements can be sent through: 1. **Command-Line Interfaces:** Tools like `mysql` or `psql`. 2. **Graphical User Interfaces (GUIs):** Applications like MySQL Workbench or DBeaver. 3. **Application Programming Interfaces (APIs):** Code in languages like Python, Java, or PHP uses drivers (e.g., JDBC, ODBC) to connect to the database and send SQL commands. The DBMS receives the SQL, processes it, and returns the result."
    },
    {
        "id": 73,
        "question": "How does SQL differ from conventional programming languages?",
        "answer": "SQL is a declarative language, whereas conventional languages like Java or Python are imperative (procedural). This is the key difference: *   **SQL (Declarative):** You specify *what* data you want, but not *how* to get it. You describe the desired result set, and the DBMS's query optimizer figures out the most efficient algorithm, execution plan, and steps to retrieve it. *   **Conventional Languages (Imperative):** You provide explicit, step-by-step instructions for the computer to follow to achieve a result. You control the flow and logic (loops, conditionals) in detail. SQL is also set-oriented, processing groups of records, while procedural languages typically process one record at a time."
    },
    {
        "id": 74,
        "question": "What are the three major sets of files that compose an Oracle database?",
        "answer": "An Oracle database is physically comprised of three essential file types: 1. **Datafiles:** These files store the actual database data, including tables, indexes, and other segments. All the user data and most of the system data resides here. 2. **Control Files:** A small but critical binary file that records the physical structure of the database. It contains information like the database name, timestamps, and the locations of all datafiles and redo log files. The database cannot start without it. 3. **Redo Log Files:** These files record all changes made to the database data. They are crucial for recovery, allowing the database to replay transactions in the event of a failure. They are written to in a circular fashion."
    },
    {
        "id": 75,
        "question": "What is a database trigger?",
        "answer": "A database trigger is a named program unit, written in a procedural language like PL/SQL, that is stored in the database and automatically executed ('fired') in response to a specific event on a particular table or view. The events are typically Data Manipulation Language (DML) statements: `INSERT`, `UPDATE`, or `DELETE`. Triggers can be defined to fire once per statement or once for every row affected by the statement. They are used to enforce complex business rules, audit changes, maintain derived data, and enhance security."
    },
    {
        "id": 76,
        "question": "What are stored procedures and what advantages do they offer?",
        "answer": "Stored procedures are named programs containing a sequence of SQL and procedural statements that are stored, compiled, and executed on the database server. Their advantages include: 1. **Performance:** They are pre-compiled, reducing parsing and optimization overhead. 2. **Reduced Network Traffic:** Applications can call a single procedure instead of sending multiple SQL statements. 3. **Modularity & Reusability:** Business logic is written once, stored centrally, and can be called by any application. 4. **Security:** Users can be granted permission to execute a procedure without having direct access to the underlying tables, providing a strong security layer. 5. **Maintainability:** Logic is centralized, making it easier to change and debug."
    },
    {
        "id": 77,
        "question": "What is the role of the Storage Manager in a DBMS?",
        "answer": "The Storage Manager is a crucial DBMS component that provides the interface between the low-level data stored on disk and the rest of the DBMS components (e.g., the query processor). It is responsible for: 1. **Managing Storage:** Allocating space on disk for database files. 2. **Managing Data Structures:** Implementing and managing efficient file structures for storing data (e.g., heap files, hashing) and indexes. 3. **Translating Requests:** Converting the logical requests for data (e.g., 'get row with id=5') into low-level commands to read from or write to the physical storage system."
    },
    {
        "id": 78,
        "question": "What does the Buffer Manager do?",
        "answer": "The Buffer Manager is responsible for managing the database buffer pool in main memory. Its core duties are: 1. **Data Transfer:** Fetching data pages from disk into memory buffers when needed by a query, and writing modified pages ('dirty pages') back to disk. 2. **Caching:** Deciding which pages to keep in memory to maximize the chance that future requests can be served from the much faster RAM, thus minimizing slow disk I/O. 3. **Page Replacement:** Using algorithms (like LRU - Least Recently Used) to decide which pages to evict from the buffer when space is needed for new pages."
    },
    {
        "id": 79,
        "question": "What is the function of the Transaction Manager?",
        "answer": "The Transaction Manager is the component that ensures transactions satisfy the ACID properties. Its key responsibilities are: 1. **Atomicity & Durability:** It works with the Log Manager to ensure transactions are all-or-nothing and durable upon commit. 2. **Consistency:** It ensures that a transaction takes the database from one consistent state to another. 3. **Isolation:** It works with the Lock Manager (or other concurrency control schemes) to control the interaction between concurrent transactions, preventing problems like dirty reads and lost updates. It is the central coordinator for transaction scheduling and recovery."
    },
    {
        "id": 80,
        "question": "What is the role of the File Manager?",
        "answer": "The File Manager is a lower-level component within the Storage Manager. It handles the interaction with the operating system's file system. Its role is to: 1. **File Management:** Create, delete, and allocate database files on disk. 2. **Space Management:** Manage the free space available within these files. 3. **Page Management:** Read and write fixed-size blocks of data (pages) from and to these files as requested by the Buffer Manager. It abstracts the OS file system for the rest of the DBMS."
    },
    {
        "id": 81,
        "question": "What is the Authorization and Integrity Manager?",
        "answer": "This DBMS component is responsible for enforcing rules that maintain database security and correctness: 1. **Authorization (Security):** It checks the authority of every user and program attempting to access the database. It verifies if the user has the required privileges (SELECT, INSERT, etc.) to perform the requested operation on the specified data object. 2. **Integrity (Correctness):** It tests for the satisfaction of integrity constraints (e.g., primary key uniqueness, foreign key validity, CHECK constraints) whenever data is inserted, updated, or deleted. It prevents invalid data from entering the database."
    },
    {
        "id": 82,
        "question": "What are stand-alone procedures?",
        "answer": "Stand-alone procedures are stored procedures that are not encapsulated within a package. They are independently defined database objects. While they offer the general benefits of stored procedures (like reduced network traffic), they have limitations compared to packaged procedures: they cannot be grouped logically with related functions and procedures, and in some older systems, they might have different dependency management and performance characteristics. Packaged procedures generally offer better organization, security, and performance due to their ability to be loaded into memory as a unit."
    },
    {
        "id": 83,
        "question": "What are database cursors and what types exist?",
        "answer": "A cursor is a database control structure that enables traversal over the rows in a result set. It provides a way to retrieve more than one row from a database and then process each row individually. The two main types are: 1. **Implicit Cursors:** Automatically created and managed by the DBMS for every SQL statement that returns a result set. The programmer has no direct control over them. 2. **Explicit Cursors:** Defined and controlled explicitly by the programmer within a procedural code block (like PL/SQL). They offer fine-grained control, allowing the programmer to open, fetch rows from, and close the cursor as needed for complex row-by-row processing."
    },
    {
        "id": 84,
        "question": "What is the difference between a cold backup and a hot backup?",
        "answer": "This distinction is based on the state of the database during the backup: *   **Cold Backup (Offline Backup):** Performed while the database is shut down. This ensures a perfectly consistent copy of all database files (datafiles, control files, redo logs) at a single point in time. It is simple and safe but requires database downtime, making it unsuitable for 24/7 systems. *   **Hot Backup (Online Backup):** Performed while the database is running and open for use. This allows backups without downtime. It is more complex because it requires the DBMS to put tablespaces into a special backup mode to ensure the backed-up files are consistent and can be used for recovery. This is essential for high-availability systems."
    },
    {
        "id": 85,
        "question": "What are proactive, retroactive, and simultaneous updates?",
        "answer": "These terms classify database updates based on their timing relative to real-world events: 1. **Proactive Update:** Applied to the database *before* it becomes effective in the real world. For example, scheduling a price change to take effect in the system at a future date. 2. **Retroactive Update:** Applied to the database *after* it has become effective in the real world. This is often a correction of a past error or a late entry of data. 3. **Simultaneous Update:** Applied to the database *at the same time* it becomes effective in the real world. This is the most common type of update for transactional systems (e.g., deducting payment at the moment of sale)."
    },
    {
        "id": 86,
        "question": "What is the relationship between data and information in a database context?",
        "answer": "Data are raw, unorganized facts and figures (e.g., the numbers 25, 10, 2023). Information is data that has been processed, organized, and interpreted to be useful and meaningful in a specific context (e.g., 'Total Sales for ProductX on October 10, 2023, were $25'). A database stores data in a structured way. When this data is queried, filtered, aggregated, and presented, it is transformed into valuable information that supports decision-making and knowledge."
    },
    {
        "id": 87,
        "question": "What is ERP and what type of database does it use?",
        "answer": "Enterprise Resource Planning (ERP) is a suite of integrated applications that a company uses to manage its core business processes, such as finance, supply chain, manufacturing, operations, reporting, and human resources. An ERP system relies on a central, unified database to eliminate data redundancy and provide a single source of truth across the entire organization. This database is typically a robust, multi-user **Relational Database Management System (RDBMS)** like Oracle, SQL Server, or SAP HANA, capable of handling massive transactional workloads and complex queries from various functional modules."
    },
    {
        "id": 88,
        "question": "Can you define DBMS in a comprehensive way?",
        "answer": "A Database Management System (DBMS) is a complex software system that serves as an intermediary between the database of stored data and the users or application programs that need to access it. Its primary purpose is to provide a convenient, efficient, and secure environment for defining, creating, manipulating, and controlling access to databases. It shields users from the physical storage details and provides abstract data views, while simultaneously ensuring data integrity, security, concurrency control, and reliable recovery from failures."
    },
    {
        "id": 89,
        "question": "Why is a database considered 'self-describing'?",
        "answer": "A database is called self-describing because it contains not only the users' data but also a detailed description of its own structure. This descriptive data, known as **metadata**, is stored in the system catalog or data dictionary. The metadata includes definitions of all tables, columns, data types, constraints, indexes, views, and user privileges. Because this information is stored within the database itself, the DBMS always has the necessary context to understand and manage the data it contains, without relying on external sources."
    },
    {
        "id": 90,
        "question": "Who was E.F. Codd and why is he important to database technology?",
        "answer": "Dr. Edgar F. Codd was a British computer scientist who worked for IBM. In 1970, he published a seminal paper titled 'A Relational Model of Data for Large Shared Data Banks' that introduced the **relational model** for database management. This groundbreaking work laid the theoretical foundation for relational databases. His 12 rules (Codd's Rules) defined the criteria for a system to be considered a fully relational database management system (RDBMS). His ideas are the reason SQL and modern RDBMS like Oracle, DB2, SQL Server, and MySQL exist today."
    },
    {
        "id": 91,
        "question": "What is SQL and why is it so significant?",
        "answer": "Structured Query Language (SQL) is the standard and most widely used programming language for managing and querying data in relational database management systems (RDBMS). Its significance stems from several factors: 1. **Standardization:** It is an ANSI/ISO standard, ensuring portability across different database systems (though with some vendor-specific extensions). 2. **Declarative Nature:** Users specify *what* data they want, not *how* to get it, making it powerful and relatively easy to learn. 3. **Comprehensive Functionality:** It provides a unified language for data definition (DDL), data manipulation (DML), data control (DCL), and transaction control (TCL). 4. **Ubiquity:** It is supported by all major relational databases, making it an essential skill for developers, analysts, and administrators."
    },
    {
        "id": 92,
        "question": "Write a SQL query to select all data for students with a grade of 90 or higher.",
        "answer": "```sql\nSELECT *\nFROM STUDENT\nWHERE Grade >= 90;\n```\nThis query uses the `SELECT *` statement to retrieve all columns from the `STUDENT` table. The `WHERE` clause acts as a filter, specifying that only those rows where the value in the `Grade` column is greater than or equal to 90 should be included in the result set."
    },
    {
        "id": 93,
        "question": "What are the five primary SQL aggregate functions?",
        "answer": "Aggregate functions perform a calculation on a set of values and return a single summary value. The five primary ones are: 1. **COUNT():** Returns the number of rows in a group. 2. **SUM():** Returns the sum of a numeric column. 3. **AVG():** Returns the average value of a numeric column. 4. **MAX():** Returns the maximum value in a column. 5. **MIN():** Returns the minimum value in a column. These functions are almost always used with the `GROUP BY` clause to calculate values for categories of data."
    },
    {
        "id": 94,
        "question": "Write a SQL query to count all students and display the count with a column alias.",
        "answer": "```sql\nSELECT COUNT(*) AS NumStudents\nFROM STUDENT;\n```\nThis query uses the `COUNT(*)` function to count every row in the `STUDENT` table. The `AS NumStudents` clause provides a user-friendly alias for the output column, so the result will have a column named `NumStudents` instead of the default `COUNT(*)`."
    },
    {
        "id": 95,
        "question": "What is an SQL subquery?",
        "answer": "An SQL subquery (or nested query) is a query placed within another SQL query. It is a `SELECT` statement enclosed in parentheses and embedded within a clause (most commonly the `WHERE` or `HAVING` clause) of the outer query. The result of the inner subquery is used by the outer query to complete its operation. Subqueries are powerful tools for performing complex filtering, calculations, and data retrieval that would be difficult or impossible with a single query. They can be used for comparisons using operators like `IN`, `ANY`, `ALL`, or `EXISTS`."
    },
    {
        "id": 96,
        "question": "What alternative terms are used for the components of the relational model?",
        "answer": "The formal terms of the relational model have more common equivalents: *   **Relation** is commonly called a **Table**. *   **Tuple** is commonly called a **Row** or **Record**. *   **Attribute** is commonly called a **Column** or **Field**. *   **Relation Schema** is often referred to as the **Table Definition** or **Structure**. While the formal terms are used in academic and design contexts, the common terms (table, row, column) are ubiquitous in everyday database use and SQL syntax."
    },
    {
        "id": 97,
        "question": "Why are functional dependencies not considered mathematical equations?",
        "answer": "Functional dependencies (FDs) are not equations because they represent existence and constraint, not numerical equality. An equation (like A + B = C) denotes a numerical relationship where the values on both sides are equal and calculable. A functional dependency (like Zipcode → City) denotes a deterministic relationship: if you know the value of the determinant (Zipcode), you know the value of the dependent attribute (City). It's a statement about uniqueness and constraint, not arithmetic. The value '12345' is not equal to 'Springfield', but it does determine it."
    },
    {
        "id": 98,
        "question": "What is a foreign key and what is its purpose?",
        "answer": "A foreign key is an attribute (or set of attributes) in one table that references the primary key of another table. Its purpose is to establish and enforce a link between the data in these two tables. This link is the mechanism for creating relationships in the relational model. The foreign key constraint ensures **referential integrity**, meaning that any value in the foreign key column must either be NULL or must match an existing value in the primary key of the referenced table, preventing orphaned records."
    },
    {
        "id": 99,
        "question": "What are insertion and deletion anomalies?",
        "answer": "These are problems that occur in poorly designed, unnormalized tables: *   **Insertion Anomaly:** The inability to add data about one entity without adding data about another, unrelated entity. For example, you cannot record a new department's information until at least one employee is assigned to it. *   **Deletion Anomaly:** The unintended loss of data about one entity when deleting data about another entity. For example, if you delete the only employee in a department, you might also lose all information about that department itself. Normalization aims to eliminate these anomalies by splitting data into appropriate tables."
    },
    {
        "id": 100,
        "question": "What does it mean for a relation to be in Boyce-Codd Normal Form (BCNF)?",
        "answer": "A relation is in Boyce-Codd Normal Form (BCNF) if it meets the following condition: For every non-trivial functional dependency (X → Y) in the relation, the determinant (X) must be a superkey. A superkey is any set of attributes that uniquely identifies a tuple. In simpler terms, BCNF requires that the *only* determinants in the table are candidate keys. This eliminates all redundancy due to functional dependencies. A common summary of the goal of normalization is: *Every non-key attribute must provide a fact about the key, the whole key, and nothing but the key.* BCNF enforces this very strictly."
    },
    {
        "id": 101,
        "question": "What should you look for in table data when designing a new database?",
        "answer": "When given existing data to model, you should analyze it to discover: 1. **Functional Dependencies (FDs):** What attributes determine others? (e.g., ProductID determines Price). 2. **Multi-Valued Dependencies (MVDs):** Are there independent multi-valued facts? (e.g., an employee has multiple skills and multiple certifications independently). 3. **Candidate Keys:** What set(s) of attributes can uniquely identify each row? 4. **Primary Key:** Which candidate key is best suited to be the main identifier? 5. **Foreign Keys:** What relationships exist between potential tables? This analysis directly informs the normalization process and schema design."
    },
    {
        "id": 102,
        "question": "Why does using normalized tables often lead to more complex SQL in applications?",
        "answer": "Normalization involves breaking data down into multiple smaller tables to eliminate redundancy. To answer a business question that requires data from several of these tables, application code must reassemble it. This reassembly is done in SQL using **JOIN** operations to link tables together on their primary and foreign keys, and **subqueries**. Writing these multi-table joins and correlated subqueries is inherently more complex than writing a simple `SELECT * FROM one_big_table`. The trade-off is that this complexity in querying is accepted to gain massive benefits in data integrity, non-redundancy, and update performance."
    },
    {
        "id": 103,
        "question": "What is the multivalue, multicolumn problem?",
        "answer": "This is a common design flaw where a table is created with multiple columns that store variations of the same type of attribute, violating First Normal Form. For example, a table for customer surveys might have columns like `Phone_Model_1`, `Phone_Model_2`, `Phone_Model_3` to store up to three phone models a customer owns. This design causes problems: it limits the number of values, makes querying difficult (e.g., 'find all customers who own iPhone X'), and wastes space. The correct solution is to create a separate related table with one row per phone model per customer."
    },
    {
        "id": 104,
        "question": "How is the multivalue, multicolumn problem related to multivalued dependencies?",
        "answer": "Both problems are different manifestations of the same core issue: trying to store multiple values for a single attribute within a single table. *   **Multivalue, Multicolumn Problem:** Stores the multiple values in *multiple columns* within the same row. *   **Multivalued Dependency (MVD):** Implicitly exists when the multiple values would be stored in *multiple rows* if the table were properly normalized. Both designs are incorrect. The solution for both is identical: remove the repeating groups and place them in a separate table. The multicolumn problem is essentially a pre-1NF violation, while an MVD describes a dependency in a 1NF table that needs to be resolved to achieve 4NF."
    },
    {
        "id": 105,
        "question": "What is the inconsistent values problem?",
        "answer": "The inconsistent values problem occurs when the same real-world data is recorded in different, inconsistent formats within the same database. This often happens when data comes from multiple sources or different users enter data without validation rules. For example, the same color might be entered as 'Red', 'RED', 'R', '01' (a code), or 'Scarlet'. This inconsistency makes querying and reporting unreliable and difficult. Solutions include implementing strong data validation, using check constraints, and providing users with pick-lists or dropdown menus instead of free-text fields."
    },
    {
        "id": 106,
        "question": "Explain the relationship between entity, entity class, and entity instance.",
        "answer": "These terms describe different levels of abstraction in data modeling: *   **Entity:** A general term for a 'thing' or object in the real world that can be distinctly identified (e.g., a specific customer named John Doe). *   **Entity Class (Entity Type):** A classification or category of entities that share common properties. It is the template or definition (e.g., the 'Customer' class). *   **Entity Instance:** A single, specific occurrence or example of an entity class (e.g., the database record for customer John Doe with ID 123). Analogy: 'Vehicle' is the class; your specific car with VIN 123ABC is an instance of that class."
    },
    {
        "id": 107,
        "question": "What is the difference between attributes and identifiers?",
        "answer": "Both are properties of entities, but they serve different purposes: *   **Attributes:** These are descriptive properties that characterize an entity. They describe the entity's features (e.g., for a 'Product' entity: `Color`, `Weight`, `Price`). *   **Identifiers (Keys):** These are special attributes that are used to uniquely identify an entity instance. An identifier's purpose is to ensure each instance is distinct and can be reliably referenced (e.g., for a 'Product' entity: `ProductID` or `SKU`). All identifiers are attributes, but not all attributes are identifiers."
    },
    {
        "id": 108,
        "question": "What are the three types of binary relationship cardinalities?",
        "answer": "Binary relationship cardinality defines the numerical relationship between instances of two entity classes: 1. **One-to-One (1:1):** One instance of Entity A is associated with at most one instance of Entity B, and vice versa. (e.g., a `Country` has exactly one `CapitalCity`, and a `CapitalCity` is the capital of exactly one `Country`). 2. **One-to-Many (1:N):** One instance of Entity A can be associated with many instances of Entity B, but an instance of B is associated with at most one instance of A. (e.g., one `Department` has many `Employees`, but one `Employee` works in only one `Department`). 3. **Many-to-Many (M:N):** One instance of Entity A can be associated with many instances of Entity B, and one instance of Entity B can be associated with many instances of Entity A. (e.g., one `Student` takes many `Courses`, and one `Course` is taken by many `Students`)."
    },
    {
        "id": 109,
        "question": "What is the archetype/instance pattern?",
        "answer": "The archetype/instance pattern is a common data modeling pattern used to represent a template (the archetype) and its specific occurrences (the instances). A classic example is the relationship between a `CLASS` (the archetype, e.g., 'Introduction to Database Systems') and a `SECTION` (the instance, e.g., 'Fall 2023, Section 01' which has a specific time, room, and instructor). The instance is often ID-dependent on the archetype, meaning the primary key of the `SECTION` includes the key of the `CLASS`. This pattern separates the definition of a thing from its specific manifestations."
    },
    {
        "id": 110,
        "question": "What is a recursive relationship?",
        "answer": "A recursive relationship (or unary relationship) is a relationship where an entity is related to itself. It links different instances of the same entity type. For example, in an `EMPLOYEE` entity: *   A recursive relationship `Supervises` can model who supervises whom (an employee can supervise other employees). *   A recursive relationship `Precedes` in a `TASK` entity can model task dependencies (one task must be completed before another can start). In the database table, this is implemented by adding a foreign key column that references the primary key of the same table (e.g., an `ReportsTo` column in the `EMPLOYEE` table that contains the `EmployeeID` of the manager)."
    },
    {
        "id": 111,
        "question": "What are the steps for transforming an entity into a database table?",
        "answer": "Transforming an entity from an ER diagram into a physical table involves: 1. **Define the Table Structure:** Create a table with the same name as the entity. 2. **Specify Attributes as Columns:** Create a column for each of the entity's attributes, choosing appropriate data types and lengths. 3. **Define the Primary Key:** Choose the identifier attribute to be the primary key. For weak entities, form a composite key with the owner's primary key. 4. **Specify Constraints:** Define null status (NOT NULL), default values, and check constraints for columns. 5. **Define Foreign Keys:** Based on relationships with other entities, add foreign key columns to establish links. 6. **Verify Normalization:** Ensure the table design adheres to the desired normal form to avoid anomalies."
    },
    {
        "id": 112,
        "question": "What is a surrogate key and why is it considered an ideal primary key?",
        "answer": "A surrogate key is an artificial, system-generated primary key that has no business meaning. It is typically a simple integer that auto-increments with each new record (e.g., `CustomerID`, `OrderID`). It is considered ideal for several reasons: 1. **Stability:** It never changes, unlike natural keys (e.g., an email address can change). 2. **Simplicity:** It is usually a single, numeric column, making joins and indexing very efficient. 3. **Anonymity:** It reveals no information about the entity it identifies. 4. **Uniqueness Guarantee:** The system ensures its uniqueness. While natural keys exist in the business domain, surrogate keys are often preferred for internal system efficiency and maintenance."
    },
    {
        "id": 113,
        "question": "What are data constraints and what are their types?",
        "answer": "Data constraints are rules enforced on data columns to ensure the accuracy, integrity, and reliability of the data in a database. The main types are: 1. **Domain Constraints:** Ensure a column's value is of a valid data type and falls within a defined set of values or range (e.g., `Age` must be a number between 0 and 120). 2. **Entity Integrity Constraints:** Ensure each row is uniquely identifiable (Primary Key constraint: unique and not null). 3. **Referential Integrity Constraints:** Ensure relationships between tables remain consistent (Foreign Key constraint). 4. **User-Defined Integrity Constraints (CHECK):** Enforce custom business rules that are specific to the application (e.g., `EndDate` must be after `StartDate`)."
    },
    {
        "id": 114,
        "question": "How are recursive relationships typically implemented in a database?",
        "answer": "A recursive relationship is implemented by adding a foreign key column within the same table. This column references the primary key of another row in the same table. For example, to model an employee hierarchy in an `EMPLOYEE` table: 1. The table has an `EmployeeID` primary key column. 2. It also has a `ManagerID` foreign key column. 3. The `ManagerID` column contains the `EmployeeID` of the employee who is the manager of the current employee. It references the `EmployeeID` column in the same table. If an employee has no manager (e.g., the CEO), the `ManagerID` would be NULL. Queries on such tables often use self-joins (joining the table to itself) to navigate the hierarchy."
    },
    {
        "id": 115,
        "question": "What is a cascading update?",
        "answer": "A cascading update is a referential integrity action defined on a foreign key. When the primary key value referenced by a foreign key is updated in the parent table, the DBMS automatically propagates that update to all matching foreign key values in the child table. This ensures that the relationship between the parent and child records is not broken. It is a crucial feature for maintaining data consistency when primary keys need to be changed, which is rare with stable surrogate keys but might be necessary with natural keys."
    },
    {
        "id": 116,
        "question": "What is a SQL view and what are its primary uses?",
        "answer": "A view is a virtual table whose contents are defined by a query. It does not store data itself but displays data from one or more underlying base tables. Its primary uses are: 1. **Simplification:** Hides the complexity of multi-table joins and calculations. 2. **Security:** Restricts user access to specific rows and/or columns, providing a tailored interface. 3. **Logical Data Independence:** Can provide a consistent interface to applications even if the underlying table structures change. 4. **Data Integrity:** Can enforce certain checks at the view level for users who only access data through the view."
    },
    {
        "id": 117,
        "question": "What is the 'paradigm mismatch' between SQL and application programming languages?",
        "answer": "The paradigm mismatch refers to the fundamental difference in how data is handled: *   **SQL is set-oriented:** It processes and returns entire sets of rows at a time. *   **Application Languages (e.g., Java, Python) are record-oriented (row-oriented):** They process data one object or record at a time using loops and individual variables. This mismatch means that the result of an SQL query (a set) cannot be directly processed by an application language. A mechanism like a **cursor** is required to bridge this gap, allowing the application to iterate through the result set one row at a time."
    },
    {
        "id": 118,
        "question": "What are four common applications for database triggers?",
        "answer": "Triggers are used to automatically enforce business rules and logic at the database level: 1. **Auditing and Logging:** Automatically recording changes made to sensitive data (e.g., logging every update to a `Salary` column into an `AUDIT_TRAIL` table). 2. **Enforcing Complex Constraints:** Implementing business rules that are too complex for standard CHECK or foreign key constraints (e.g., 'cannot update an order after it has been shipped'). 3. **Deriving Data:** Automatically maintaining derived or denormalized data (e.g., updating a `TotalOrderAmount` column in a `CUSTOMER` table whenever a new order is inserted). 4. **Implementing Security Authorizations:** Performing additional security checks beyond standard GRANT permissions before allowing a data modification."
    },
    {
        "id": 119,
        "question": "How do stored procedures differ from triggers?",
        "answer": "| Feature | Stored Procedure | Trigger |\n| :--- | :--- | :--- |\n| **Execution** | Explicitly called and executed by a user or application. | Automatically fired (executed) by the DBMS in response to a DML event (`INSERT`, `UPDATE`, `DELETE`). |\n| **Parameters** | Can have input and output parameters. | Cannot have explicit parameters; they use special pseudo-records (e.g., `:NEW`, `:OLD` in Oracle) to access row data. |\n| **Transaction Control** | Can contain transaction control statements like `COMMIT` or `ROLLBACK` (use with caution). | Generally cannot contain transaction control statements; they are part of the transaction that fired them. |\n| **Purpose** | Used to encapsulate and execute a defined business process. | Used to enforce business rules or maintain integrity in response to data changes."
    },
    {
        "id": 120,
        "question": "What are the advantages of using stored procedures?",
        "answer": "Stored procedures offer significant benefits for application development and performance: 1. **Enhanced Performance:** They are pre-compiled and stored in executable form, reducing parsing and optimization overhead on the database server. 2. **Reduced Network Traffic:** An application can call a single procedure instead of sending multiple SQL statements across the network. 3. **Improved Security:** Users can be granted execute permission on a procedure without having direct access to the underlying tables, providing a strong security layer. 4. **Code Reusability and Maintainability:** Business logic is stored centrally in the database, making it reusable by any application and easier to maintain and change in one place. 5. **Data Integrity:** Complex business rules can be enforced consistently within the database itself."
    },{
        "id": 121,
        "question": "Why is database redesign sometimes a necessary process?",
        "answer": "Database redesign is a critical maintenance activity driven by two primary factors: 1. **Correcting Initial Design Flaws:** Mistakes, oversights, or misunderstandings of requirements during the initial design phase are often discovered only after the system is in use. Redesign is needed to fix these issues, such as poor performance due to lack of indexes or data anomalies from insufficient normalization. 2. **Adapting to Evolving Requirements:** Businesses and their processes change over time. New features, regulations, or business models can necessitate changes to the database structure to store new data or support new types of queries. A database must evolve to remain useful and relevant."
    },
    {
        "id": 122,
        "question": "What is the key difference between a correlated subquery and a regular subquery?",
        "answer": "The key difference lies in their execution and dependency: *   **Regular (Non-Correlated) Subquery:** The inner query can be executed independently of the outer query. It is processed once, and its result is passed to the outer query. It is like a constant value within the outer query. *   **Correlated Subquery:** The inner query cannot be executed independently because it references a column from the outer query. It is executed repeatedly, once for each row candidate processed by the outer query. The result of the inner query depends on the specific value of the current row in the outer query, creating a looping mechanism."
    },
    {
        "id": 123,
        "question": "What is a dependency graph used for in database management?",
        "answer": "A dependency graph is a visual tool or diagram used to map the connections and dependencies between various elements within a database system. It helps database administrators and developers understand the potential impact of a proposed change. For example, it can show that a specific table is used by several views, application modules, stored procedures, and reports. Before altering or dropping that table, the graph makes it clear which other components will be affected and need to be reviewed, updated, or tested, thus preventing system failures after deployment."
    },
    {
        "id": 124,
        "question": "What is the correct process for adding a NOT NULL column to an existing table with data?",
        "answer": "You cannot directly add a `NOT NULL` column to a populated table because existing rows would have `NULL` for the new column, violating the constraint. The correct process is: 1. **Add the Column as NULLable:** First, add the column without the `NOT NULL` constraint. `ALTER TABLE table_name ADD column_name data_type;` 2. **Populate the Column:** Update the new column for all existing rows with a valid default value. `UPDATE table_name SET column_name = default_value;` 3. **Add the NOT NULL Constraint:** Finally, alter the column to add the `NOT NULL` constraint now that all rows have a value. `ALTER TABLE table_name ALTER COLUMN column_name SET NOT NULL;`"
    },
    {
        "id": 125,
        "question": "How do you convert a one-to-one relationship to a one-to-many relationship in a database schema?",
        "answer": "Consider two tables, `EMPLOYEE` and `COMPUTER`, in a 1:1 relationship where a computer is assigned to one employee and an employee has one computer. The `COMPUTER` table has a foreign key `EmpNumber` that must be unique. To change this to a 1:N relationship (one employee can have many computers), you simply need to **remove the uniqueness constraint** on the foreign key column (`EmpNumber`) in the `COMPUTER` table. This allows multiple rows in the `COMPUTER` table to have the same `EmpNumber` value, meaning one employee can now be associated with multiple computers. The physical structure of the tables remains the same; only the constraint changes."
    },
    {
        "id": 126,
        "question": "What is the difference between an exclusive lock and a shared lock?",
        "answer": "These locks control how multiple transactions can access the same data item concurrently: *   **Exclusive Lock (X Lock):** Grants a transaction exclusive write access to a data item. While an exclusive lock is held, no other transaction can acquire any type of lock (shared or exclusive) on the same data item. It is used for `UPDATE`, `DELETE`, and `INSERT` operations. *   **Shared Lock (S Lock):** Grants a transaction read-only access to a data item. Multiple transactions can hold shared locks on the same data item simultaneously, allowing for concurrent reads. However, no transaction can acquire an exclusive lock on the data item until all shared locks are released."
    },
    {
        "id": 127,
        "question": "What is the difference between optimistic and pessimistic concurrency control?",
        "answer": "These are two strategies for managing concurrent access to data: *   **Pessimistic Locking:** Assumes conflicts are likely. It prevents conflicts by locking data *before* a transaction begins to use it. Readers block writers and writers block readers. It is used in high-contention environments. *   **Optimistic Locking:** Assumes conflicts are rare. It allows transactions to proceed without locking. Conflicts are detected at the *end* of the transaction when a commit is attempted. If a conflict is detected (e.g., the data was changed by another transaction after it was read), the transaction is rolled back and must be restarted. It is preferred for low-contention environments like web applications, as it provides better scalability."
    },
    {
        "id": 128,
        "question": "What is a deadlock and how is it handled?",
        "answer": "A deadlock is a situation where two or more transactions are permanently blocked because each transaction holds a lock on a resource that the other transactions need to proceed. It's a cyclic wait condition (e.g., Transaction A holds Lock 1 and waits for Lock 2, while Transaction B holds Lock 2 and waits for Lock 1). *   **Prevention:** Systems can use protocols to ensure that deadlocks cannot occur (e.g., requiring transactions to acquire all locks at once). *   **Detection and Resolution:** The DBMS has a deadlock detector that periodically checks the wait-for graph for cycles. When a deadlock is found, the system resolves it by choosing a **victim transaction** and rolling it back, releasing its locks and allowing the other transaction(s) to proceed. The aborted transaction must be restarted by the application."
    },
    {
        "id": 129,
        "question": "What are the primary responsibilities of a Database Administrator (DBA)?",
        "answer": "A DBA's role is multifaceted and crucial for ensuring a database's health, performance, and security. Key responsibilities include: 1. **Database Design & Implementation:** Planning and creating new databases. 2. **Performance Tuning:** Monitoring and optimizing database performance (indexing, query optimization). 3. **Security Management:** Creating users, roles, and managing access permissions. 4. **Backup and Recovery:** Designing and testing robust strategies to prevent data loss. 5. **Availability Management:** Ensuring the database is highly available and online. 6. **Change Management:** Applying patches, upgrades, and managing schema changes. 7. **Capacity Planning:** Forecasting future storage and performance needs."
    },
    {
        "id": 130,
        "question": "What does ACID mean in the context of database transactions?",
        "answer": "ACID is an acronym that describes the four essential properties of a reliable database transaction: *   **Atomicity:** Guarantees that a transaction is treated as a single, indivisible unit of work. It either executes completely ('All') or not at all ('Nothing'). *   **Consistency:** Ensures that a transaction takes the database from one valid state to another, preserving all defined rules and constraints (e.g., foreign keys, unique keys). *   **Isolation:** Ensures that the execution of concurrent transactions leaves the database in the same state as if they were executed sequentially. Transactions are protected from intermediate states of other transactions. *   **Durability:** Guarantees that once a transaction is committed, its changes are permanent and will survive any subsequent system failure."
    },
    {
        "id": 131,
        "question": "What are the common methods for creating an Oracle database?",
        "answer": "There are three primary methods to create an Oracle database: 1. **Using the Database Configuration Assistant (DBCA):** This is a graphical user interface (GUI) tool that guides the DBA through the creation process with easy-to-follow steps. It is the simplest and most common method. 2. **Using Oracle-Managed Scripts:** This involves running prepared SQL scripts provided by Oracle. It offers more control than DBCA but is more complex. 3. **Manual Creation with the CREATE DATABASE Statement:** This is an advanced method where the DBA manually issues SQL commands to create the database. It provides the ultimate level of control but requires deep knowledge of the Oracle architecture and is error-prone."
    },
    {
        "id": 132,
        "question": "What are database sequences and what are potential issues with their use?",
        "answer": "A sequence is a database object that generates a sequence of unique integers, typically used to create artificial primary key values. **Potential Issues:** 1. **Gaps in Sequence Values:** Gaps can occur naturally due to transaction rollbacks, database crashes, or caching. This is usually not a functional problem but can be undesirable for some business requirements (e.g., invoice numbers). 2. **Misuse:** A sequence created for one table might be accidentally used for another, or a developer might insert rows without using the sequence, breaking the consistency of key generation. 3. **Lack of Enforced Relationship:** The sequence itself is independent of the table; the DBMS does not enforce that its values are actually used as the primary key, which is the application's responsibility."
    },
    {
        "id": 133,
        "question": "Under what conditions should you create a database index?",
        "answer": "Indexes should be created strategically to improve query performance. Consider creating an index when: 1. **Frequent Query Filters:** A column is commonly used in the `WHERE` clause for filtering (e.g., `WHERE email = 'x@y.com'`). 2. **Join Conditions:** A column is used frequently to join tables. 3. **Sorting and Grouping:** A column is often used in `ORDER BY` or `GROUP BY` clauses. 4. **Enforcing Uniqueness:** A unique index is required to enforce a primary key or unique constraint. **Caution:** Indexes slow down `INSERT`, `UPDATE`, and `DELETE` operations because the index must be maintained. Therefore, avoid over-indexing, especially on tables with heavy write operations."
    },
    {
        "id": 134,
        "question": "What are the common transaction isolation levels in Oracle?",
        "answer": "Oracle Database primarily supports the following isolation levels: 1. **READ COMMITTED (Default):** Guarantees that a statement will only see data that was committed before the statement began (not the transaction). It prevents dirty reads but allows non-repeatable reads and phantoms. 2. **SERIALIZABLE:** Guarantees that a transaction will see only data that was committed before the transaction began and changes made by the transaction itself. It provides the highest level of isolation, preventing dirty reads, non-repeatable reads, and phantoms. 3. **READ ONLY:** A variant that specifies the transaction cannot perform any DML operations and sees only data committed at the start of the transaction."
    },
    {
        "id": 135,
        "question": "What file types are crucial for Oracle database recovery?",
        "answer": "A successful recovery depends on these files: 1. **Datafiles:** Contain the actual data. Backups of these files are the foundation of restoration. 2. **Control Files:** Essential for mounting and opening the database. They describe the structure of the database, including the location of all datafiles and online redo log files. A backup is critical. 3. **Online Redo Log Files:** Record all changes made to the database. They are used for instance recovery after a crash. 4. **Archived Redo Log Files:** These are copies of filled online redo log files. They are absolutely vital for complete media recovery, allowing you to 'replay' all transactions up to the point of failure."
    },
    {
        "id": 136,
        "question": "What is the difference between a complete and a differential backup in SQL Server?",
        "answer": "These are two backup strategies in a recovery plan: *   **Complete Backup:** A full copy of the entire database. It backs up all data files and enough of the transaction log to allow for recovering that database. It is the foundation for any restore operation but can be large and time-consuming. *   **Differential Backup:** Backs up only the data pages that have changed since the last complete backup. It is smaller and faster to create than a full backup. To restore, you first restore the last complete backup and then restore the last differential backup. This strategy reduces the number of transaction log files needed for recovery compared to using only full backups."
    },
    {
        "id": 137,
        "question": "What are the different transaction isolation levels in SQL Server and their meanings?",
        "answer": "SQL Server defines several isolation levels: 1. **READ UNCOMMITTED:** The lowest level. Allows dirty reads, meaning a transaction may see uncommitted changes from other transactions. No shared locks are taken. 2. **READ COMMITTED (Default):** Prevents dirty reads. A transaction will only see committed data. It uses locking to hold read locks only for the duration of the statement. 3. **REPEATABLE READ:** Prevents dirty reads and non-repeatable reads. Locks are held on all data read by the transaction until it completes. 4. **SERIALIZABLE:** The highest level. Prevents dirty reads, non-repeatable reads, and phantom reads. It uses range locks to prevent other transactions from inserting new rows that would fall into the range of data read by the transaction. 5. **SNAPSHOT:** Provides a transactionally consistent view of the data as it existed at the start of the transaction, using row versioning instead of locking."
    },
    {
        "id": 138,
        "question": "What are the differences between Simple, Full, and Bulk-Logged recovery models in SQL Server?",
        "answer": "The recovery model determines how the transaction log is used and what restore options are available: *   **Simple Recovery Model:** Transaction log space is automatically reused, minimally logging most operations. Point-in-time recovery is *not* supported. You can only restore to the exact time of a full or differential backup. *   **Full Recovery Model:** All transactions are fully logged. This allows for point-in-time recovery up to the last committed transaction before a failure, using the transaction log backups. This is required for maximum data protection. *   **Bulk-Logged Recovery Model:** A special-purpose model that performs minimal logging for certain bulk operations (like BULK INSERT) to improve performance, while otherwise behaving like the Full model. It allows point-in-time recovery unless a bulk operation occurred, in which case the entire log backup containing that operation must be restored."
    },
    {
        "id": 139,
        "question": "What is the difference between a clustered and a nonclustered index in SQL Server?",
        "answer": "This is a fundamental physical storage difference: *   **Clustered Index:** Determines the physical order of data rows in the table. The data rows themselves are stored in the leaf pages of the index. Therefore, a table can have **only one** clustered index. It is typically created on the primary key. Retrieving data via a clustered index is very fast. *   **Nonclustered Index:** Creates a separate structure from the data rows. The leaf pages of a nonclustered index contain pointers to the actual data rows (either the clustered index key or a row identifier if no clustered index exists). A table can have **many** nonclustered indexes. They are used to improve query performance on columns that are not the primary key."
    },
    {
        "id": 140,
        "question": "What types of triggers does SQL Server support?",
        "answer": "SQL Server supports two main types of triggers based on their timing and purpose: 1. **AFTER Triggers (FOR Triggers):** These fire *after* the triggering DML action (`INSERT`, `UPDATE`, `DELETE`) has been processed. They are used for auditing, enforcing business rules, or creating follow-up actions. A table can have multiple AFTER triggers for each action. 2. **INSTEAD OF Triggers:** These fire *in place of* the triggering action. The original DML operation is not performed; instead, the code within the INSTEAD OF trigger is executed. They are often used to allow updates to complex views that would otherwise be non-updatable. A view or table can have at most one INSTEAD OF trigger per triggering action. SQL Server does not have built-in BEFORE triggers."
    },
    {
        "id": 141,
        "question": "How are ODBC, OLE DB, and ADO related to each other?",
        "answer": "These are Microsoft data access technologies that evolved over time: 1. **ODBC (Open Database Connectivity):** The oldest standard. It provides a C-language API specifically for accessing relational databases. 2. **OLE DB (Object Linking and Embedding for Databases):** The successor to ODBC. It is a COM-based specification that provides access to a broader range of data sources, not just relational databases (e.g., spreadsheets, text files, email). ODBC is for relational data; OLE DB is for any data. 3. **ADO (ActiveX Data Objects):** A high-level, easy-to-use API that provides an object-oriented interface to OLE DB. It simplifies data access for programmers in languages like Visual Basic and ASP by wrapping the complexity of OLE DB. The relationship is often visualized as ADO -> OLE DB -> ODBC -> Data Source."
    },
    {
        "id": 142,
        "question": "What are the three types of ODBC data sources?",
        "answer": "An ODBC data source is a stored configuration that defines how to connect to a specific database. The three types are: 1. **User DSN:** The data source information is stored in the Windows registry and is visible only to the user who created it. 2. **System DSN:** The data source information is stored in the Windows registry and is visible to all users on the machine, including Windows services. This is the most common type for server applications. 3. **File DSN:** The data source information is stored in a text file (with a .dsn extension) on the disk. This file can be shared with other users who have the same ODBC driver installed, making it portable."
    },
    {
        "id": 143,
        "question": "What disadvantage of ODBC did OLE DB aim to overcome?",
        "answer": "ODBC's primary disadvantage was its focus solely on **relational data** that could be accessed via SQL. OLE DB was designed to overcome this limitation by providing a universal data access model. Its COM-based architecture allowed it to create 'providers' for any type of data store, whether relational (e.g., SQL Server, Oracle) or non-relational (e.g., spreadsheets, email systems, directory services, text files). This enabled developers to use a more consistent programming model to access a much wider variety of data sources."
    },
    {
        "id": 144,
        "question": "What were the primary design goals of OLE DB?",
        "answer": "The major goals of OLE DB were to: 1. **Provide Universal Data Access:** Create a single, unified interface for accessing data from any source, relational or non-relational. 2. **Component Object Model (COM) Based:** Use the COM standard to define interoperable objects for data access, promoting software reusability. 3. **Increase Flexibility:** Allow data providers to expose functionality in pieces, enabling them to implement only the features they supported. 4. **Avoid Data Movement:** Provide a means to access and manipulate data in place, in its native store, without requiring it to be moved or converted into a different format first."
    },
    {
        "id": 145,
        "question": "In OLE DB, what distinguishes an interface from an implementation?",
        "answer": "This is a core concept of COM, which OLE DB is built upon: *   **Interface:** A contract. It is a defined set of properties and methods that an object must expose. It specifies *what* an object can do, but not *how* it does it. OLE DB defines standardized interfaces. *   **Implementation:** The actual code inside the object that fulfills the contract specified by the interface. It defines *how* the properties and methods work. The implementation is hidden from the user (data consumer). This separation allows a provider (implementation) to change its internal code without breaking any applications that use it, as long as it continues to honor the interface contract."
    },
    {
        "id": 146,
        "question": "Why is XML often considered a better markup language than HTML?",
        "answer": "The key difference is in their purpose: *   **HTML (HyperText Markup Language):** Designed for *presenting* and displaying data in a web browser. It uses a fixed set of tags that describe appearance (e.g., `<b>`, `<i>`). It mixes structure, content, and presentation. *   **XML (eXtensible Markup Language):** Designed for *storing and transporting* data. It is extensible—you define your own tags that describe the meaning of the data (e.g., `<price>`, `<author>`). It provides a clear separation between the structure of the data, the data itself, and its presentation. This makes XML self-describing, portable, and far superior for data exchange between heterogeneous systems."
    },
    {
        "id": 147,
        "question": "What are the two main ways to describe the structure and content of an XML document?",
        "answer": "To define the legal structure, elements, and attributes of an XML document, you use a schema language: 1. **Document Type Definition (DTD):** The older, simpler method. It defines the basic structure with a list of legal elements and attributes. An XML document that conforms to its DTD is 'valid'. 2. **XML Schema Definition (XSD):** A more powerful and modern language written in XML itself. XSD provides much stronger data typing (e.g., integers, dates), supports namespaces, and allows for more complex constraints than DTD. XSD has largely replaced DTD for most serious data exchange applications."
    },
    {
        "id": 148,
        "question": "What is the difference between simple elements and complexType elements in XML Schema?",
        "answer": "In XML Schema (XSD), this distinction defines what content an element can contain: *   **Simple Element:** An element that can contain only text (data value). It cannot contain other elements or attributes. For example: `<first_name>John</first_name>`. *   **complexType Element:** An element that can contain other elements, attributes, and text. It defines a complex structure. For example, a `<customer>` element that contains child elements like `<name>`, `<address>`, and an attribute like `id='123'` must be defined as a complexType."
    },
    {
        "id": 149,
        "question": "What is ADO.NET?",
        "answer": "ADO.NET is the primary data access technology within the Microsoft .NET framework. It is the successor to ADO (ActiveX Data Objects). ADO.NET provides a set of classes for connecting to data sources, executing commands, and retrieving results. Its key innovation is the **disconnected architecture**, centered around the `DataSet` object, which allows applications to work with a local in-memory copy of data, disconnect from the database server, and later reconnect to update the server with changes. It provides high performance and scalability for multi-tier applications."
    },
    {
        "id": 150,
        "question": "What is a DataSet in ADO.NET?",
        "answer": "A `DataSet` is an in-memory, disconnected representation of data. It is a major component of ADO.NET's disconnected architecture. Think of it as a miniature, in-memory database. It can contain: *   Multiple `DataTable` objects (like tables). *   `DataRelation` objects that define relationships between these tables (like foreign keys). *   Constraints (`UniqueConstraint`, `ForeignKeyConstraint`) to enforce data integrity. Because it is disconnected from the data source, a `DataSet` allows an application to work with data locally without maintaining a continuous database connection, which is crucial for web application scalability."
    },
    {
        "id": 151,
        "question": "What are the four JDBC driver types defined by Sun?",
        "answer": "JDBC driver types are categorized by how they implement the connection to the database: 1. **Type 1: JDBC-ODBC Bridge Driver:** Uses an ODBC driver to connect to the database. Requires ODBC to be configured on the client machine. Legacy and not recommended for production. 2. **Type 2: Native-API Driver (Partly Java Driver):** Uses the client-side libraries of the database. Converts JDBC calls into calls to the native API (e.g., OCI for Oracle). Requires native libraries on the client. 3. **Type 3: Network-Protocol Driver (Pure Java Driver for Middleware):** Uses a middleware application server that converts JDBC calls into a database-independent network protocol. The middleware then translates this to the database-specific protocol. 4. **Type 4: Database-Protocol Driver (Pure Java Driver):** Directly converts JDBC calls into the network protocol used by the DBMS. This is a direct-to-database pure Java driver. It is the most common and efficient driver type for most applications (e.g., MySQL Connector/J)."
    },
    {
        "id": 152,
        "question": "What is the difference between a Java Servlet and a Java Applet?",
        "answer": "These are two very different Java technologies: *   **Java Applet:** A small client-side application that runs *inside* a web browser on the user's machine. It is downloaded from a web server and executed by the browser's Java Virtual Machine (JVM). Applets are largely obsolete due to security concerns and lack of browser support. *   **Java Servlet:** A server-side Java program that runs *on* a web server. It extends the capabilities of the server to generate dynamic web content. Servlets receive requests from clients (e.g., web browsers), process them (often involving database access), and return responses (usually HTML). They are a fundamental part of Java web development."
    },
    {
        "id": 153,
        "question": "What is the standard pattern for using JDBC in Java code?",
        "answer": "The fundamental steps for JDBC database access are: 1. **Load and Register the Driver:** `Class.forName('com.mysql.cj.jdbc.Driver');` (Note: JDBC 4.0+ often auto-loads drivers). 2. **Establish a Connection:** `Connection conn = DriverManager.getConnection(url, user, password);` 3. **Create a Statement:** `Statement stmt = conn.createStatement();` (or `PreparedStatement` for parameterized queries). 4. **Execute the Query:** `ResultSet rs = stmt.executeQuery('SELECT * FROM table');` 5. **Process the ResultSet:** `while (rs.next()) { String data = rs.getString('column_name'); }` 6. **Close Resources:** Close the `ResultSet`, `Statement`, and `Connection` objects in a `finally` block or using try-with-resources to avoid memory leaks."
    },
    {
        "id": 154,
        "question": "What is a JavaBean?",
        "answer": "A JavaBean is a reusable software component model for Java. It is a standard class that follows specific conventions: 1. **It has a public no-argument constructor.** 2. **Its properties are accessed through 'getter' and 'setter' methods** following a naming pattern (e.g., `getPropertyName()`, `setPropertyName()`). 3. **It is serializable,** meaning its state can be saved and restored. JavaBeans are primarily used to encapsulate many objects into a single object (the bean), making them easier to work with. They are widely used in Java frameworks for applications like JSP (JavaServer Pages) to represent form data or business objects."
    },
    {
        "id": 155,
        "question": "How does MySQL handle surrogate keys and metadata?",
        "answer": "*   **Surrogate Keys:** MySQL uses the `AUTO_INCREMENT` attribute for a column (typically an `INTEGER` or `BIGINT`) to automatically generate unique surrogate key values. When inserting a new row, you omit this column, and MySQL automatically assigns the next sequential number. *   **Metadata:** MySQL stores its metadata in a special database named `mysql`. This database contains tables that store information about users, privileges, databases, tables, columns, and other system settings. For example, the `user` table stores user accounts and global privileges, while the `tables` table in the `information_schema` database provides information about all tables."
    },
    {
        "id": 156,
        "question": "What is a data mart?",
        "answer": "A data mart is a focused subset of a data warehouse that is tailored to the specific needs of a particular business unit, department, or team (e.g., marketing, sales, finance). It contains a curated collection of data designed to serve a specific group of users with a common analytical need. A data mart can be dependent (derived from an existing enterprise data warehouse) or independent (built directly from operational sources without a data warehouse). It is typically smaller, simpler, and more focused than a full data warehouse, allowing for faster implementation and more targeted analysis."
    },
    {
        "id": 157,
        "question": "What is RFM analysis?",
        "answer": "RFM analysis is a customer segmentation technique used in marketing and business intelligence to rank customers based on their past purchasing behavior. The acronym stands for: *   **Recency (R):** How recently did the customer make a purchase? (Customers who bought more recently are more likely to respond to promotions.) *   **Frequency (F):** How often do they purchase? (Frequent purchasers are more engaged.) *   **Monetary Value (M):** How much money do they spend? (High-spending customers are more valuable.) Customers are scored on each dimension (e.g., on a scale of 1-5, with 5 being best). An RFM score like '5-1-3' represents a customer who bought very recently (5), buys infrequently (1), but spends a good amount when they do (3)."
    },
    {
        "id": 158,
        "question": "What are the core functions of a BI reporting system?",
        "answer": "A Business Intelligence (BI) reporting system has three primary functional areas: 1. **Report Authoring:** Allows users to create reports by connecting to data sources, defining the report structure (queries, groupings, filters), and formatting the layout and style. 2. **Report Management:** Handles the 'what, who, when, and how' of report delivery. It involves defining schedules, user subscriptions, security permissions, and delivery channels (e.g., email, portal). 3. **Report Delivery:** Executes the management plan. It is the engine that generates the report based on the authoring definition and delivers it to the intended recipients at the scheduled time, either by 'pushing' it to them or making it available for them to 'pull' (view on demand)."
    },
    {
        "id": 159,
        "question": "What is OLAP?",
        "answer": "OLAP (Online Analytical Processing) is a category of software technology that allows users to interactively analyze multidimensional data from multiple perspectives. It is a core component of BI systems. Key characteristics include: *   **Multidimensional View:** Data is organized in cubes with dimensions (e.g., Time, Product, Location) and measures (e.g., Sales, Profit). *   **Complex Calculations:** Supports advanced operations like drilling down/up, slicing (selecting one dimension), dicing (selecting sub-cubes), and pivoting (rotating the view). *   **Fast Query Performance:** Pre-aggregates data to provide very quick answers to complex analytical queries. OLAP enables users to gain insights from historical data for strategic planning and decision support."
    },
    {
        "id": 160,
        "question": "What is market basket analysis?",
        "answer": "Market basket analysis is a data mining technique used to discover relationships between items that are frequently purchased together. It is the science behind 'customers who bought this also bought...' recommendations. The analysis is based on calculating: *   **Support:** The frequency with which an itemset appears in all transactions. (How popular is this combination?) *   **Confidence:** The probability that if item A is purchased, item B will also be purchased. (How strong is the rule A -> B?) *   **Lift:** Measures how much more likely item B is to be purchased when item A is purchased, compared to its general probability of being purchased. (Is this association real or random?) It helps retailers with strategies for product placement, cross-selling, and promotions."
    },
    {
        "id": 161,
        "question": "What is the difference between structured and unstructured data?",
        "answer": "This is a fundamental classification of data: *   **Structured Data:** Data that is organized in a predefined format, typically a tabular schema with rows and columns. It is easily stored, queried, and analyzed in relational databases. Examples include numbers, dates, and strings in database tables or CSV files. *   **Unstructured Data:** Data that does not have a predefined data model or is not organized in a predefined manner. It accounts for the vast majority of enterprise data. Examples include text documents, emails, videos, photos, audio files, social media posts, and web pages. Specialized databases (NoSQL) and techniques (NLP, computer vision) are needed to manage and analyze it."
    },
    {
        "id": 162,
        "question": "Why is it important to understand file processing systems even though they are outdated?",
        "answer": "Understanding the limitations of traditional file processing systems is crucial for two reasons: 1. **Legacy Systems:** Many businesses still rely on legacy applications built on file processing systems. DBAs and developers need to understand them to maintain, interface with, or migrate these systems. 2. **Appreciating DBMS Benefits:** Studying the problems of file processing (data redundancy, inconsistency, program-data dependence) provides a deep appreciation for the features and benefits of modern DBMS. It answers the 'why' behind database principles like data independence, data integrity, and non-redundancy, reinforcing their importance in good design."
    },
    {
        "id": 163,
        "question": "What are the major disadvantages of conventional file processing systems?",
        "answer": "File processing systems suffer from several critical drawbacks: 1. **Data Redundancy and Inconsistency:** The same data is often stored in multiple files, leading to duplication, wasted storage, and potential inconsistencies. 2. **Difficulty in Accessing Data:** Writing ad-hoc queries is hard; new programs need to be written for each new task. 3. **Data Isolation:** Data is scattered in various files, making it difficult to get a unified view. 4. **Integrity Problems:** It is hard to apply consistency constraints (e.g., account balance > 0) across multiple files. 5. **Atomicity Problems:** Ensuring a transaction (like a fund transfer) completes entirely or not at all is difficult to program. 6. **Concurrent Access Anomalies:** Uncontrolled multi-user access can lead to incorrect data."
    },
    {
        "id": 164,
        "question": "What are the five categories of database applications based on scope?",
        "answer": "Databases can be categorized by the number of users and the organizational scope they support: 1. **Personal Database:** Designed to support a single user, typically on a personal computer (e.g., a contacts database). 2. **Workgroup Database:** Supports a small team of users (usually fewer than 25). 3. **Department Database:** Supports the major functions of a single department within an organization (e.g., HR, Marketing). 4. **Enterprise Database:** Supports the organization-wide operations and decision-making of an entire company. It spans multiple departments and is often very complex. 5. **Internet/Web Database:** Accessible to anyone via the internet or an intranet/extranet, supporting global user bases (e.g., Amazon.com, Google Search)."
    },
    {
        "id": 165,
        "question": "What is the difference between an intranet and an extranet?",
        "answer": "Both are private networks, but their access differs: *   **Intranet:** A private network that uses internet technologies (like web servers and browsers) to serve the internal needs of an organization. It is accessible only to the organization's employees, members, or others with internal authorization. It is behind a firewall. *   **Extranet:** An extension of an organization's intranet that provides controlled access to external parties, such as partners, vendors, suppliers, or specific customers. It allows for secure collaboration and business-to-business (B2B) transactions over the internet. An extranet is a semi-private network."
    },
    {
        "id": 166,
        "question": "What are the five components of an Information Systems Architecture?",
        "answer": "The Zachman Framework outlines these five fundamental components that must be aligned for a successful system: 1. **Data (What):** The entities and information the business uses and needs. 2. **Function (How):** The business processes and functions that transform the data. 3. **Network (Where):** The geographical distribution of the business and its systems. 4. **People (Who):** The people involved—the organizational units and actors. 5. **Time (When):** The timing and business cycles that trigger events and processes. 6. **Motivation (Why):** The business goals, strategies, and rules that govern the other components. (Note: Some versions list 6, including Motivation.)"
    },
    {
        "id": 167,
        "question": "What is the Systems Development Life Cycle (SDLC)?",
        "answer": "The SDLC is a structured, phased process for planning, creating, testing, and deploying an information system. It provides a framework for managing the complexity of system development. The traditional phases are: 1. **Planning:** Defining the system's scope, goals, and feasibility. 2. **Analysis:** Determining business requirements. 3. **Design:** Designing the system architecture, databases, and interfaces. 4. **Implementation:** Building, testing, and installing the system. 5. **Maintenance:** Fixing issues, making enhancements, and adapting to new requirements. While often depicted as a waterfall, modern approaches use iterative and agile variations of the SDLC."
    },
    {
        "id": 168,
        "question": "What are the two main types of packaged data models?",
        "answer": "Packaged data models are pre-built, generic data models that can be purchased and customized: 1. **Universal Data Models:** Very generic models that represent common business functions found in almost every organization, such as 'Party' (people and organizations), 'Product', 'Order', 'Invoice', and 'Shipment'. They provide a robust starting point. 2. **Industry-Specific Data Models:** Models tailored to the specific needs, terminology, and processes of a particular industry, such as telecommunications, healthcare, insurance, or retail. They incorporate the industry's standard best practices and regulatory requirements."
    },
    {
        "id": 169,
        "question": "Who are the key members of a systems or database development team?",
        "answer": "A successful project requires a team with diverse skills: 1. **Systems Analyst:** Acts as a liaison between stakeholders and the technical team, gathering and translating business requirements. 2. **Database Designer (Data Architect):** Focuses on designing the database structure, including data models, schemas, and integrity constraints. 3. **Application Programmers:** Write the application code that interacts with the database. 4. **Database Administrator (DBA):** Implements and manages the database environment, ensuring performance, security, and availability. 5. **End Users:** The ultimate consumers of the system, who provide crucial input on requirements and test the final product. 6. **Project Manager:** Oversees the project's budget, schedule, and scope."
    },
    {
        "id": 170,
        "question": "What are the key database activities within the SDLC?",
        "answer": "Database development activities run in parallel with the general SDLC phases: 1. **Enterprise Modeling (Planning):** Analyzing the current data processing environment. 2. **Conceptual Data Modeling (Analysis):** Creating an ERD to identify entities, relationships, and attributes based on business requirements. 3. **Logical Database Design (Design):** Transforming the conceptual model into a logical schema (e.g., relational tables), normalizing the design, and defining integrity rules. 4. **Physical Database Design (Design):** Mapping the logical design to a specific DBMS, defining storage structures, indexing, and partitioning for performance. 5. **Database Implementation (Implementation):** Creating the database, loading data, and implementing application programs. 6. **Database Maintenance (Maintenance):** Tuning performance, fixing bugs, and adapting the schema to new requirements."
    },
    {
        "id": 171,
        "question": "What is an Entity-Relationship Diagram (ERD)?",
        "answer": "An Entity-Relationship Diagram (ERD) is a visual graphical representation of the logical structure of a database. It is a conceptual data model that uses standardized symbols to depict: *   **Entities:** Represented as rectangles. These are the 'things' (e.g., Customer, Order). *   **Attributes:** Represented as ovals or within the entity rectangle. These are the properties of entities (e.g., CustomerName, OrderDate). *   **Relationships:** Represented as diamonds or lines connecting entities. These show how entities are associated (e.g., a Customer 'places' an Order). ERDs are a crucial communication tool between stakeholders and database designers in the early stages of the SDLC."
    },
    {
        "id": 172,
        "question": "What are the characteristics of a good data definition?",
        "answer": "A good data definition, often stored in a data dictionary, is clear, precise, and comprehensive. It should include: 1. **A clear, concise name and description** of the data element. 2. **The data type and length** (e.g., VARCHAR(50), DATE). 3. **The allowable values or range** (e.g., 'Y'/'N', 1-100). 4. **Its nullability** (whether it is required or optional). 5. **The source** of the data. 6. **Ownership** (who is responsible for the data). 7. **Examples** of valid data. 8. **Security and privacy classifications.** This ensures everyone in the organization has a shared understanding of the data's meaning and usage rules."
    },
    {
        "id": 173,
        "question": "What do minimum and maximum cardinality represent in a relationship?",
        "answer": "Cardinality defines the numerical attributes of the relationship between two entities. *   **Minimum Cardinality:** Specifies whether the relationship is mandatory or optional. A minimum of 0 means participation is optional for an entity. A minimum of 1 means participation is mandatory. (e.g., An `Order` *must* be placed by a `Customer` -> min cardinality for Customer is 1). *   **Maximum Cardinality:** Specifies the maximum number of entity instances that can be involved in the relationship. This defines the relationship type: 1 (one) or N (many). (e.g., One `Customer` can place many `Orders` -> max cardinality for Order is N). Together, they are often written as (min, max) - e.g., (1, N)."
    },
    {
        "id": 174,
        "question": "What are the best practices for naming relationships in an ERD?",
        "answer": "Relationship names should be meaningful action verbs or verb phrases that accurately describe the nature of the association between entities. Best practices include: 1. **Use a Verb Phrase:** The name should indicate the action (e.g., 'places', 'is assigned to', 'contains'). 2. **Be Specific:** Avoid vague names like 'has' or 'is related to'. Use 'manages', 'submits', 'belongs to'. 3. **Readable in Both Directions:** The relationship should make sense when read from either entity. For example: `Customer` *places* `Order` / `Order` *is placed by* `Customer`. 4. **Use Present Tense:** This makes the model feel current and active."
    },
    {
        "id": 175,
        "question": "Why is it important to model time-dependent data with time stamps?",
        "answer": "Modeling time-dependent data with time stamps (e.g., `StartDate`, `EndDate`, `LastModifiedDate`) is crucial for maintaining a historical record and understanding the state of data over time. Without it, you can only see the current state, losing all history. This is important for: 1. **Historical Reporting and Auditing:** Tracking changes for compliance and understanding past states. 2. **Temporal Queries:** Answering questions like 'What was the price of this product on January 1st?' or 'Who was the manager of this department in 2020?'. 3. **Correcting Errors:** Allowing you to roll back to a previous valid state if an error is made. This concept is central to temporal databases and slowly changing dimensions in data warehousing."
    },
    {
        "id": 176,
        "question": "What is the difference between total and partial specialization in a supertype/subtype relationship?",
        "answer": "This concept defines whether all instances of a supertype must also be an instance of a subtype. *   **Total Specialization ( completeness constraint):** Every instance of the supertype *must* be an instance of at least one subtype. In an ERD, this is represented by a double line connecting the supertype to the circle. (e.g., Every `Vehicle` in the database MUST be either a `Car` or a `Truck`). *   **Partial Specialization:** An instance of the supertype *can* be an instance of a subtype, but it is not mandatory. It is represented by a single line. (e.g., An `Employee` could be a `Manager` or a `Engineer`, but some employees might not be classified into either subtype)."
    },
    {
        "id": 177,
        "question": "What is the difference between an ERD and an EERD?",
        "answer": "An ERD (Entity-Relationship Diagram) represents the basic concepts of entities, attributes, and relationships. An EERD (Enhanced Entity-Relationship Diagram) extends the classical ER model with additional semantic concepts to represent more complex real-world situations. The key enhancements in an EERD are: 1. **Subtyping and Inheritance:** The ability to model supertype/subtype relationships (generalization/specialization hierarchies), where subtypes inherit attributes and relationships from their supertype. 2. **Union (Category):** Modeling a subtype that can inherit from different supertypes. 3. **More Detailed Constraints:** Expressing disjointness and completeness constraints on subtypes more explicitly."
    },
    {
        "id": 178,
        "question": "What is the difference between the disjoint and overlap rules in subtyping?",
        "answer": "These rules constrain whether an instance of a supertype can be an instance of more than one subtype. *   **Disjoint Rule:** An instance of the supertype can be an instance of *only one* of the subtypes. The subtypes are mutually exclusive. (e.g., A `Person` is either a `Male` or a `Female` - represented by a 'd' in the circle on the EERD). *   **Overlap Rule:** An instance of the supertype can be an instance of *more than one* subtype. The subtypes are not mutually exclusive. (e.g., A `Staff` member at a university could be both a `Teacher` and a `Researcher` - represented by an 'o' in the circle)."
    },
    {
        "id": 179,
        "question": "What are the three main types of business rules?",
        "answer": "Business rules are precise statements that define or constrain some aspect of a business. They are categorized as: 1. **Derivations:** Rules that define how knowledge is derived from other knowledge. They are often formulas or calculations. (e.g., `employee_bonus = yearly_sales * 0.05`). 2. **Structural Assertions (Terms and Facts):** Rules that express static structure and definitions. They are the 'nouns' and 'verbs' of the business. (e.g., 'A customer may place many orders', 'An order must have a valid customer'). 3. **Action Assertions (Constraints):** Rules that constrain the actions of the business, often with conditions. (e.g., 'An employee's salary cannot be decreased', 'A loan application must be approved by a manager if the amount is over $10,000')."
    },
    {
        "id": 180,
        "question": "How is a scenario used to define business rules?",
        "answer": "A scenario is a concise, narrative description of a specific sequence of events or actions within a business process. It is used as a tool to discover, validate, and document business rules. By walking through a realistic 'story' (e.g., 'A customer returns a purchased item to a store'), analysts can identify the constraints, derivations, and structural facts that govern that process. The scenario helps to ask the right questions: 'What data is needed?', 'What checks must be performed?', 'What are the possible outcomes?'. The answers to these questions are formalized into the business rules that the database and application must enforce."
    },
    {
        "id": 181,
        "question": "What are the primary objectives of database normalization?",
        "answer": "The core goals of normalization are to: 1. **Minimize Data Redundancy:** Store each logical data item in only one place to conserve space and prevent update anomalies. 2. **Eliminate Anomalies:** Prevent inconsistencies that arise from insertion, deletion, and update operations. 3. **Simplify Data Integrity:** Make it easier to enforce integrity constraints by structuring data logically. 4. **Produce a Stable Database Structure:** Create a design that is less susceptible to changes in requirements and is logically flexible. The process achieves this by organizing data into tables based on their functional dependencies."
    },
    {
        "id": 182,
        "question": "What are the key properties of a well-defined relation in a database?",
        "answer": "A proper relation in the relational model adheres to these properties: 1. **Unique Name:** Every relation (table) must have a distinct name within its schema. 2. **Atomic Values:** Every attribute (column) must contain only atomic (indivisible) values, satisfying 1NF. 3. **Unique Rows:** Every tuple (row) must be unique; no two rows can be identical. 4. **Unordered Rows:** The order of the rows is not significant for data meaning. 5. **Unordered Columns:** The order of the columns is not significant; each column is identified by its name, not its position. 6. **Unique Attribute Names:** Every attribute within a relation must have a unique name."
    },
    {
        "id": 183,
        "question": "What are the steps to convert a relation into Third Normal Form (3NF)?",
        "answer": "Converting a relation to 3NF is a systematic process: 1. **Ensure 1NF:** Verify that the table has no repeating groups or composite attributes; all values are atomic. 2. **Ensure 2NF:** Identify the primary key and verify that all non-prime attributes are fully functionally dependent on the entire key (remove partial dependencies). 3. **Identify Transitive Dependencies:** Find non-prime attributes that are dependent on another non-prime attribute instead of directly on the primary key. 4. **Remove Transitive Dependencies:** For each transitive dependency (e.g., PK -> A -> B), create a new relation. The determinant (A) becomes the primary key of the new relation, and all attributes dependent on it (B) are moved into it. Leave the determinant (A) as a foreign key in the original relation to maintain the link."
    },
    {
        "id": 184,
        "question": "How is a supertype/subtype relationship mapped to physical database tables?",
        "answer": "The most common mapping strategy for supertype/subtype hierarchies is to create a separate table for the supertype and for each subtype: 1. **Supertype Table:** Contains all attributes common to all subtypes. Its primary key is the primary key for the entire hierarchy. 2. **Subtype Tables:** Each subtype table contains two types of columns: a) The primary key of the supertype (which is also the primary key of the subtype table and a foreign key back to the supertype). b) The attributes that are unique to that specific subtype. This design supports the 'is-a' relationship and allows for efficient querying of all instances (via the supertype) or just specific types (via the subtypes). A discriminator attribute (e.g., `EmployeeType`) is often added to the supertype to indicate which subtype table to look in for additional details."
    },
    {
        "id": 185,
        "question": "How would you describe domain constraints in a database?",
        "answer": "Domain constraints are the most fundamental rules that enforce data integrity at the column level. A domain is the set of all possible valid values that an attribute is allowed to contain. Domain constraints ensure that the value for a given attribute must be: 1. **Of the Correct Data Type:** An `Age` column must be an integer. 2. **Within a Defined Set or Range:** An `Age` value must be between 0 and 120. A `Status` value must be in ('Active', 'Inactive', 'Pending'). 3. **A Member of a Distinct List:** Enforced by `CHECK` constraints or user-defined data types. 4. **Consistent with Nullability:** If a column is defined as `NOT NULL`, it must always contain a value. These constraints are checked whenever a value is inserted or updated."
    },
    {
        "id": 186,
        "question": "What are the four key objectives when selecting a data type for an attribute?",
        "answer": "Choosing the right data type is crucial for efficiency and integrity. The objectives are: 1. **Represent All Possible Values:** The data type must be able to represent all valid values for the attribute, both current and future. 2. **Improve Data Integrity:** The data type should inherently prevent invalid data (e.g., a `DATE` type prevents the entry of 'abc' as a date). 3. **Support All Required Data Manipulations:** The data type must allow for the necessary operations (e.g., you can perform arithmetic on `INTEGER` but not on `VARCHAR`). 4. **Minimize Storage Space:** Choose the most space-efficient data type that meets the other objectives (e.g., use `SMALLINT` instead of `INTEGER` if values will never exceed 32,767)."
    },
    {
        "id": 187,
        "question": "What are the four primary types of indexes and their purposes?",
        "answer": "Indexes are categorized based on their structure and uniqueness: 1. **Unique Primary Index:** Determines the physical storage order of the data in a table (the clustered index). There can be only one per table. It must contain unique values. 2. **Nonunique Primary Index:** Used in some database systems where the primary index does not require uniqueness, but still governs storage. 3. **Unique Secondary Index:** A non-clustered index that enforces uniqueness on a column or set of columns (e.g., a unique constraint on an `Email` column). It provides fast access for lookups. 4. **Nonunique Secondary Index:** A non-clustered index that does not enforce uniqueness. It is used to speed up queries on columns that have many duplicate values and are frequently used in `WHERE` clauses or as join conditions."
    },
    {
        "id": 188,
        "question": "What is denormalization and why would a database designer use it?",
        "answer": "Denormalization is the **strategic process of intentionally introducing redundancy into a previously normalized database table.** It is a performance optimization technique that trades off some degree of data redundancy and potential anomaly risk for improved query speed. Designers use it when: 1. **Query Performance is Critical:** When joins between multiple normalized tables are too slow for frequently run reports or queries. 2. **Heavy Read Operations:** In data warehouses or reporting databases where the vast majority of operations are reads (SELECT), and writes (INSERT/UPDATE) are less frequent and controlled. The goal is to reduce the number of table joins needed by pre-consolidating data, which can dramatically speed up complex queries."
    },
    {
        "id": 189,
        "question": "How do the hierarchical and network database models differ?",
        "answer": "These are two pre-relational data models with key differences: *   **Hierarchical Model:** Structures data in a strict tree-like, parent-child hierarchy. Each parent can have many children, but each child can have only one parent. It efficiently represents one-to-many relationships but struggles with many-to-many relationships. Data access is navigational, following predefined paths. *   **Network Model:** An extension of the hierarchical model that allows a child (member) to have multiple parents (owners). This allows it to directly represent more complex relationships, including many-to-many. It is more flexible than the hierarchical model but is also more complex to design and navigate. Both models lack the simplicity and ad-hoc query capability of the relational model."
    },
    {
        "id": 190,
        "question": "What is the difference between horizontal and vertical partitioning?",
        "answer": "Partitioning is the physical process of splitting a large table into smaller, more manageable pieces: *   **Horizontal Partitioning (Sharding):** Splits a table by **rows**. Each partition has the same columns but contains a different subset of the total rows. This is often done based on a range of values (e.g., orders from 2023, orders from 2024) or a hash key. It is useful for distributing data across storage devices and improving query performance on specific data segments. *   **Vertical Partitioning:** Splits a table by **columns**. One partition contains frequently accessed columns, while another contains less frequently accessed or large (BLOB) columns. This is often used to improve I/O performance for common queries that don't need all the table's data."
    },
    {
        "id": 191,
        "question": "What is the difference between a dynamic view and a materialized view?",
        "answer": "This is a key distinction in how views are implemented and updated: *   **Dynamic View (Standard View):** A virtual table that does not store data. Whenever the view is queried, its defining `SELECT` statement is executed against the underlying base tables in real-time. It always returns the most current data but can be slow if the query is complex. *   **Materialized View (Snapshot):** A physical copy of the view's result set is stored as a separate table. The data is persisted to disk. It is extremely fast to query because it avoids recomputing the join and aggregation, but the data can become stale. The view must be periodically **refreshed** to update its stored data with changes from the base tables. This is a trade-off between performance and data currency."
    },
    {
        "id": 192,
        "question": "What techniques can be used to tune the performance of an operational database?",
        "answer": "Database performance tuning involves multiple strategies: 1. **Query Optimization:** Rewriting application SQL to be more efficient (e.g., avoiding `SELECT *`, using joins instead of subqueries). 2. **Indexing:** Adding appropriate indexes on columns used in `WHERE`, `JOIN`, `ORDER BY`, and `GROUP BY` clauses. 3. **Database Design Adjustment:** Considering denormalization for critical, slow queries. 4. **Hardware Optimization:** Adding more RAM (for buffer cache), using faster disks (SSDs), or adding more CPUs. 5. **Configuration Tuning:** Adjusting DBMS configuration parameters (e.g., memory allocation, parallelism settings). 6. **Statistics Maintenance:** Ensuring the DBMS's optimizer has up-to-date statistics on table sizes and data distribution to choose the best execution plans."
    },
    {
        "id": 193,
        "question": "What are the three main categories of SQL commands?",
        "answer": "SQL commands are broadly classified based on their function: 1. **Data Definition Language (DDL):** Commands used to define, alter, and drop the structure of database objects. These commands implicitly commit transactions. Key commands: `CREATE`, `ALTER`, `DROP`, `TRUNCATE`, `RENAME`. 2. **Data Manipulation Language (DML):** Commands used to manipulate data within existing objects. Key commands: `SELECT` (retrieval), `INSERT` (add), `UPDATE` (modify), `DELETE` (remove). 3. **Data Control Language (DCL):** Commands used to control access to the database and its objects. Key commands: `GRANT` (give privileges), `REVOKE` (take away privileges). Some systems also define Transaction Control Language (TCL) with commands like `COMMIT` and `ROLLBACK`."
    },
    {
        "id": 194,
        "question": "What are the key steps to prepare for creating a database table?",
        "answer": "Thorough preparation before executing a `CREATE TABLE` statement is essential: 1. **Define Attributes:** Finalize the list of column names. 2. **Assign Data Types:** Choose the most appropriate data type, length, and precision for each column. 3. **Identify Keys:** Designate the primary key and any foreign keys. 4. **Establish Constraints:** Decide on `NOT NULL`, `UNIQUE`, `CHECK`, and `DEFAULT` constraints for each column. 5. **Plan Indexes:** Identify which columns will need indexes for performance. 6. **Consider Relationships:** Understand how this table relates to others to define foreign keys correctly. 7. **Review Normalization:** Ensure the table design adheres to the desired normal form to avoid redundancy. This planning is often done using a data modeling tool or ER diagram."
    },
    {
        "id": 195,
        "question": "What are some potential disadvantages of a standardized language like SQL?",
        "answer": "While standardization is largely beneficial, it has some drawbacks: 1. **Complexity:** The SQL standard is vast and complex, making full compliance difficult for vendors and mastery difficult for developers. 2. **Vendor Extensions:** DBMS vendors often add proprietary extensions to standard SQL to provide additional functionality. This can hurt portability between different database systems. 3. **Pace of Innovation:** The formal standards process can be slow, sometimes lagging behind the innovative features introduced by individual vendors. 4. **'Lowest Common Denominator' Effect:** To ensure portability, developers might avoid using powerful vendor-specific features, potentially resulting in less efficient or more complex code."
    },
    {
        "id": 196,
        "question": "What is a table join and why is it fundamental to the relational model?",
        "answer": "A join is a fundamental SQL operation that combines rows from two or more tables based on a related column between them. It is the primary mechanism for querying data across multiple tables, which is the core principle of the relational model. By storing data in separate, normalized tables and using joins to reassemble it, the model achieves its goals of minimizing redundancy and maintaining data integrity. The join operation, typically based on primary key-foreign key relationships, allows the database to reconstruct complex entity relationships at query time, providing tremendous flexibility."
    },
    {
        "id": 197,
        "question": "How would you contrast a database trigger with a stored procedure?",
        "answer": "| Feature | Trigger | Stored Procedure |\n| :--- | :--- | :--- |\n| **Invocation** | Automatically fired (executed) by the DBMS in response to a specific DML event (`INSERT`, `UPDATE`, `DELETE`) on a table. | Explicitly called and executed by a user, application, or another procedure. |\n| **Control** | Event-driven; the programmer has no control over when it runs. | Called on demand; the programmer has full control over execution. |\n| **Parameters** | Cannot accept explicit parameters. They use special pseudo-records (e.g., `NEW` and `OLD`) to access the affected row data. | Can accept input, output, and input/output parameters. |\n| **Transaction Context** | Part of the transaction that caused it to fire. | Can contain its own transaction control statements (`COMMIT`, `ROLLBACK`). |\n| **Common Use Case** | Enforcing complex business rules, auditing, maintaining derived data automatically. | Modularizing application logic, performing complex operations, improving performance."
    },
    {
        "id": 198,
        "question": "What is an outer join and when would you use it?",
        "answer": "An outer join is a type of join that returns not only the matching rows between two tables but also the non-matching rows from one or both tables. The result set includes `NULL` values for columns from the table that lacks a matching row. Types include: *   **LEFT OUTER JOIN:** Returns all rows from the left table and the matched rows from the right table. *   **RIGHT OUTER JOIN:** Returns all rows from the right table and the matched rows from the left table. *   **FULL OUTER JOIN:** Returns all rows when there is a match in either the left or right table. You use an outer join when you need a complete list from one table regardless of whether a corresponding record exists in the other table (e.g., list all customers and their orders, including customers who have never placed an order)."
    },
    {
        "id": 199,
        "question": "What is a subquery and how is it used?",
        "answer": "A subquery (or inner query or nested query) is a `SELECT` statement embedded within the `WHERE` or `HAVING` clause of another SQL statement (the outer query). It is used to return a value or set of values that the outer query uses to complete its search condition. Subqueries are powerful for: 1. **Set Membership:** Using `IN` or `NOT IN` (e.g., `SELECT * FROM Products WHERE CategoryID IN (SELECT CategoryID FROM Categories WHERE Name = 'Beverages')`). 2. **Comparisons:** Using operators like `=`, `>`, `ANY`, `ALL`. 3. **Existence Checks:** Using `EXISTS` or `NOT EXISTS` with correlated subqueries. They allow for dynamic, data-driven query conditions that would be impossible to hard-code."
    },
    {
        "id": 200,
        "question": "What is the difference between embedded SQL and dynamic SQL?",
        "answer": "This distinction lies in when the SQL statement is constructed and prepared: *   **Embedded SQL:** SQL statements are **hard-coded** directly within the source code of a host programming language (like C, COBOL, or Java). The statements are static and known at application compile time. They are pre-compiled into an executable access plan. *   **Dynamic SQL:** SQL statements are constructed and **assembled as strings at runtime** within the application. The application can build the statement on the fly based on user input or other conditions. The DBMS must parse, optimize, and compile the statement at runtime, which adds overhead but provides ultimate flexibility for ad-hoc query tools."
    },
    {
        "id": 201,
        "question": "What is the difference between two-tier and three-tier client-server architecture?",
        "answer": "This refers to the logical separation of an application's components: *   **Two-Tier Architecture:** Has only two logical layers: 1) The **Client Tier** (Presentation Layer), which handles the user interface and application logic, and 2) The **Database Server Tier** (Data Layer), which houses the DBMS and database. The client communicates directly with the database. It is simpler but can lead to performance and scalability issues as the number of clients grows. *   **Three-Tier Architecture:** Introduces a middle tier: 1) **Client Tier** (Presentation Layer: UI), 2) **Application Server Tier** (Business Logic Layer: rules, processing), and 3) **Database Server Tier** (Data Layer). The client talks to the application server, which in turn talks to the database. This improves scalability, security, and flexibility, as business logic is centralized and separated from the data and presentation layers."
    },
    {
        "id": 202,
        "question": "How do SQL and QBE differ as database query languages?",
        "answer": "SQL and QBE offer different paradigms for querying a database: *   **SQL (Structured Query Language):** A text-based, declarative language. The user writes a command in a syntax similar to a sentence to specify what data is wanted. It is a standard, powerful, and precise language used by programmers and power users. *   **QBE (Query-by-Example):** A graphical, visual language. The user retrieves data by filling in templates or putting example elements directly into table skeletons on the screen. It is often considered more intuitive for novice users. Many modern GUI database tools (like Microsoft Access's query designer) translate QBE-like actions into SQL commands behind the scenes. SQL is the universal standard, while QBE is a user-friendly interface that generates SQL."
    },
    {
        "id": 203,
        "question": "What is ODBC and what problem does it solve?",
        "answer": "ODBC (Open Database Connectivity) is a standard application programming interface (API) for accessing database management systems (DBMS). It solves the problem of application **portability** and **interoperability**. Before ODBC, applications had to be written to use the specific API of a single DBMS vendor (e.g., Oracle's OCI). To switch databases, the application had to be rewritten. ODBC provides a universal, vendor-neutral interface. An application written to the ODBC standard can connect to any DBMS (Oracle, SQL Server, MySQL, etc.) for which an ODBC driver exists, dramatically reducing development and maintenance costs."
    },
    {
        "id": 204,
        "question": "What is the difference between a 'thin client' and a 'fat client'?",
        "answer": "This classification depends on how much processing logic is handled by the client machine: *   **Fat Client (Thick Client):** A client machine that runs the majority of the application's logic, including data processing, business rules, and the user interface. It requires significant resources on the client PC and often maintains a direct connection to the database. It can function offline but is complex to deploy and update. *   **Thin Client:** A client machine that primarily handles only the **user interface**. The application's core logic and data processing are executed on one or more application servers. It requires minimal resources on the client PC, is easy to deploy and update, and relies on a constant network connection. Web browsers are the ultimate thin clients."
    },
    {
        "id": 205,
        "question": "Why would an Access user learn VBA?",
        "answer": "While Microsoft Access provides a powerful interface for building databases without code, learning VBA (Visual Basic for Applications) unlocks advanced capabilities: 1. **Complex Functionality:** Create custom functions and procedures that go beyond the built-in wizards and expression builder. 2. **Error Handling:** Write robust code that can gracefully handle unexpected errors and user mistakes. 3. **Automation:** Automate complex, multi-step tasks that involve forms, reports, and data manipulation. 4. **Integration:** Interact with other Windows applications (like Excel, Word, Outlook) through OLE Automation. 5. **Performance:** VBA code can execute complex operations faster than a series of macro actions. 6. **User Interaction:** Create sophisticated custom dialog boxes and user forms for data entry and navigation."
    },
    {
        "id": 206,
        "question": "What is middleware in the context of web-database connectivity?",
        "answer": "In web-database integration, middleware is software that acts as a bridge between a web server and a database server. It resides on the web server and facilitates communication between the two. Its primary role is to: 1. **Receive Requests:** Accept requests from a user's web browser sent to the web server. 2. **Process Business Logic:** Execute the application's programming logic. 3. **Interact with the Database:** Generate database queries, connect to the DBMS, pass the queries, and retrieve the results. 4. **Format Results:** Format the database results (e.g., into HTML, JSON, or XML). 5. **Return Response:** Send the formatted response back to the web server, which then delivers it to the user's browser. Examples include ASP.NET, PHP, Java Servlets, and ColdFusion."
    },
    {
        "id": 207,
        "question": "What are JavaScript and VBScript and how were they used?",
        "answer": "JavaScript and VBScript are client-side scripting languages primarily used to add interactivity and dynamic behavior to web pages within a user's browser. *   **JavaScript:** A versatile language developed by Netscape. It became the dominant client-side scripting standard due to its cross-browser support. It is used for tasks like form validation, creating dynamic menus, and interacting with the HTML Document Object Model (DOM). It is not related to Java. *   **VBScript:** A scripting language from Microsoft based on Visual Basic syntax. It was primarily used to script web pages in Microsoft's Internet Explorer browser. It never achieved cross-browser adoption and is now largely obsolete. Both executed code on the client machine, reducing the load on the web server but depending on the browser's capabilities."
    },
    {
        "id": 208,
        "question": "What are Web Services?",
        "answer": "Web Services are a standardized way for different software applications, running on a variety of platforms and frameworks, to communicate and exchange data over a network (typically the internet). They use open standards: *   **XML** to tag the data. *   **SOAP** (or REST) to define the message format. *   **WSDL** to describe the service available. *   **UDDI** to list what services are available. The key idea is **interoperability**. A Java application on a Linux server can call a web service provided by a .NET application on a Windows server to request data or trigger a process, without needing to know the internal details of the other system."
    },
    {
        "id": 209,
        "question": "Can you provide an overview of XML and its role?",
        "answer": "XML (eXtensible Markup Language) is a meta-markup language that provides a flexible, self-describing way to structure and store data. Its key roles are: 1. **Data Transport:** It is the fundamental language for transmitting data between heterogeneous systems, especially in web services. 2. **Data Storage:** It can be used to store configuration files and complex, hierarchical data. 3. **Separation of Concerns:** It strictly separates data content from its presentation, unlike HTML which mixes them. 4. **Interoperability:** As a text-based, platform-independent standard, it enables data exchange between vastly different applications. XML uses custom tags (e.g., `<price>29.99</price>`) that describe the meaning of the data, making it both human and machine-readable."
    },
    {
        "id": 210,
        "question": "What are the primary website security concerns?",
        "answer": "Website security involves protecting all components of a web application from threats. Primary concerns include: 1. **Unauthorized Data Access:** Preventing hackers from accessing sensitive data in the database (e.g., through SQL Injection attacks). 2. **Data Integrity:** Ensuring data cannot be maliciously altered. 3. **Availability:** Protecting against Denial-of-Service (DoS) attacks that make the site unavailable to users. 4. **Authentication & Authorization:** Ensuring users are who they claim to be (authentication) and can only access data and functions they are permitted to (authorization). 5. **Client-Side Attacks:** Protecting users from cross-site scripting (XSS) and other attacks launched from the website. Security must be addressed at every level: network, operating system, web server, application code, and database."
    },
    {
        "id": 211,
        "question": "What is the role of metadata in a three-layer data architecture?",
        "answer": "In a three-layer architecture (e.g., a data warehouse with operational, reconciled, and derived data layers), metadata acts as the essential 'glue' and 'map' for the entire system. Each layer has its own associated metadata: 1. **Operational Metadata:** Describes the data in the operational source systems - its structure, format, and meaning. It is used for extraction and transformation. 2. **Reconciled Data Metadata (Enterprise Data Warehouse):** Describes the integrated, cleansed, and historical data in the central warehouse. It defines the 'single version of the truth' for the enterprise. 3. **Derived Data Metadata (Data Marts):** Describes the data as it is structured in departmental data marts for specific business analysis. It includes definitions of pre-calculated measures, dimensions, and hierarchies used in OLAP and reporting. This metadata is crucial for developers, administrators, and business users to understand and trust the data."
    },
    {
        "id": 212,
        "question": "Why are operational and informational systems typically kept separate?",
        "answer": "Operational systems (OLTP - Online Transaction Processing) and informational systems (OLAP - Online Analytical Processing) are separated because they have fundamentally different and conflicting purposes and characteristics: | Aspect | Operational System (OLTP) | Informational System (OLAP) |\n| :--- | :--- | :--- |\n| **Primary Purpose** | Run the day-to-day business. | Support decision-making and analysis. |\n| **Data Content** | Current, detailed data. | Historical, summarized, and consolidated data. |\n| **Data Model** | Highly normalized for integrity and update speed. | Denormalized (star/snowflake schema) for query speed. |\n| **Access Pattern** | Many small, quick read/write transactions. | Few, but complex and long-running, read-only queries. |\n| **Users** | Clerks, customers, administrators. | Managers, analysts, data scientists. |\n| Separating them prevents analytical queries from slowing down critical transaction processing and allows each system to be optimized for its specific workload."
    },
    {
        "id": 213,
        "question": "What are the defining characteristics of a data warehouse?",
        "answer": "A data warehouse, as defined by Bill Inmon, has four key characteristics: 1. **Subject-Oriented:** Data is organized around major subjects of the enterprise (e.g., customers, products, sales), rather than by specific operational applications. 2. **Integrated:** Data is gathered from various disparate source systems and made consistent. This involves resolving naming conflicts, data type differences, and encoding inconsistencies. 3. **Nonvolatile:** Once data is entered into the warehouse, it is not updated or deleted in the same way operational data is. It is a stable, read-only environment for analysis. Data is loaded and refreshed, not changed. 4. **Time-Variant:** Data is stored to provide a historical perspective. Every record is accurate with respect to some moment in time, enabling analysis of trends and changes over time."
    },
    {
        "id": 214,
        "question": "Why does an 'information gap' often exist in organizations?",
        "answer": "An information gap exists when decision-makers lack the timely, integrated, and relevant information they need. This gap is caused by two main factors: 1. **Data Silos:** Operational systems are typically built independently for specific functions (e.g., sales, inventory, HR). This leads to data being stored in isolated 'silos' with different structures and meanings, making it difficult to get a unified, enterprise-wide view. 2. **Differing Processing Needs:** The primary design goal of operational systems is to support high-volume transaction processing, not complex decision-support queries. Running analytical queries directly on operational systems would degrade their performance, creating a technical barrier to accessing information. Data warehouses and data marts are built specifically to bridge this gap."
    },
    {
        "id": 215,
        "question": "How do a data warehouse and a data mart differ?",
        "answer": "| Characteristic | Data Warehouse | Data Mart |\n| :--- | :--- | :--- |\n| **Scope** | Enterprise-wide. Centralized. | Departmental or functional. Decentralized. |\n| **Subject** | Multiple integrated subjects. | A single, specific subject (e.g., 'Sales', 'Finance'). |\n| **Data Sources** | Many diverse sources from across the organization. | Fewer sources, often a subset of the data warehouse. |\n| **Size** | Very large (TB to PB range). | Smaller (GB to TB range). |\n| **Implementation Time** | Long (months to years). | Shorter (months). |\n| **Design** | Highly normalized (Inmon) or dimensional (Kimball). | Dimensional (star schema). |\n| A data mart can be a **dependent** subset of a data warehouse or an **independent** stand-alone system built directly from operational sources."
    },
    {
        "id": 216,
        "question": "What is the difference between data administration and database administration?",
        "answer": "These are two distinct but related roles: *   **Data Administration (DA):** A **business-oriented** function focused on the management of data as a strategic corporate asset. DA responsibilities are high-level and include: data planning, defining data standards and policies, managing the data dictionary, and resolving data ownership and privacy issues. *   **Database Administration (DBA):** A **technically-oriented** function focused on the physical implementation and maintenance of the database management system (DBMS). DBA responsibilities are hands-on and include: DBMS installation, database design, performance tuning, backup and recovery, security implementation, and troubleshooting. DA is about *what* data is needed and *why*; DBA is about *how* to store and manage it effectively."
    },
    {
        "id": 217,
        "question": "What are key security features provided by a DBMS?",
        "answer": "A robust DBMS provides multiple layers of security features: 1. **Authentication:** Verifying the identity of a user trying to connect to the database (e.g., via username/password, integrated Windows authentication). 2. **Authorization:** Controlling what a user can do through **privileges** (e.g., `SELECT`, `INSERT`) and **roles** (groups of privileges). 3. **Views:** Providing a security mechanism to hide sensitive columns or rows from users. 4. **Encryption:** Scrambling data so it is unreadable without a key. Can be applied to data at rest (on disk) or in transit (over the network). 5. **Auditing:** Tracking and logging database activity to monitor for suspicious behavior and ensure compliance with regulations. 6. **Data Integrity Controls:** Ensuring data is valid through constraints, which is also a security measure."
    },
    {
        "id": 218,
        "question": "What is concurrency control?",
        "answer": "Concurrency control is the set of techniques a DBMS uses to manage simultaneous operations by multiple users or transactions on a database without allowing them to interfere with each other. Its primary goal is to ensure the **isolation** property of transactions, preventing problems like lost updates, dirty reads, and unrepeatable reads. The two main approaches are: 1. **Pessimistic Concurrency Control:** Prevents conflicts by locking data before it is used. This is the most common method. 2. **Optimistic Concurrency Control:** Allows conflicts to occur but detects them at transaction commit time. If a conflict is detected, the transaction is rolled back. It is used in low-contention environments."
    },
    {
        "id": 219,
        "question": "What is database locking?",
        "answer": "Locking is the primary mechanism used for pessimistic concurrency control. A lock is a flag or variable associated with a data item (e.g., a row, a page, a table) that controls how transactions can access it. The basic rules are: *   Before a transaction can read or write a data item, it must first acquire the appropriate lock. *   If a lock is held by another transaction, the requesting transaction must wait. *   Locks are released when the transaction commits or rolls back. The main types are **shared locks** (for reading) and **exclusive locks** (for writing). The lock manager subsystem of the DBMS is responsible for granting and tracking locks."
    },
    {
        "id": 220,
        "question": "What are the key factors affecting database performance?",
        "answer": "Database performance is influenced by factors across the entire system stack: 1. **Database Design:** Poorly designed schemas (lack of normalization, missing indexes, inefficient data types) are a primary cause of performance issues. 2. **Application Design:** Inefficient SQL queries (e.g., `SELECT *`, unnecessary loops, Cartesian products) and poor application logic place unnecessary load on the database. 3. **DBMS Configuration:** Improperly set memory allocation, cache sizes, and parallelism parameters can cripple performance. 4. **Hardware Resources:** Insufficient CPU power, RAM (leading to excessive disk I/O), and slow disk subsystems are common bottlenecks. 5. **Concurrency:** High levels of user contention for the same data can lead to locking and blocking, slowing down all transactions. Tuning requires a holistic approach addressing all these areas."
    },
    {
        "id": 221,
        "question": "What is the difference between a homogeneous and a heterogeneous distributed database?",
        "answer": "This classification depends on the uniformity of the DBMS software across sites: *   **Homogeneous DDBMS:** All sites in the distributed system use the **same** DBMS software (e.g., all run Oracle). The underlying operating system can be different. This is easier to manage, design, and implement because the software is uniform. *   **Heterogeneous DDBMS:** Different sites may use **different** DBMS software (e.g., one site uses Oracle, another uses SQL Server, another uses IMS). This is much more complex. It requires additional middleware (gateways) to translate queries and resolve differences in data models, query languages, and transaction protocols. It is more common in real-world scenarios where different departments have chosen different systems over time."
    },
    {
        "id": 222,
        "question": "What is a distributed database?",
        "answer": "A distributed database is a single logical database that is physically spread across multiple computers (nodes) located in different geographical locations and connected by a network. The key principle is that users can access the data as if it were all stored on their local machine, without needing to know where the data is physically located. The system is managed by a Distributed Database Management System (DDBMS) that provides: 1. **Location Transparency:** Hides the physical location of the data from the user. 2. **Replication Transparency:** Hides the fact that data may be duplicated (replicated) at multiple sites. 3. **Fragmentation Transparency:** Hides the fact that a table may be split (horizontally or vertically) and stored at different sites."
    },
    {
        "id": 223,
        "question": "Explain the difference between horizontal and vertical fragmentation.",
        "answer": "Fragmentation is the technique of breaking a table into smaller pieces (fragments) and distributing them across different sites in a distributed database. *   **Horizontal Fragmentation:** Splits a table by **rows**. Each fragment contains a subset of the table's rows. For example, a `Customer` table could be fragmented so that customers from the East Coast are stored on a server in New York, and customers from the West Coast are stored on a server in California. A predicate (e.g., `Region = 'East'`) defines each fragment. *   **Vertical Fragmentation:** Splits a table by **columns**. Each fragment contains a subset of the table's columns. For example, one fragment could contain `CustomerID`, `Name`, `Address`, while another contains `CustomerID`, `CreditLimit`, `AccountBalance`. The `CustomerID` column is repeated in all fragments to allow reconstruction of the original row."
    },
    {
        "id": 224,
        "question": "What is concurrency transparency in a distributed system?",
        "answer": "Concurrency transparency is a design goal for distributed DBMS where multiple transactions, which may be executing at different sites, are scheduled in such a way that their concurrent execution produces the same final result as if they had been executed one after the other in some serial order (serializability). The user and the application programmer should be unaware of the concurrency and distribution. The DDBMS, specifically the distributed transaction manager, is responsible for ensuring this property across all sites, making the complexity of coordinating distributed locks and schedules invisible to the user."
    },
    {
        "id": 225,
        "question": "What is snapshot replication?",
        "answer": "Snapshot replication is a method of data replication where the entire current state of a published table (or other database object) is copied and distributed to subscribers at a specific point in time. It does not monitor for incremental updates to data. Instead, every time the snapshot is applied, it completely overwrites the previous snapshot at the subscriber. It is best used for: 1. **Data that changes infrequently** (e.g., lookup tables, dimension tables in a data warehouse). 2. **When a large volume of changes occurs** all at once. 3. **When subscribers do not need to have up-to-the-minute data** and can work with data that is refreshed periodically (e.g., nightly, weekly). It is simpler to set up than transactional replication but can place a significant load on the network when transferring large snapshots."
    },
    {
    "id": 226,
    "question": "What is a file-based system and what are its main disadvantages?",
    "answer": "A file-based system is an application program designed to manipulate data files where information is stored in permanent files. Each application program is designed to manipulate specific data files, and new applications are added as needed. The main disadvantages include: 1) Data redundancy - same information kept in several places leading to inconsistency, 2) Data isolation - difficulty retrieving appropriate data stored in various files, 3) Integrity problems - difficulty maintaining data correctness and consistency, 4) Security problems - constraints regarding accessing privileges, and 5) Concurrency access issues - files are locked when opened, preventing multiple users from accessing the same file simultaneously."
    },
    {
        "id": 227,
        "question": "What is the database approach and how does it differ from file-based systems?",
        "answer": "The database approach is a method of managing large amounts of organizational information that was developed to address the difficulties arising from using file-based systems. Unlike file-based systems where each application manages its own data files, the database approach provides a shared collection of related data that supports the activities of a particular organization. Key differences include: concurrency management allowing multiple users access to the same record, reduced data redundancy, improved data integrity, better security controls, and centralized data management."
    },
    {
        "id": 228,
        "question": "What is a Database Management System (DBMS)?",
        "answer": "A Database Management System (DBMS) is a collection of programs that enables users to create and maintain databases and control all access to them. The primary goal of a DBMS is to provide an environment that is both convenient and efficient for users to retrieve and store information. It serves as an interface between the database and users/application programs, ensuring data is organized and accessible while maintaining security and integrity."
    },
    {
        "id": 229,
        "question": "What are the key properties of a database?",
        "answer": "A database has the following properties: 1) It is a representation of some aspect of the real world or a collection of data elements representing real-world information, 2) It is logical, coherent and internally consistent, 3) It is designed, built and populated with data for a specific purpose, 4) Each data item is stored in a field, and 5) A combination of fields makes up a table. A database can contain many tables that are related to each other."
    },
    {
        "id": 230,
        "question": "What are the main characteristics and benefits of a database system?",
        "answer": "The main characteristics and benefits include: 1) Self-describing nature - contains both data and metadata, 2) Insulation between program and data (program-data independence), 3) Support for multiple views of data, 4) Sharing of data and multiuser system support, 5) Control of data redundancy, 6) Data sharing capabilities, 7) Enforcement of integrity constraints, 8) Restriction of unauthorized access, 9) Data independence, 10) Transaction processing capabilities, and 11) Backup and recovery facilities."
    },
    {
        "id": 231,
        "question": "What is data independence and why is it important?",
        "answer": "Data independence is the immunity of user applications to changes made in the definition and organization of data. It is important because it allows changes to the data structure without affecting application programs. There are two types: logical data independence (ability to change logical schema without changing external schema) and physical data independence (immunity of internal model to changes in physical model). This separation ensures that applications continue to work even when database structures are modified."
    },
    {
        "id": 232,
        "question": "What are the different types of data models?",
        "answer": "The main types of data models are: 1) High-level conceptual data models (like Entity Relationship model) that provide concepts close to how people perceive data, 2) Record-based logical data models including relational data models (data as tables/relations), network data models (data as record types with set types), and hierarchical data models (data as hierarchical tree structures)."
    },
    {
        "id": 233,
        "question": "What is data modeling and what is its purpose in database design?",
        "answer": "Data modeling is the first step in the process of database design, sometimes considered a high-level and abstract design phase (conceptual design). The purpose is to describe: the data contained in the database (entities), the relationships between data items, and the constraints on data. It results in a (semi) formal representation of the database structure that is easy to understand and serves as a reference to ensure all user requirements are met before implementation."
    },
    {
        "id": 234,
        "question": "What are the different degrees of data abstraction in database design?",
        "answer": "The degrees of data abstraction include: 1) External models - represent the user's view of the database containing multiple different external views, 2) Conceptual models - provide flexible data-structuring capabilities and present a 'community view' of the entire database, 3) Internal models - closer to physical level representation (relational, network, hierarchical), and 4) Physical models - the physical representation of the database with the lowest level of abstraction dealing with storage details."
    },
    {
        "id": 235,
        "question": "How are database management systems classified?",
        "answer": "DBMS can be classified based on: 1) Data model - relational, hierarchical, network, or object-oriented models, 2) User numbers - single-user or multiuser database systems, and 3) Database distribution - centralized systems (DBMS and database stored at single site), distributed database systems (database distributed across multiple sites), homogeneous distributed systems (same DBMS software), or heterogeneous distributed systems (different DBMS software with common support for data exchange)."
    },
    {
        "id": 236,
        "question": "What is the Relational Data Model and who introduced it?",
        "answer": "The relational data model was introduced by E. F. Codd in 1970. It describes the world as 'a collection of inter-related relations (or tables)' and has provided the basis for research on data theory, numerous database design methodologies, the SQL standard, and almost all modern commercial database management systems."
    },
    {
        "id": 237,
        "question": "What are the key components of the relational data model?",
        "answer": "The key components are: 1) Relation/Table - a subset of the Cartesian product of domains, 2) Tuple/Row - a group of related data values, 3) Attribute/Column/Field - defines the record characteristics, 4) Domain - a set of acceptable values for a column, and 5) Degree - the number of attributes in a table."
    },
    {
        "id": 238,
        "question": "What are the properties of a table in the relational model?",
        "answer": "Properties include: 1) Distinct table name, 2) No duplicate rows, 3) Atomic entries in columns (no repeating groups), 4) Entries from same domain based on data type, 5) No operations combining different data types, 6) Distinct attribute names, 7) Insignificant column sequence, and 8) Insignificant row sequence."
    },
    {
        "id": 239,
        "question": "What is an Entity-Relationship (ER) data model?",
        "answer": "The ER data model is well-suited for database modeling because it is fairly abstract and easy to discuss. It is based on two concepts: entities (tables holding specific information) and relationships (associations between entities). ER models are represented by ER diagrams and are readily translated to relations."
    },
    {
        "id": 240,
        "question": "What are the different types of entities in ER modeling?",
        "answer": "Entity types include: 1) Independent entities (kernels) - backbone of database with primary keys not being foreign keys, 2) Dependent entities - depend on other tables for meaning and connect kernels together, and 3) Characteristic entities - provide more information about another table and represent multivalued attributes."
    },
    {
        "id": 241,
        "question": "What are the different types of attributes in ER modeling?",
        "answer": "Attribute types include: 1) Simple attributes - drawn from atomic value domains (single-valued), 2) Composite attributes - consist of a hierarchy of attributes, 3) Multivalued attributes - have a set of values for each entity, and 4) Derived attributes - contain values calculated from other attributes."
    },
    {
        "id": 242,
        "question": "What are the different types of keys in database systems?",
        "answer": "Key types include: 1) Candidate key - unique and minimal identifier, 2) Primary key - selected candidate key for identifying tuples, 3) Composite key - composed of two or more attributes, 4) Secondary key - used strictly for retrieval, 5) Alternate key - candidate keys not chosen as primary key, and 6) Foreign key - references primary key in another table."
    },
    {
        "id": 243,
        "question": "What are the main types of relationships in ER modeling?",
        "answer": "Relationship types include: 1) One-to-many (1:M) - should be the norm in relational databases, 2) One-to-one (1:1) - should be rare and may indicate entities belong in same table, 3) Many-to-many (M:N) - implemented through composite entities and broken into two 1:M relationships, and 4) Unary/recursive - relationship between occurrences of the same entity set."
    },
    {
        "id": 244,
        "question": "What is relational algebra and what are its fundamental operations?",
        "answer": "Relational algebra is a procedural query language that provides a set of operations to manipulate relations. Fundamental operations include: 1) Selection (σ) - selects rows satisfying a predicate, 2) Projection (π) - selects specific columns, 3) Union (∪) - combines tuples from two relations, 4) Set difference (−) - finds tuples in one relation not in another, 5) Cartesian product (×) - combines all tuples from two relations, and 6) Rename (ρ) - renames relations or attributes."
    },
    {
        "id": 245,
        "question": "What are the additional relational algebra operations derived from fundamental ones?",
        "answer": "Additional operations include: 1) Join (⨝) - combines tuples from two relations based on matching condition (derived from selection and Cartesian product), 2) Natural join - equijoin that automatically matches columns with same names, 3) Outer joins (left, right, full) - preserve tuples with no matching counterparts, 4) Division (÷) - finds all values of one relation that are associated with all values of another, and 5) Intersection (∩) - finds common tuples between two relations."
    },
    {
        "id": 246,
        "question": "What is concurrency control and why is it important in database systems?",
        "answer": "Concurrency control manages simultaneous access to the database by multiple users while maintaining data consistency. It is crucial because without proper control, concurrent transactions can lead to problems like: 1) Lost updates - one transaction overwrites another's changes, 2) Dirty reads - reading uncommitted data, 3) Non-repeatable reads - different values read in same transaction, and 4) Phantom reads - new rows appearing during transaction."
    },
    {
        "id": 247,
        "question": "What are the main concurrency control protocols?",
        "answer": "Main protocols include: 1) Lock-based protocols - use shared and exclusive locks to control access, 2) Two-phase locking (2PL) - growing phase (acquiring locks) and shrinking phase (releasing locks), 3) Timestamp-based protocols - order transactions based on timestamps, 4) Multi-version concurrency control (MVCC) - maintain multiple versions of data items, and 5) Optimistic concurrency control - assume conflicts are rare and check at commit time."
    },
    {
        "id": 248,
        "question": "What are database transactions and what are the ACID properties?",
        "answer": "A transaction is a logical unit of work that contains one or more SQL statements. ACID properties ensure transaction reliability: 1) Atomicity - all or nothing execution, 2) Consistency - preserves database constraints, 3) Isolation - concurrent transactions don't interfere, and 4) Durability - committed changes persist despite failures."
    },
    {
        "id": 249,
        "question": "What is transaction scheduling and what are conflict serializability and view serializability?",
        "answer": "Transaction scheduling determines the order of operation execution. Conflict serializability requires that conflicting operations (read-write, write-read, write-write) of different transactions appear in the same order. View serializability is less strict and allows schedules where transactions see the same data views as some serial execution, even if conflict orders differ."
    },
    {
        "id": 250,
        "question": "What are the different transaction isolation levels in SQL?",
        "answer": "SQL isolation levels include: 1) Read Uncommitted - allows dirty reads, 2) Read Committed - prevents dirty reads but allows non-repeatable reads, 3) Repeatable Read - prevents dirty and non-repeatable reads but allows phantom reads, and 4) Serializable - prevents all concurrency problems but reduces performance. Each level offers different trade-offs between consistency and concurrency."
    },
    {
        "id": 251,
        "question": "What is database recovery and what techniques are used?",
        "answer": "Database recovery restores the database to a consistent state after failures. Techniques include: 1) Log-based recovery - maintains audit trail of changes, 2) Write-ahead logging (WAL) - log records must be written before actual data updates, 3) Checkpoints - periodic saving of database state, 4) Shadow paging - maintains two page tables during transactions, and 5) ARIES algorithm - widely used recovery algorithm that uses log analysis and redo/undo phases."
    },
    {
        "id": 252,
        "question": "What are deadlocks in database systems and how are they handled?",
        "answer": "Deadlocks occur when two or more transactions are waiting for each other to release locks, creating a circular wait. Handling methods include: 1) Prevention - ensuring deadlocks cannot occur through ordering or timeout mechanisms, 2) Avoidance - using resource allocation graphs and banker's algorithm, 3) Detection - using wait-for graphs to identify cycles, and 4) Recovery - aborting one or more transactions to break the deadlock, with victim selection based on factors like transaction age or work completed."
    },
    {
        "id": 253,
        "question": "What is query optimization and what are the main approaches?",
        "answer": "Query optimization is the process of selecting the most efficient execution strategy for a query. Main approaches include: 1) Rule-based optimization - uses heuristic rules to transform queries, 2) Cost-based optimization - estimates costs of different execution plans using statistics, 3) Semantic optimization - uses constraints and semantics to simplify queries, and 4) Physical optimization - considers storage structures and access methods. The optimizer considers factors like join order, index usage, and access methods."
    },
    {
        "id": 254,
        "question": "What are database indices and what types exist?",
        "answer": "Indices are data structures that improve data retrieval speed. Types include: 1) B-tree indices - balanced tree structure for range queries, 2) Hash indices - for equality queries using hash functions, 3) Bitmap indices - for columns with few distinct values, 4) Clustered indices - determine physical storage order of data, 5) Non-clustered indices - separate structure from data storage, and 6) Composite indices - on multiple columns. Indices trade off query performance against update overhead and storage space."
    },
    {
        "id": 255,
        "question": "What is normalization and what are the normal forms?",
        "answer": "Normalization is the process of organizing data to reduce redundancy and improve data integrity. The normal forms are: 1) 1NF - eliminate repeating groups, ensure atomic values, 2) 2NF - remove partial dependencies (all non-key attributes fully dependent on PK), 3) 3NF - remove transitive dependencies (no non-key attribute dependent on another non-key attribute), 4) BCNF - every determinant is a candidate key, 5) 4NF - remove multi-valued dependencies, and 6) 5NF - remove join dependencies. Each normal form addresses specific types of redundancy and update anomalies."
    }

]