[
    {
        "id": 1,
        "question": "What is a database in the context of software systems?",
        "answer": "A database is an organized, persistent collection of logically related data designed to meet the information needs of an organization. It represents some aspect of the real world (the miniworld) and is built to be shared by multiple users and applications. Unlike a simple file system, a database minimizes redundancy and allows for efficient querying and data management. For example, a social media platform's database stores user profiles, posts, comments, and connections in separate but interrelated tables, enabling complex features like news feeds and search."
    },
    {
        "id": 2,
        "question": "What is a DBMS and what are its primary functions?",
        "answer": "A Database Management System (DBMS) is a software suite that provides a systematic and scalable way to create, retrieve, update, and manage data in a database. It acts as an intermediary between the database and end-users/application programs. Its core functions include: data storage and retrieval, concurrency control to manage simultaneous access, security and authorization, integrity enforcement to ensure data accuracy, and backup and recovery. Examples include MySQL, PostgreSQL, Oracle, and MongoDB."
    },
    {
        "id": 3,
        "question": "How does a Database System differ from just a database?",
        "answer": "The term 'Database System' or 'Database Management System (DBMS)' refers to the entire software application, which includes both the database itself (the stored data) and the DBMS software that manipulates it. It's the complete ecosystem: the data, the hardware it resides on, the software to manage it, and the personnel who use and administer it. You can't have a functional database system without the DBMS software."
    },
    {
        "id": 4,
        "question": "What are the key advantages of using a DBMS over traditional file systems?",
        "answer": "DBMS offers significant advantages over file processing systems: 1. **Reduced Data Redundancy:** Data is stored in one place, minimizing duplication. 2. **Improved Data Integrity:** Rules and constraints ensure data is accurate and consistent. 3. **Enhanced Data Sharing and Security:** Multiple users can access data concurrently with controlled permissions. 4. **Data Independence:** Application programs are insulated from changes in how the data is stored. 5. **Powerful Data Retrieval:** SQL provides a efficient, standardized language for complex queries. 6. **Backup and Recovery:** Built-in mechanisms protect data from system failures."
    },
    {
        "id": 5,
        "question": "What were the main limitations of traditional file processing systems that led to the DBMS?",
        "answer": "File processing systems suffered from several critical flaws: 1. **Data Redundancy and Inconsistency:** The same data was often stored in multiple files, leading to wasted space and potential inconsistencies. 2. **Difficulty in Accessing Data:** Writing new queries to extract specific data was often complex and required deep knowledge of the file structure. 3. **Data Isolation:** Data was scattered across various files in different formats, making it hard to get a unified view. 4. **Integrity Problems:** It was hard to apply and enforce business rules (e.g., 'account balance > 0') across the entire system. 5. **Atomicity Problems:** Difficulty ensuring operations like bank transfers completed entirely or not at all. 6. **Concurrent Access Anomalies:** Uncontrolled simultaneous access could lead to incorrect data updates."
    },
    {
        "id": 6,
        "question": "Can you describe the three-schema architecture for data abstraction in DBMS?",
        "answer": "The three-schema architecture defines different levels of abstraction to achieve data independence: 1. **Internal Level (Physical Schema):** Describes *how* the data is physically stored on the storage device (files, indices, storage structures). 2. **Conceptual Level (Logical Schema):** Describes *what* data is stored and the relationships between them. It defines the structure of the entire database for the community of users (entities, attributes, relationships, constraints). 3. **External Level (View Schema):** Describes the data as it is seen by specific end-user groups or applications. It is a tailored subset of the conceptual schema, hiding irrelevant data."
    },
    {
        "id": 7,
        "question": "What are the two primary integrity rules in the relational model?",
        "answer": "The relational model is governed by two key integrity rules: 1. **Entity Integrity:** This rule states that no attribute that is part of the primary key in a base relation (table) can have a NULL value. This ensures every row can be uniquely identified. 2. **Referential Integrity:** This rule states that if a relation has a foreign key, then every value of that foreign key must either be NULL or must match the value of the primary key in the relation it references. This maintains consistency between related tables."
    },
    {
        "id": 8,
        "question": "What is the difference between the intension and extension of a database?",
        "answer": "This distinction separates the database's blueprint from its current state: *   **Intension (Database Schema):** This is the constant, logical design of the database. It's the overall structure, including table definitions, attributes, data types, constraints, and relationships. It changes infrequently, only when the design is modified. *   **Extension (Database Instance):** This is the dynamic, time-dependent collection of data stored in the database at a particular moment. It's the set of all tuples (rows) in all the tables. The extension changes every time data is inserted, updated, or deleted."
    },
    {
        "id": 9,
        "question": "What was System R and why was it historically significant?",
        "answer": "System R was a groundbreaking research project at IBM's San Jose Research Laboratory in the 1970s. It was one of the first systems to demonstrate that a relational database management system could be implemented with practical performance. Its two major subsystems were: 1. **Research Storage (RS):** Managed the low-level storage of data on disk. 2. **System Relational Data System (RDS):** Provided the higher-level relational interface, including the pioneering SQL query language (then called SEQUEL). System R proved the viability of relational databases and directly influenced the development of commercial RDBMS like IBM's DB2 and SQL/DS."
    },
    {
        "id": 10,
        "question": "How did the data model of System R differ from the pure relational model defined by Codd?",
        "answer": "As a pioneering prototype, System R did not implement the full relational model as originally defined: 1. **No Domain Support:** It did not support the concept of domains (distinct data types with constraints), using simple data types instead. 2. **Optional Uniqueness:** Enforcement of candidate key uniqueness was not mandatory. 3. **Optional Entity Integrity:** The rule that primary keys cannot be null was not strictly enforced. 4. **No Referential Integrity:** The system did not automatically enforce foreign key constraints. These simplifications were practical choices for the initial implementation but were addressed in later commercial systems."
    },
    {
        "id": 11,
        "question": "What is meant by data independence in a DBMS?",
        "answer": "Data independence is the immunity of application programs to changes in the definition and organization of data. It is a fundamental benefit of the three-schema architecture. There are two types: 1. **Physical Data Independence:** The ability to modify the physical schema (storage structures, indexing) without needing to change the logical or conceptual schema. Applications are shielded from changes like switching from one storage engine to another. 2. **Logical Data Independence:** The ability to modify the conceptual schema (adding a new column, splitting a table) without having to change existing external schemas (views) or application programs, as long as the data they use remains available."
    },
    {
        "id": 12,
        "question": "What is a database view and how does it promote data independence?",
        "answer": "A view is a virtual table derived from one or more base tables. It does not store data itself but represents a stored query. Views are crucial for logical data independence because they create an abstraction layer. Users and applications can interact with views instead of base tables. If the underlying base tables need to be restructured (e.g., a table is split), the view's definition can be updated to reconstruct the original virtual table for the user. This means the application's interface to the data remains unchanged, insulating it from changes in the database's logical structure."
    },
    {
        "id": 13,
        "question": "What is a data model?",
        "answer": "A data model is a collection of conceptual tools used to describe the structure of a database. It provides a framework for defining data elements, their relationships, the semantics (meaning) of the data, and the consistency constraints that apply to the data. Think of it as a blueprint. Common data models include the Relational Model (tables), Entity-Relationship Model (entities and relationships), and Document Model (JSON-like documents)."
    },
    {
        "id": 14,
        "question": "How does the Entity-Relationship (E-R) Model represent data?",
        "answer": "The Entity-Relationship Model is a high-level, conceptual data model based on perceiving the real world as a set of basic objects called **entities** and the **relationships** among these objects. *   **Entities:** Represent real-world objects (e.g., a `Student`, a `Course`, an `Employee`). *   **Attributes:** Properties that describe an entity (e.g., `StudentID`, `CourseName`, `Salary`). *   **Relationships:** Associations between entities (e.g., a Student *is enrolled in* a Course). It is primarily used for database design to visually map out requirements before implementation."
    },
    {
        "id": 15,
        "question": "What are the core concepts of the Object-Oriented Data Model?",
        "answer": "The Object-Oriented Data Model represents data as **objects**, similar to those in object-oriented programming. Its core concepts are: 1. **Objects:** Contain both data (in **instance variables** or attributes) and behavior (**methods** or functions that operate on the data). 2. **Classes:** A collection of similar objects that share the same attributes and methods. A class is a blueprint for creating objects. 3. **Inheritance:** A class (subclass) can inherit attributes and methods from a more general class (superclass), promoting code reusability. This model is useful for representing complex data with rich behavior, like in CAD or multimedia systems."
    },
    {
        "id": 16,
        "question": "In database design, what is an entity?",
        "answer": "An entity is a distinct, identifiable object or concept in the real world that we want to store information about. It has an independent existence and can be uniquely identified. Entities are represented as tables in a relational database. *   **Example:** In a hospital management system, `PATIENT`, `DOCTOR`, `APPOINTMENT`, and `MEDICATION` could all be entities."
    },
    {
        "id": 17,
        "question": "What defines an entity type?",
        "answer": "An entity type defines a category or a blueprint for a set of entities that share the same properties or attributes. It is the structure or schema. *   **Example:** The entity type `CAR` defines the structure for all car entities. Its attributes could be `VIN`, `Make`, `Model`, `Year`, and `Color`. Every individual car (e.g., the specific car with VIN '123ABC') is an instance of the `CAR` entity type."
    },
    {
        "id": 18,
        "question": "How is an entity set different from an entity type?",
        "answer": "This is the critical distinction between a *definition* and a *collection*: *   **Entity Type:** The *definition* or category (e.g., `STUDENT`). *   **Entity Set:** The actual *collection* of all entities of a particular type that exist in the database at a given time (e.g., the set of all student rows in the STUDENT table: {Alice, Bob, Charlie...}). The entity set is the extension of the entity type."
    },
    {
        "id": 19,
        "question": "What is the extension of an entity type?",
        "answer": "The extension of an entity type is simply another name for the **entity set**—the collection of all entities of that specific type present in the database at a specific moment in time. It is dynamic and changes as data is added or removed. If `PRODUCT` is the entity type, the extension is the complete, current list of all products in the database."
    },
    {
        "id": 20,
        "question": "What characterizes a weak entity set?",
        "answer": "A weak entity set is an entity set that does not have a sufficient set of attributes to form a primary key on its own. Its existence is dependent on another entity, called the **owner** or **identifying** entity. It must relate to the owner entity via a **identifying relationship**. The primary key of a weak entity is formed by combining its **partial key** (a discriminator) with the primary key of its owner entity. *   **Example:** An `DEPENDENT` entity (e.g., a child) for an `EMPLOYEE`. `DependentName` might be a partial key, but the full primary key for `DEPENDENT` would be {`EmployeeID` (from EMPLOYEE), `DependentName`}."
    },
    {
        "id": 21,
        "question": "What is an attribute in the context of a database entity?",
        "answer": "An attribute is a defining property or characteristic of an entity or a relationship. It describes the details we want to store about an entity. Each attribute has a name and a domain, which defines the set of possible values it can hold (e.g., INTEGER, VARCHAR(100), DATE). *   **Example:** For an entity type `PRODUCT`, possible attributes include `ProductID`, `Name`, `Description`, `Price`, and `StockQuantity`."
    },
    {
        "id": 22,
        "question": "Can you explain the difference between a relation schema and a relation instance?",
        "answer": "This is the structure vs. content distinction for a table: *   **Relation Schema:** This is the table's structure or blueprint. It is defined by the relation name and a fixed set of attributes. It is static and rarely changes. For example: `Student(StudentID, Name, Major, GPA)`. *   **Relation Instance:** This is the actual set of data (the rows or tuples) that populates the table at a given moment. It is dynamic and changes frequently with data manipulation operations (INSERT, UPDATE, DELETE). The instance is the current 'state' of the relation."
    },
    {
        "id": 23,
        "question": "What is meant by the 'degree' of a relation?",
        "answer": "The degree of a relation is the number of attributes (columns) in its relation schema. It's a measure of how 'wide' the table is. *   **Example:** A relation schema `Customer(CustomerID, FirstName, LastName, Email)` has a degree of 4. A relation schema `LogEntry(Timestamp, Message)` has a degree of 2."
    },
    {
        "id": 24,
        "question": "In data modeling, what is a relationship?",
        "answer": "A relationship is an association between two or more entities. It represents a business rule or a interaction between the entities. Relationships are crucial for connecting data across different tables. *   **Example:** In a library database, there is a relationship between the `Member` entity and the `Book` entity. This relationship, which we might call `BORROWS`, records which member has borrowed which book."
    },
    {
        "id": 25,
        "question": "What is a relationship set?",
        "answer": "A relationship set is a collection of relationships of the same type. It groups together all the individual instances of a specific relationship that exist in the database. *   **Example:** If the relationship is `WORKS_FOR` between `Employee` and `Department`, then the relationship set is the complete list of all (Employee, Department) pairs that currently exist, such as {(Alice, Sales), (Bob, Engineering), (Charlie, Sales)}."
    },
    {
        "id": 26,
        "question": "How is a relationship type defined?",
        "answer": "A relationship type defines the nature of the association between entity types. It is the schema for a relationship. It specifies the entity types that participate in the relationship and the name of the relationship itself. *   **Example:** The relationship type `Enrollment` defines an association between the `Student` entity type and the `Course` entity type. It establishes that students can be enrolled in courses."
    },
    {
        "id": 27,
        "question": "What is the degree of a relationship type?",
        "answer": "The degree of a relationship type is the number of entity types that participate in the relationship. *   **Binary Relationship:** Degree 2. This is the most common type (e.g., `Employee` WORKS_FOR `Department`). *   **Ternary Relationship:** Degree 3 (e.g., a `Doctor` prescribes a `Medication` to a `Patient`). *   **N-ary Relationship:** Degree n, for relationships involving more than two entity types."
    },
    {
        "id": 28,
        "question": "What is the purpose of the Data Definition Language (DDL)?",
        "answer": "Data Definition Language (DDL) is a subset of SQL used to define, modify, and delete the structure of database objects, but not the data within them. It is used by database designers and administrators to create the database's schema. Common DDL commands are `CREATE` (to create tables, indexes, views), `ALTER` (to modify existing structures), and `DROP` (to delete objects). DDL statements implicitly commit transactions."
    },
    {
        "id": 29,
        "question": "What role does a View Definition Language (VDL) play?",
        "answer": "View Definition Language (VDL) is the component of a DBMS used to specify user views and their mappings to the conceptual schema. In modern SQL-based systems, this functionality is integrated into the standard DDL using the `CREATE VIEW` statement. This statement allows the definition of a virtual table (a view) as a query on other base tables or views, effectively customizing how different users perceive the database."
    },
    {
        "id": 30,
        "question": "What is the function of the Storage Definition Language (SDL)?",
        "answer": "Storage Definition Language (SDL) is used to specify the internal schema of the database. It defines how the data is physically stored on the storage device. This includes specifying file organizations, indexing techniques, storage media, and the mapping between the conceptual and internal levels. SDL is typically used by database administrators to tune the database for performance and is often hidden from regular users and application developers."
    },
    {
        "id": 31,
        "question": "How does Data Storage-Definition Language relate to physical storage?",
        "answer": "Data Storage-Definition Language is a specific type of DDL focused exclusively on the physical layer. It is used to define the low-level storage structures and access methods that the database system will use. This includes commands to create table spaces, data files, and to control physical properties like page size, buffer pools, and encryption at rest. It provides the crucial link between the logical data model and its physical implementation on disk."
    },
    {
        "id": 32,
        "question": "What are the two main types of Data Manipulation Language (DML)?",
        "answer": "DML is used for managing data within database objects. The two main types are: 1. **Procedural DML (Low-Level):** Requires the user to specify *what* data is needed and *how* to get it. The user must express the logic to navigate the database (e.g., using loops and pointers). Relational Algebra is a procedural language. 2. **Non-Procedural DML (High-Level/Declarative):** Requires the user to specify *only what* data is needed, without describing how to retrieve it. The DBMS's query optimizer determines the most efficient execution path. SQL's `SELECT`, `INSERT`, `UPDATE`, `DELETE` are non-procedural."
    },
    {
        "id": 33,
        "question": "What is the role of a DML Compiler?",
        "answer": "The DML Compiler is a crucial component of the DBMS query processing engine. Its primary role is to translate high-level, non-procedural DML statements (like SQL queries) into a sequence of low-level instructions (often called a query plan or access plan). These low-level instructions are in a form that the query evaluation engine can understand and execute efficiently against the physical storage system."
    },
    {
        "id": 34,
        "question": "What does the Query Evaluation Engine do?",
        "answer": "The Query Evaluation Engine is the component that executes the low-level instructions generated by the DML compiler. It is responsible for carrying out the actual operations required to fulfill a data request. This includes reading data from storage, performing operations like sorting and joining, applying filters, and returning the final result set to the user or application. It interacts directly with the storage manager."
    },
    {
        "id": 35,
        "question": "What is the function of a DDL Interpreter?",
        "answer": "The DDL Interpreter processes Data Definition Language (DDL) statements. It parses DDL commands (like `CREATE TABLE`, `ALTER VIEW`) and executes them. Its key function is to record the definitions of database objects (their metadata) in the system catalog or data dictionary. This metadata includes table schemas, constraints, indexes, and privileges, which is essential for the DBMS to understand and manage the database structure."
    },
    {
        "id": 36,
        "question": "What is a Record-at-a-time DML operation?",
        "answer": "Record-at-a-time DML is a low-level, procedural style of data manipulation where operations are performed on a single record (row) at a time. The application code must typically open a cursor, loop through a set of records, and process each one individually. This approach gives the programmer fine-grained control but is more complex and less efficient for set-based operations. It is characteristic of navigational database models and older programming interfaces."
    },
    {
        "id": 37,
        "question": "What is a Set-at-a-time DML operation?",
        "answer": "Set-at-a-time DML is a high-level, declarative style of data manipulation where operations are performed on entire sets of records simultaneously. A single DML statement (like a SQL `UPDATE` or `DELETE` with a `WHERE` clause) can identify and modify multiple rows in one go. This is a fundamental strength of the relational model, leading to more concise code and allowing the DBMS to optimize execution for high performance."
    },
    {
        "id": 38,
        "question": "What is Relational Algebra?",
        "answer": "Relational Algebra is a formal, procedural query language for the relational model. It provides a set of operations that take one or two relations (tables) as input and produce a new relation as output. Its operations form the theoretical foundation for SQL. Core operations include **Select (σ)** to filter rows, **Project (π)** to select columns, **Union (∪)**, **Set Difference (-)**, **Cartesian Product (×)**, and various **Join** operations. It specifies *how* a query should be executed."
    },
    {
        "id": 39,
        "question": "What is Relational Calculus?",
        "answer": "Relational Calculus is a formal, non-procedural query language for the relational model. It is based on first-order predicate logic. Instead of specifying *how* to retrieve data, it describes *what* data is desired by stating the desired properties of the result set. There are two variants: Tuple Relational Calculus and Domain Relational Calculus. SQL's `SELECT...WHERE` clause is heavily influenced by relational calculus, as you declare the conditions the result must satisfy."
    },
    {
        "id": 40,
        "question": "What is the key difference between Tuple and Domain Relational Calculus?",
        "answer": "The key difference lies in what the variables in the formulas represent: *   **Tuple Relational Calculus (TRC):** Variables range over tuples (rows) from relations. A query specifies the tuples we want based on a condition involving their attribute values. It is closer to how SQL is written. *   **Domain Relational Calculus (DRC):** Variables range over values from the domains of attributes. A query specifies the constraints on the values we want to appear in the result. It operates at a more atomic level than TRC."
    },
    {
        "id": 41,
        "question": "What is database normalization and what are its goals?",
        "answer": "Normalization is a systematic process of decomposing (breaking down) complex database tables into smaller, simpler tables to eliminate data redundancy and avoid data anomalies. The primary goals are: 1. **Minimize Data Redundancy:** Store each piece of data only once to save space and prevent update anomalies. 2. **Eliminate Anomalies:** Prevent inconsistencies that can occur during data insertion, deletion, and updating. 3. **Simplify Data Integrity:** Make it easier to enforce integrity constraints. The process involves analyzing tables based on their functional dependencies and primary keys."
    },
    {
        "id": 42,
        "question": "What is a Functional Dependency (FD) in a database?",
        "answer": "A Functional Dependency is a constraint between two sets of attributes in a relation. It is denoted as X → Y, meaning that the set of attributes X functionally determines the set of attributes Y. This implies that for any two tuples (rows) in the relation, if they have the same values for X, they must also have the same values for Y. X is called the determinant. Functional dependencies are derived from the real-world meaning of the data and are fundamental to the process of normalization."
    },
    {
        "id": 43,
        "question": "What is the Lossless Join (Non-Additive Join) property in decomposition?",
        "answer": "The Lossless Join property is a critical characteristic of a valid database decomposition. It guarantees that when the decomposed relations (tables) are joined back together using a natural join operation, the result is exactly the original relation—no more and no fewer tuples. A decomposition that does not have this property is considered faulty because it creates spurious (fake) tuples that were not in the original data, leading to incorrect information."
    },
    {
        "id": 44,
        "question": "What defines the First Normal Form (1NF)?",
        "answer": "A relation is in First Normal Form (1NF) if and only if every attribute (column) contains only atomic (indivisible) values. This means: 1. **No Multi-Valued Attributes:** Each cell must contain a single value, not a list or set of values. 2. **No Composite Attributes:** Attributes should not be broken down into smaller sub-parts within the same column. 3. **A Fixed Set of Columns:** All rows must have the same number of columns. 1NF is the most basic requirement for a relational table."
    },
    {
        "id": 45,
        "question": "What is a Full Functional Dependency?",
        "answer": "A functional dependency X → Y is a **full functional dependency** if the removal of any attribute A from the determinant X means that the dependency no longer holds. In other words, Y is functionally dependent on the entire key X, and not on any proper subset of X. This concept is central to the definition of Second Normal Form (2NF). *   **Example:** In {StudentID, CourseID} → Grade, if Grade depends on both the student and the course (i.e., you can't determine the grade with just StudentID or just CourseID), then it is a full functional dependency."
    },
    {
        "id": 46,
        "question": "What are the conditions for a relation to be in Second Normal Form (2NF)?",
        "answer": "A relation is in Second Normal Form (2NF) if it meets two criteria: 1. It is already in First Normal Form (1NF). 2. **No Partial Dependency:** Every non-prime attribute (an attribute not part of any candidate key) must be fully functionally dependent on the entire primary key. This means no non-prime attribute should be dependent on only a part of a composite primary key. Relations with a single-column primary key are automatically in 2NF if they are in 1NF."
    },
    {
        "id": 47,
        "question": "What are the conditions for a relation to be in Third Normal Form (3NF)?",
        "answer": "A relation is in Third Normal Form (3NF) if it meets two criteria: 1. It is in Second Normal Form (2NF). 2. **No Transitive Dependency:** No non-prime attribute is transitively dependent on the primary key. Transitive dependency occurs when a non-prime attribute depends on another non-prime attribute, which in turn depends on the primary key (e.g., PK → A → B). In 3NF, non-prime attributes must depend directly on the primary key. A common definition states that for every functional dependency X → A, either X is a superkey or A is a prime attribute."
    },
    {
        "id": 48,
        "question": "What is Boyce-Codd Normal Form (BCNF) and how is it stronger than 3NF?",
        "answer": "Boyce-Codd Normal Form (BCNF) is a stronger version of 3NF. A relation is in BCNF if for every non-trivial functional dependency X → Y, the determinant X must be a superkey (a superset of a candidate key). BCNF addresses rare anomalies that can remain in a 3NF relation when there are multiple overlapping candidate keys. In simpler terms, BCNF ensures that the only determinants (things on the left-hand side of a functional dependency) in the table are candidate keys. Every relation in BCNF is also in 3NF, but not vice-versa."
    },
    {
        "id": 49,
        "question": "What is Fourth Normal Form (4NF) and what problem does it solve?",
        "answer": "Fourth Normal Form (4NF) deals with dependencies beyond functional dependencies, specifically Multi-Valued Dependencies (MVDs). A relation is in 4NF if it is in BCNF and for every non-trivial multi-valued dependency X →→ Y, X must be a superkey. A multi-valued dependency exists when for a single value of X, there is a set of values for Y, and this set is independent of other attributes. 4NF eliminates redundancy caused by independent multi-valued facts about an entity. For example, it separates a table storing employees, their children, and their skills into two separate tables."
    },
    {
        "id": 50,
        "question": "What is Fifth Normal Form (5NF) or Project-Join Normal Form (PJNF)?",
        "answer": "Fifth Normal Form (5NF) or Project-Join Normal Form (PJNF) is the highest level of normalization based on join dependencies. A relation is in 5NF if it is in 4NF and every join dependency in the relation is implied by its candidate keys. A join dependency means that the relation can be recreated by joining its projections (subsets of columns) without losing information. 5NF deals with very complex, subtle cases of redundancy that are unlikely to be intentionally designed into a schema. It is more of theoretical interest than practical use."
    },
    {
        "id": 51,
        "question": "What is Domain-Key Normal Form (DKNF)?",
        "answer": "Domain-Key Normal Form (DKNF) is a theoretical ideal normal form. A relation is in DKNF if every constraint on the relation is a logical consequence of the definitions of its domains and keys. Constraints include functional dependencies, multi-valued dependencies, and join dependencies. DKNF ensures that no insertion or deletion anomalies exist. Achieving DKNF is often not practical as it requires all business rules to be expressed solely through domain and key constraints. It represents a 'perfect' database design."
    },
    {
        "id": 52,
        "question": "Can you explain different types of keys used in database design?",
        "answer": "Various keys serve different purposes in uniquely identifying and linking data: 1. **Partial Key:** A set of attributes that uniquely identifies a weak entity relative to its owner entity. Also called a discriminator. 2. **Alternate Key:** A candidate key that is not chosen as the primary key. A table can have multiple alternate keys. 3. **Artificial Key (Surrogate Key):** A system-generated, meaningless numeric identifier (e.g., an auto-increment number) created solely to act as the primary key. It has no business meaning. 4. **Compound Key (Composite Key):** A primary key that consists of two or more attributes. 5. **Natural Key:** A candidate key that has business meaning and is used to identify an entity in the real world (e.g., SocialSecurityNumber, ISBN)."
    },
    {
        "id": 53,
        "question": "What is database indexing and what are common types of indexes?",
        "answer": "Indexing is a database optimization technique that creates a separate, smaller data structure (an index) to allow faster retrieval of records from a table. An index works like a book's index, providing a sorted list of values and pointers to their location in the table. Common types include: 1. **B-Tree Index:** The most common type, efficient for equality and range queries. 2. **Hash Index:** Excellent for exact-match queries but useless for ranges. 3. **Bitmap Index:** Ideal for columns with a low cardinality (few distinct values), like gender or status flags. 4. **Clustered Index:** Determines the physical order of data storage in a table. A table can have only one. 5. **Non-Clustered Index:** Creates a separate sorted structure with pointers to the data. A table can have many."
    },
    {
        "id": 54,
        "question": "What is the system catalog and what is its common name?",
        "answer": "The system catalog is a collection of special tables within a database that contain metadata, which is 'data about the data'. It stores comprehensive information about every database object, including tables, columns, data types, constraints, indexes, views, privileges, and users. This catalog is maintained automatically by the DBMS itself. It is more commonly known as the **Data Dictionary**. The DBMS constantly consults the data dictionary to parse queries, enforce constraints, and manage security."
    },
    {
        "id": 55,
        "question": "What is query optimization in a DBMS?",
        "answer": "Query optimization is the process performed by the DBMS where it analyzes a declarative query (like a SQL `SELECT` statement) and selects the most efficient execution plan (or query plan) from among many possible alternatives. The goal is to minimize the total estimated cost of executing the query, which is typically measured in terms of disk I/O, CPU usage, and memory consumption. The optimizer uses statistics about the data (e.g., table sizes, index availability) and sophisticated algorithms to choose the best way to access and join tables."
    },
    {
        "id": 56,
        "question": "What does the 'Durability' property of a transaction mean?",
        "answer": "Durability is the 'D' in the ACID properties of a transaction. It guarantees that once a transaction has been committed, its effects are permanent and will persist even in the event of a system failure (e.g., power outage, crash). The DBMS ensures durability typically by writing the transaction's changes to non-volatile storage (like a hard disk or SSD) in a transaction log *before* the commit operation is reported as successful to the user. After a crash, the DBMS uses this log to recover and restore all committed transactions."
    },
    {
        "id": 57,
        "question": "What is the difference between Atomicity and Aggregation in databases?",
        "answer": "These are two distinct concepts: *   **Atomicity:** This is the 'A' in ACID. It is a transaction property that ensures a transaction is treated as an indivisible unit of work. It must execute entirely ('all') or not at all ('nothing'). If any part of the transaction fails, the entire transaction is rolled back, leaving the database unchanged. *   **Aggregation:** This is a conceptual data modeling concept. It represents a relationship between a relationship and an entity (or between multiple relationships). It is used to model a 'has-a' relationship where one entity is composed of others, or when a relationship itself has attributes that need to be tracked."
    },
    {
        "id": 58,
        "question": "What is a Phantom Deadlock in distributed systems?",
        "answer": "A Phantom Deadlock is a false positive in deadlock detection that can occur in distributed database systems. It happens due to delays in propagating local state information (like lock ownership and wait-for graphs) across different nodes in the network. The distributed deadlock detection algorithm might incorrectly infer a cycle in the global wait-for graph based on outdated information, identifying a deadlock that does not actually exist. This can lead to the unnecessary aborting of a transaction."
    },
    {
        "id": 59,
        "question": "What is a database checkpoint and why is it important?",
        "answer": "A checkpoint is a mechanism where the DBMS periodically forces all modified buffers (dirty pages) in memory to be written to disk. It creates a consistent snapshot of the database state at a point in time. Checkpoints are crucial for recovery: during restart after a crash, the DBMS only needs to redo (replay) transactions committed after the last checkpoint and undo (roll back) transactions that were active at the time of the crash. This significantly reduces recovery time compared to processing the entire transaction log."
    },
    {
        "id": 60,
        "question": "What are the phases of transaction recovery?",
        "answer": "After a system crash, the DBMS goes through a recovery process with distinct phases: 1. **Analysis Phase:** The transaction log is scanned to identify the state of all transactions at the time of the crash. It determines which transactions need to be redone (committed) and which need to be undone (not committed). 2. **Redo Phase:** This phase reapplies all the updates of committed transactions. It starts from the oldest log record of a transaction that was not yet written to disk at the time of the crash and moves forward, ensuring all committed changes are durable. 3. **Undo Phase:** This phase rolls back (reverses) the updates of any transactions that were active but not committed at the time of the crash, restoring the database to a consistent state."
    },
    {
        "id": 61,
        "question": "What is a flat file database?",
        "answer": "A flat file database is a simple database that stores all its data in a single table, often in a plain text file (like a CSV or TSV file). Unlike relational databases, it lacks programmatic access languages (like SQL) and has no capability to establish relationships or enforce referential integrity between different files. While user-friendly for very small, simple datasets, it suffers severely from data redundancy, inconsistency, and the other limitations of file processing systems as the data grows."
    },
    {
        "id": 62,
        "question": "What is a transparent DBMS?",
        "answer": "A transparent DBMS is one that hides its physical storage details and internal implementation complexities from the users and application programs. Users interact with the database through a logical, high-level interface (like SQL) without needing to know how or where the data is physically stored, what indexes are used, or how queries are optimized. This transparency is a key benefit of the DBMS, providing physical and logical data independence."
    },
    {
        "id": 63,
        "question": "What is a database query?",
        "answer": "A query is a request for data or for manipulation of data from a database. It is a user command, written in a database query language, that instructs the DBMS to perform a specific operation. Queries can be used to retrieve, insert, update, or delete data. The most common standard is the Structured Query Language (SQL), which includes Data Manipulation Language (DML) commands like `SELECT`, `INSERT`, `UPDATE`, and `DELETE`."
    },
    {
        "id": 64,
        "question": "What is a correlated subquery in SQL?",
        "answer": "A correlated subquery is an inner subquery that is executed repeatedly, once for each row processed by the outer main query. It is 'correlated' because it references a column from the outer query within its `WHERE` clause. The result of the inner query depends on the value of the current row being evaluated in the outer query. This is different from a regular (non-correlated) subquery, which is executed only once, independently of the outer query. Correlated subqueries can be less performant than joins but are powerful for solving certain types of problems."
    },
    {
        "id": 65,
        "question": "What are the primitive operations common to all record management systems?",
        "answer": "At their most fundamental level, all record management systems, from simple flat files to complex DBMS, must support three basic primitive operations on data: 1. **Addition:** Inserting new records into the dataset. 2. **Deletion:** Removing existing records from the dataset. 3. **Modification (Update):** Changing the values within existing records. All other complex operations, like querying and reporting, are built upon these three core actions."
    },
    {
        "id": 66,
        "question": "What are the unary operations in Relational Algebra?",
        "answer": "Unary operations are those that operate on a single relation (table). The two fundamental unary operations in Relational Algebra are: 1. **Selection (σ):** This operation filters rows from a relation based on a given condition or predicate. It chooses a horizontal subset of a table. 2. **Projection (π):** This operation selects specific columns from a relation, eliminating all others. It also removes duplicate rows from the result. It chooses a vertical subset of a table."
    },
    {
        "id": 67,
        "question": "Are the results of the PRODUCT and JOIN operations the same?",
        "answer": "No, the results are fundamentally different. *   **PRODUCT (Cartesian Product):** This operation returns all possible combinations of rows from the two involved relations. If table A has 'm' rows and table B has 'n' rows, the product A × B will have m * n rows. It does not require or use any logical relationship between the tables. *   **JOIN:** This operation combines rows from two relations based on a related column (a join condition) between them. It is essentially a Cartesian Product followed by a Selection operation to filter only the meaningful combinations. The result of a join is always a subset of the Cartesian Product."
    },
    {
        "id": 68,
        "question": "What is the RDBMS Kernel?",
        "answer": "The RDBMS Kernel, often just called the kernel, is the core heart of the database management system. It is the central software component that resides in memory and handles the most critical tasks. Its functions include: parsing and optimizing SQL statements, managing memory buffers and caching, controlling transaction management (ACID properties), handling locking and concurrency control, enforcing security and authorization, and interacting directly with the data storage layer. Think of it as the database's operating system."
    },
    {
        "id": 69,
        "question": "What are the major subsystems of an RDBMS?",
        "answer": "A full-featured RDBMS is composed of several integrated subsystems that work together: 1. **Query Processor:** Handles parsing, optimization, and execution of queries. 2. **Storage Manager:** Manages disk space, data files, and data structures like indexes. 3. **Transaction Manager:** Ensures ACID properties (Atomicity, Consistency, Isolation, Durability). 4. **Buffer Manager:** Handles the transfer of data between disk and main memory. 5. **Lock Manager:** Controls concurrent access to data items. 6. **Log Manager:** Records all changes for recovery and auditing. 7. **Security and Authorization Subsystem:** Manages users, roles, and permissions. 8. **Network Communication Subsystem:** Handles communication with database clients."
    },
    {
        "id": 70,
        "question": "Which part of the RDBMS manages the data dictionary and how?",
        "answer": "The data dictionary is exclusively managed and maintained by the RDBMS kernel itself. It is stored as a set of special system tables within the database. The kernel is the only software component with the privilege to directly modify these tables. When a user issues a DDL statement (like `CREATE TABLE`), the kernel's DDL interpreter processes the command and updates the corresponding metadata entries in the data dictionary tables. All other components of the RDBMS and all user queries constantly read from the data dictionary to function correctly."
    },
    {
        "id": 71,
        "question": "What is the purpose of the information stored in the data dictionary?",
        "answer": "The data dictionary's metadata serves as the central nervous system for the DBMS. Its primary purposes are: 1. **Validation:** It validates the existence of database objects (tables, columns, users) when they are referenced in SQL statements. 2. **Access Control:** It stores security information, determining which users have what permissions on which objects. 3. **Mapping and Translation:** It provides the essential mapping between the logical schema (table and column names) and the physical schema (file locations, storage details), enabling data independence. 4. **Query Optimization:** The optimizer uses statistics and structural information from the data dictionary to choose efficient query execution plans."
    },
    {
        "id": 72,
        "question": "How do you communicate with a relational database management system?",
        "answer": "The primary and standard method to communicate with an RDBMS is through the Structured Query Language (SQL). Applications and users send SQL statements to the DBMS. These statements can be sent through: 1. **Command-Line Interfaces:** Tools like `mysql` or `psql`. 2. **Graphical User Interfaces (GUIs):** Applications like MySQL Workbench or DBeaver. 3. **Application Programming Interfaces (APIs):** Code in languages like Python, Java, or PHP uses drivers (e.g., JDBC, ODBC) to connect to the database and send SQL commands. The DBMS receives the SQL, processes it, and returns the result."
    },
    {
        "id": 73,
        "question": "How does SQL differ from conventional programming languages?",
        "answer": "SQL is a declarative language, whereas conventional languages like Java or Python are imperative (procedural). This is the key difference: *   **SQL (Declarative):** You specify *what* data you want, but not *how* to get it. You describe the desired result set, and the DBMS's query optimizer figures out the most efficient algorithm, execution plan, and steps to retrieve it. *   **Conventional Languages (Imperative):** You provide explicit, step-by-step instructions for the computer to follow to achieve a result. You control the flow and logic (loops, conditionals) in detail. SQL is also set-oriented, processing groups of records, while procedural languages typically process one record at a time."
    },
    {
        "id": 74,
        "question": "What are the three major sets of files that compose an Oracle database?",
        "answer": "An Oracle database is physically comprised of three essential file types: 1. **Datafiles:** These files store the actual database data, including tables, indexes, and other segments. All the user data and most of the system data resides here. 2. **Control Files:** A small but critical binary file that records the physical structure of the database. It contains information like the database name, timestamps, and the locations of all datafiles and redo log files. The database cannot start without it. 3. **Redo Log Files:** These files record all changes made to the database data. They are crucial for recovery, allowing the database to replay transactions in the event of a failure. They are written to in a circular fashion."
    },
    {
        "id": 75,
        "question": "What is a database trigger?",
        "answer": "A database trigger is a named program unit, written in a procedural language like PL/SQL, that is stored in the database and automatically executed ('fired') in response to a specific event on a particular table or view. The events are typically Data Manipulation Language (DML) statements: `INSERT`, `UPDATE`, or `DELETE`. Triggers can be defined to fire once per statement or once for every row affected by the statement. They are used to enforce complex business rules, audit changes, maintain derived data, and enhance security."
    },
    {
        "id": 76,
        "question": "What are stored procedures and what advantages do they offer?",
        "answer": "Stored procedures are named programs containing a sequence of SQL and procedural statements that are stored, compiled, and executed on the database server. Their advantages include: 1. **Performance:** They are pre-compiled, reducing parsing and optimization overhead. 2. **Reduced Network Traffic:** Applications can call a single procedure instead of sending multiple SQL statements. 3. **Modularity & Reusability:** Business logic is written once, stored centrally, and can be called by any application. 4. **Security:** Users can be granted permission to execute a procedure without having direct access to the underlying tables, providing a strong security layer. 5. **Maintainability:** Logic is centralized, making it easier to change and debug."
    },
    {
        "id": 77,
        "question": "What is the role of the Storage Manager in a DBMS?",
        "answer": "The Storage Manager is a crucial DBMS component that provides the interface between the low-level data stored on disk and the rest of the DBMS components (e.g., the query processor). It is responsible for: 1. **Managing Storage:** Allocating space on disk for database files. 2. **Managing Data Structures:** Implementing and managing efficient file structures for storing data (e.g., heap files, hashing) and indexes. 3. **Translating Requests:** Converting the logical requests for data (e.g., 'get row with id=5') into low-level commands to read from or write to the physical storage system."
    },
    {
        "id": 78,
        "question": "What does the Buffer Manager do?",
        "answer": "The Buffer Manager is responsible for managing the database buffer pool in main memory. Its core duties are: 1. **Data Transfer:** Fetching data pages from disk into memory buffers when needed by a query, and writing modified pages ('dirty pages') back to disk. 2. **Caching:** Deciding which pages to keep in memory to maximize the chance that future requests can be served from the much faster RAM, thus minimizing slow disk I/O. 3. **Page Replacement:** Using algorithms (like LRU - Least Recently Used) to decide which pages to evict from the buffer when space is needed for new pages."
    },
    {
        "id": 79,
        "question": "What is the function of the Transaction Manager?",
        "answer": "The Transaction Manager is the component that ensures transactions satisfy the ACID properties. Its key responsibilities are: 1. **Atomicity & Durability:** It works with the Log Manager to ensure transactions are all-or-nothing and durable upon commit. 2. **Consistency:** It ensures that a transaction takes the database from one consistent state to another. 3. **Isolation:** It works with the Lock Manager (or other concurrency control schemes) to control the interaction between concurrent transactions, preventing problems like dirty reads and lost updates. It is the central coordinator for transaction scheduling and recovery."
    },
    {
        "id": 80,
        "question": "What is the role of the File Manager?",
        "answer": "The File Manager is a lower-level component within the Storage Manager. It handles the interaction with the operating system's file system. Its role is to: 1. **File Management:** Create, delete, and allocate database files on disk. 2. **Space Management:** Manage the free space available within these files. 3. **Page Management:** Read and write fixed-size blocks of data (pages) from and to these files as requested by the Buffer Manager. It abstracts the OS file system for the rest of the DBMS."
    },
    {
        "id": 81,
        "question": "What is the Authorization and Integrity Manager?",
        "answer": "This DBMS component is responsible for enforcing rules that maintain database security and correctness: 1. **Authorization (Security):** It checks the authority of every user and program attempting to access the database. It verifies if the user has the required privileges (SELECT, INSERT, etc.) to perform the requested operation on the specified data object. 2. **Integrity (Correctness):** It tests for the satisfaction of integrity constraints (e.g., primary key uniqueness, foreign key validity, CHECK constraints) whenever data is inserted, updated, or deleted. It prevents invalid data from entering the database."
    },
    {
        "id": 82,
        "question": "What are stand-alone procedures?",
        "answer": "Stand-alone procedures are stored procedures that are not encapsulated within a package. They are independently defined database objects. While they offer the general benefits of stored procedures (like reduced network traffic), they have limitations compared to packaged procedures: they cannot be grouped logically with related functions and procedures, and in some older systems, they might have different dependency management and performance characteristics. Packaged procedures generally offer better organization, security, and performance due to their ability to be loaded into memory as a unit."
    },
    {
        "id": 83,
        "question": "What are database cursors and what types exist?",
        "answer": "A cursor is a database control structure that enables traversal over the rows in a result set. It provides a way to retrieve more than one row from a database and then process each row individually. The two main types are: 1. **Implicit Cursors:** Automatically created and managed by the DBMS for every SQL statement that returns a result set. The programmer has no direct control over them. 2. **Explicit Cursors:** Defined and controlled explicitly by the programmer within a procedural code block (like PL/SQL). They offer fine-grained control, allowing the programmer to open, fetch rows from, and close the cursor as needed for complex row-by-row processing."
    },
    {
        "id": 84,
        "question": "What is the difference between a cold backup and a hot backup?",
        "answer": "This distinction is based on the state of the database during the backup: *   **Cold Backup (Offline Backup):** Performed while the database is shut down. This ensures a perfectly consistent copy of all database files (datafiles, control files, redo logs) at a single point in time. It is simple and safe but requires database downtime, making it unsuitable for 24/7 systems. *   **Hot Backup (Online Backup):** Performed while the database is running and open for use. This allows backups without downtime. It is more complex because it requires the DBMS to put tablespaces into a special backup mode to ensure the backed-up files are consistent and can be used for recovery. This is essential for high-availability systems."
    },
    {
        "id": 85,
        "question": "What are proactive, retroactive, and simultaneous updates?",
        "answer": "These terms classify database updates based on their timing relative to real-world events: 1. **Proactive Update:** Applied to the database *before* it becomes effective in the real world. For example, scheduling a price change to take effect in the system at a future date. 2. **Retroactive Update:** Applied to the database *after* it has become effective in the real world. This is often a correction of a past error or a late entry of data. 3. **Simultaneous Update:** Applied to the database *at the same time* it becomes effective in the real world. This is the most common type of update for transactional systems (e.g., deducting payment at the moment of sale)."
    },
    {
        "id": 86,
        "question": "What is the relationship between data and information in a database context?",
        "answer": "Data are raw, unorganized facts and figures (e.g., the numbers 25, 10, 2023). Information is data that has been processed, organized, and interpreted to be useful and meaningful in a specific context (e.g., 'Total Sales for ProductX on October 10, 2023, were $25'). A database stores data in a structured way. When this data is queried, filtered, aggregated, and presented, it is transformed into valuable information that supports decision-making and knowledge."
    },
    {
        "id": 87,
        "question": "What is ERP and what type of database does it use?",
        "answer": "Enterprise Resource Planning (ERP) is a suite of integrated applications that a company uses to manage its core business processes, such as finance, supply chain, manufacturing, operations, reporting, and human resources. An ERP system relies on a central, unified database to eliminate data redundancy and provide a single source of truth across the entire organization. This database is typically a robust, multi-user **Relational Database Management System (RDBMS)** like Oracle, SQL Server, or SAP HANA, capable of handling massive transactional workloads and complex queries from various functional modules."
    },
    {
        "id": 88,
        "question": "Can you define DBMS in a comprehensive way?",
        "answer": "A Database Management System (DBMS) is a complex software system that serves as an intermediary between the database of stored data and the users or application programs that need to access it. Its primary purpose is to provide a convenient, efficient, and secure environment for defining, creating, manipulating, and controlling access to databases. It shields users from the physical storage details and provides abstract data views, while simultaneously ensuring data integrity, security, concurrency control, and reliable recovery from failures."
    },
    {
        "id": 89,
        "question": "Why is a database considered 'self-describing'?",
        "answer": "A database is called self-describing because it contains not only the users' data but also a detailed description of its own structure. This descriptive data, known as **metadata**, is stored in the system catalog or data dictionary. The metadata includes definitions of all tables, columns, data types, constraints, indexes, views, and user privileges. Because this information is stored within the database itself, the DBMS always has the necessary context to understand and manage the data it contains, without relying on external sources."
    },
    {
        "id": 90,
        "question": "Who was E.F. Codd and why is he important to database technology?",
        "answer": "Dr. Edgar F. Codd was a British computer scientist who worked for IBM. In 1970, he published a seminal paper titled 'A Relational Model of Data for Large Shared Data Banks' that introduced the **relational model** for database management. This groundbreaking work laid the theoretical foundation for relational databases. His 12 rules (Codd's Rules) defined the criteria for a system to be considered a fully relational database management system (RDBMS). His ideas are the reason SQL and modern RDBMS like Oracle, DB2, SQL Server, and MySQL exist today."
    },
    {
        "id": 91,
        "question": "What is SQL and why is it so significant?",
        "answer": "Structured Query Language (SQL) is the standard and most widely used programming language for managing and querying data in relational database management systems (RDBMS). Its significance stems from several factors: 1. **Standardization:** It is an ANSI/ISO standard, ensuring portability across different database systems (though with some vendor-specific extensions). 2. **Declarative Nature:** Users specify *what* data they want, not *how* to get it, making it powerful and relatively easy to learn. 3. **Comprehensive Functionality:** It provides a unified language for data definition (DDL), data manipulation (DML), data control (DCL), and transaction control (TCL). 4. **Ubiquity:** It is supported by all major relational databases, making it an essential skill for developers, analysts, and administrators."
    },
    {
        "id": 92,
        "question": "Write a SQL query to select all data for students with a grade of 90 or higher.",
        "answer": "```sql\nSELECT *\nFROM STUDENT\nWHERE Grade >= 90;\n```\nThis query uses the `SELECT *` statement to retrieve all columns from the `STUDENT` table. The `WHERE` clause acts as a filter, specifying that only those rows where the value in the `Grade` column is greater than or equal to 90 should be included in the result set."
    },
    {
        "id": 93,
        "question": "What are the five primary SQL aggregate functions?",
        "answer": "Aggregate functions perform a calculation on a set of values and return a single summary value. The five primary ones are: 1. **COUNT():** Returns the number of rows in a group. 2. **SUM():** Returns the sum of a numeric column. 3. **AVG():** Returns the average value of a numeric column. 4. **MAX():** Returns the maximum value in a column. 5. **MIN():** Returns the minimum value in a column. These functions are almost always used with the `GROUP BY` clause to calculate values for categories of data."
    },
    {
        "id": 94,
        "question": "Write a SQL query to count all students and display the count with a column alias.",
        "answer": "```sql\nSELECT COUNT(*) AS NumStudents\nFROM STUDENT;\n```\nThis query uses the `COUNT(*)` function to count every row in the `STUDENT` table. The `AS NumStudents` clause provides a user-friendly alias for the output column, so the result will have a column named `NumStudents` instead of the default `COUNT(*)`."
    },
    {
        "id": 95,
        "question": "What is an SQL subquery?",
        "answer": "An SQL subquery (or nested query) is a query placed within another SQL query. It is a `SELECT` statement enclosed in parentheses and embedded within a clause (most commonly the `WHERE` or `HAVING` clause) of the outer query. The result of the inner subquery is used by the outer query to complete its operation. Subqueries are powerful tools for performing complex filtering, calculations, and data retrieval that would be difficult or impossible with a single query. They can be used for comparisons using operators like `IN`, `ANY`, `ALL`, or `EXISTS`."
    },
    {
        "id": 96,
        "question": "What alternative terms are used for the components of the relational model?",
        "answer": "The formal terms of the relational model have more common equivalents: *   **Relation** is commonly called a **Table**. *   **Tuple** is commonly called a **Row** or **Record**. *   **Attribute** is commonly called a **Column** or **Field**. *   **Relation Schema** is often referred to as the **Table Definition** or **Structure**. While the formal terms are used in academic and design contexts, the common terms (table, row, column) are ubiquitous in everyday database use and SQL syntax."
    },
    {
        "id": 97,
        "question": "Why are functional dependencies not considered mathematical equations?",
        "answer": "Functional dependencies (FDs) are not equations because they represent existence and constraint, not numerical equality. An equation (like A + B = C) denotes a numerical relationship where the values on both sides are equal and calculable. A functional dependency (like Zipcode → City) denotes a deterministic relationship: if you know the value of the determinant (Zipcode), you know the value of the dependent attribute (City). It's a statement about uniqueness and constraint, not arithmetic. The value '12345' is not equal to 'Springfield', but it does determine it."
    },
    {
        "id": 98,
        "question": "What is a foreign key and what is its purpose?",
        "answer": "A foreign key is an attribute (or set of attributes) in one table that references the primary key of another table. Its purpose is to establish and enforce a link between the data in these two tables. This link is the mechanism for creating relationships in the relational model. The foreign key constraint ensures **referential integrity**, meaning that any value in the foreign key column must either be NULL or must match an existing value in the primary key of the referenced table, preventing orphaned records."
    },
    {
        "id": 99,
        "question": "What are insertion and deletion anomalies?",
        "answer": "These are problems that occur in poorly designed, unnormalized tables: *   **Insertion Anomaly:** The inability to add data about one entity without adding data about another, unrelated entity. For example, you cannot record a new department's information until at least one employee is assigned to it. *   **Deletion Anomaly:** The unintended loss of data about one entity when deleting data about another entity. For example, if you delete the only employee in a department, you might also lose all information about that department itself. Normalization aims to eliminate these anomalies by splitting data into appropriate tables."
    },
    {
        "id": 100,
        "question": "What does it mean for a relation to be in Boyce-Codd Normal Form (BCNF)?",
        "answer": "A relation is in Boyce-Codd Normal Form (BCNF) if it meets the following condition: For every non-trivial functional dependency (X → Y) in the relation, the determinant (X) must be a superkey. A superkey is any set of attributes that uniquely identifies a tuple. In simpler terms, BCNF requires that the *only* determinants in the table are candidate keys. This eliminates all redundancy due to functional dependencies. A common summary of the goal of normalization is: *Every non-key attribute must provide a fact about the key, the whole key, and nothing but the key.* BCNF enforces this very strictly."
    },
    {
        "id": 101,
        "question": "What should you look for in table data when designing a new database?",
        "answer": "When given existing data to model, you should analyze it to discover: 1. **Functional Dependencies (FDs):** What attributes determine others? (e.g., ProductID determines Price). 2. **Multi-Valued Dependencies (MVDs):** Are there independent multi-valued facts? (e.g., an employee has multiple skills and multiple certifications independently). 3. **Candidate Keys:** What set(s) of attributes can uniquely identify each row? 4. **Primary Key:** Which candidate key is best suited to be the main identifier? 5. **Foreign Keys:** What relationships exist between potential tables? This analysis directly informs the normalization process and schema design."
    },
    {
        "id": 102,
        "question": "Why does using normalized tables often lead to more complex SQL in applications?",
        "answer": "Normalization involves breaking data down into multiple smaller tables to eliminate redundancy. To answer a business question that requires data from several of these tables, application code must reassemble it. This reassembly is done in SQL using **JOIN** operations to link tables together on their primary and foreign keys, and **subqueries**. Writing these multi-table joins and correlated subqueries is inherently more complex than writing a simple `SELECT * FROM one_big_table`. The trade-off is that this complexity in querying is accepted to gain massive benefits in data integrity, non-redundancy, and update performance."
    },
    {
        "id": 103,
        "question": "What is the multivalue, multicolumn problem?",
        "answer": "This is a common design flaw where a table is created with multiple columns that store variations of the same type of attribute, violating First Normal Form. For example, a table for customer surveys might have columns like `Phone_Model_1`, `Phone_Model_2`, `Phone_Model_3` to store up to three phone models a customer owns. This design causes problems: it limits the number of values, makes querying difficult (e.g., 'find all customers who own iPhone X'), and wastes space. The correct solution is to create a separate related table with one row per phone model per customer."
    },
    {
        "id": 104,
        "question": "How is the multivalue, multicolumn problem related to multivalued dependencies?",
        "answer": "Both problems are different manifestations of the same core issue: trying to store multiple values for a single attribute within a single table. *   **Multivalue, Multicolumn Problem:** Stores the multiple values in *multiple columns* within the same row. *   **Multivalued Dependency (MVD):** Implicitly exists when the multiple values would be stored in *multiple rows* if the table were properly normalized. Both designs are incorrect. The solution for both is identical: remove the repeating groups and place them in a separate table. The multicolumn problem is essentially a pre-1NF violation, while an MVD describes a dependency in a 1NF table that needs to be resolved to achieve 4NF."
    },
    {
        "id": 105,
        "question": "What is the inconsistent values problem?",
        "answer": "The inconsistent values problem occurs when the same real-world data is recorded in different, inconsistent formats within the same database. This often happens when data comes from multiple sources or different users enter data without validation rules. For example, the same color might be entered as 'Red', 'RED', 'R', '01' (a code), or 'Scarlet'. This inconsistency makes querying and reporting unreliable and difficult. Solutions include implementing strong data validation, using check constraints, and providing users with pick-lists or dropdown menus instead of free-text fields."
    },
    {
        "id": 106,
        "question": "Explain the relationship between entity, entity class, and entity instance.",
        "answer": "These terms describe different levels of abstraction in data modeling: *   **Entity:** A general term for a 'thing' or object in the real world that can be distinctly identified (e.g., a specific customer named John Doe). *   **Entity Class (Entity Type):** A classification or category of entities that share common properties. It is the template or definition (e.g., the 'Customer' class). *   **Entity Instance:** A single, specific occurrence or example of an entity class (e.g., the database record for customer John Doe with ID 123). Analogy: 'Vehicle' is the class; your specific car with VIN 123ABC is an instance of that class."
    },
    {
        "id": 107,
        "question": "What is the difference between attributes and identifiers?",
        "answer": "Both are properties of entities, but they serve different purposes: *   **Attributes:** These are descriptive properties that characterize an entity. They describe the entity's features (e.g., for a 'Product' entity: `Color`, `Weight`, `Price`). *   **Identifiers (Keys):** These are special attributes that are used to uniquely identify an entity instance. An identifier's purpose is to ensure each instance is distinct and can be reliably referenced (e.g., for a 'Product' entity: `ProductID` or `SKU`). All identifiers are attributes, but not all attributes are identifiers."
    },
    {
        "id": 108,
        "question": "What are the three types of binary relationship cardinalities?",
        "answer": "Binary relationship cardinality defines the numerical relationship between instances of two entity classes: 1. **One-to-One (1:1):** One instance of Entity A is associated with at most one instance of Entity B, and vice versa. (e.g., a `Country` has exactly one `CapitalCity`, and a `CapitalCity` is the capital of exactly one `Country`). 2. **One-to-Many (1:N):** One instance of Entity A can be associated with many instances of Entity B, but an instance of B is associated with at most one instance of A. (e.g., one `Department` has many `Employees`, but one `Employee` works in only one `Department`). 3. **Many-to-Many (M:N):** One instance of Entity A can be associated with many instances of Entity B, and one instance of Entity B can be associated with many instances of Entity A. (e.g., one `Student` takes many `Courses`, and one `Course` is taken by many `Students`)."
    },
    {
        "id": 109,
        "question": "What is the archetype/instance pattern?",
        "answer": "The archetype/instance pattern is a common data modeling pattern used to represent a template (the archetype) and its specific occurrences (the instances). A classic example is the relationship between a `CLASS` (the archetype, e.g., 'Introduction to Database Systems') and a `SECTION` (the instance, e.g., 'Fall 2023, Section 01' which has a specific time, room, and instructor). The instance is often ID-dependent on the archetype, meaning the primary key of the `SECTION` includes the key of the `CLASS`. This pattern separates the definition of a thing from its specific manifestations."
    },
    {
        "id": 110,
        "question": "What is a recursive relationship?",
        "answer": "A recursive relationship (or unary relationship) is a relationship where an entity is related to itself. It links different instances of the same entity type. For example, in an `EMPLOYEE` entity: *   A recursive relationship `Supervises` can model who supervises whom (an employee can supervise other employees). *   A recursive relationship `Precedes` in a `TASK` entity can model task dependencies (one task must be completed before another can start). In the database table, this is implemented by adding a foreign key column that references the primary key of the same table (e.g., an `ReportsTo` column in the `EMPLOYEE` table that contains the `EmployeeID` of the manager)."
    },
    {
        "id": 111,
        "question": "What are the steps for transforming an entity into a database table?",
        "answer": "Transforming an entity from an ER diagram into a physical table involves: 1. **Define the Table Structure:** Create a table with the same name as the entity. 2. **Specify Attributes as Columns:** Create a column for each of the entity's attributes, choosing appropriate data types and lengths. 3. **Define the Primary Key:** Choose the identifier attribute to be the primary key. For weak entities, form a composite key with the owner's primary key. 4. **Specify Constraints:** Define null status (NOT NULL), default values, and check constraints for columns. 5. **Define Foreign Keys:** Based on relationships with other entities, add foreign key columns to establish links. 6. **Verify Normalization:** Ensure the table design adheres to the desired normal form to avoid anomalies."
    },
    {
        "id": 112,
        "question": "What is a surrogate key and why is it considered an ideal primary key?",
        "answer": "A surrogate key is an artificial, system-generated primary key that has no business meaning. It is typically a simple integer that auto-increments with each new record (e.g., `CustomerID`, `OrderID`). It is considered ideal for several reasons: 1. **Stability:** It never changes, unlike natural keys (e.g., an email address can change). 2. **Simplicity:** It is usually a single, numeric column, making joins and indexing very efficient. 3. **Anonymity:** It reveals no information about the entity it identifies. 4. **Uniqueness Guarantee:** The system ensures its uniqueness. While natural keys exist in the business domain, surrogate keys are often preferred for internal system efficiency and maintenance."
    },
    {
        "id": 113,
        "question": "What are data constraints and what are their types?",
        "answer": "Data constraints are rules enforced on data columns to ensure the accuracy, integrity, and reliability of the data in a database. The main types are: 1. **Domain Constraints:** Ensure a column's value is of a valid data type and falls within a defined set of values or range (e.g., `Age` must be a number between 0 and 120). 2. **Entity Integrity Constraints:** Ensure each row is uniquely identifiable (Primary Key constraint: unique and not null). 3. **Referential Integrity Constraints:** Ensure relationships between tables remain consistent (Foreign Key constraint). 4. **User-Defined Integrity Constraints (CHECK):** Enforce custom business rules that are specific to the application (e.g., `EndDate` must be after `StartDate`)."
    },
    {
        "id": 114,
        "question": "How are recursive relationships typically implemented in a database?",
        "answer": "A recursive relationship is implemented by adding a foreign key column within the same table. This column references the primary key of another row in the same table. For example, to model an employee hierarchy in an `EMPLOYEE` table: 1. The table has an `EmployeeID` primary key column. 2. It also has a `ManagerID` foreign key column. 3. The `ManagerID` column contains the `EmployeeID` of the employee who is the manager of the current employee. It references the `EmployeeID` column in the same table. If an employee has no manager (e.g., the CEO), the `ManagerID` would be NULL. Queries on such tables often use self-joins (joining the table to itself) to navigate the hierarchy."
    },
    {
        "id": 115,
        "question": "What is a cascading update?",
        "answer": "A cascading update is a referential integrity action defined on a foreign key. When the primary key value referenced by a foreign key is updated in the parent table, the DBMS automatically propagates that update to all matching foreign key values in the child table. This ensures that the relationship between the parent and child records is not broken. It is a crucial feature for maintaining data consistency when primary keys need to be changed, which is rare with stable surrogate keys but might be necessary with natural keys."
    },
    {
        "id": 116,
        "question": "What is a SQL view and what are its primary uses?",
        "answer": "A view is a virtual table whose contents are defined by a query. It does not store data itself but displays data from one or more underlying base tables. Its primary uses are: 1. **Simplification:** Hides the complexity of multi-table joins and calculations. 2. **Security:** Restricts user access to specific rows and/or columns, providing a tailored interface. 3. **Logical Data Independence:** Can provide a consistent interface to applications even if the underlying table structures change. 4. **Data Integrity:** Can enforce certain checks at the view level for users who only access data through the view."
    },
    {
        "id": 117,
        "question": "What is the 'paradigm mismatch' between SQL and application programming languages?",
        "answer": "The paradigm mismatch refers to the fundamental difference in how data is handled: *   **SQL is set-oriented:** It processes and returns entire sets of rows at a time. *   **Application Languages (e.g., Java, Python) are record-oriented (row-oriented):** They process data one object or record at a time using loops and individual variables. This mismatch means that the result of an SQL query (a set) cannot be directly processed by an application language. A mechanism like a **cursor** is required to bridge this gap, allowing the application to iterate through the result set one row at a time."
    },
    {
        "id": 118,
        "question": "What are four common applications for database triggers?",
        "answer": "Triggers are used to automatically enforce business rules and logic at the database level: 1. **Auditing and Logging:** Automatically recording changes made to sensitive data (e.g., logging every update to a `Salary` column into an `AUDIT_TRAIL` table). 2. **Enforcing Complex Constraints:** Implementing business rules that are too complex for standard CHECK or foreign key constraints (e.g., 'cannot update an order after it has been shipped'). 3. **Deriving Data:** Automatically maintaining derived or denormalized data (e.g., updating a `TotalOrderAmount` column in a `CUSTOMER` table whenever a new order is inserted). 4. **Implementing Security Authorizations:** Performing additional security checks beyond standard GRANT permissions before allowing a data modification."
    },
    {
        "id": 119,
        "question": "How do stored procedures differ from triggers?",
        "answer": "| Feature | Stored Procedure | Trigger |\n| :--- | :--- | :--- |\n| **Execution** | Explicitly called and executed by a user or application. | Automatically fired (executed) by the DBMS in response to a DML event (`INSERT`, `UPDATE`, `DELETE`). |\n| **Parameters** | Can have input and output parameters. | Cannot have explicit parameters; they use special pseudo-records (e.g., `:NEW`, `:OLD` in Oracle) to access row data. |\n| **Transaction Control** | Can contain transaction control statements like `COMMIT` or `ROLLBACK` (use with caution). | Generally cannot contain transaction control statements; they are part of the transaction that fired them. |\n| **Purpose** | Used to encapsulate and execute a defined business process. | Used to enforce business rules or maintain integrity in response to data changes."
    },
    {
        "id": 120,
        "question": "What are the advantages of using stored procedures?",
        "answer": "Stored procedures offer significant benefits for application development and performance: 1. **Enhanced Performance:** They are pre-compiled and stored in executable form, reducing parsing and optimization overhead on the database server. 2. **Reduced Network Traffic:** An application can call a single procedure instead of sending multiple SQL statements across the network. 3. **Improved Security:** Users can be granted execute permission on a procedure without having direct access to the underlying tables, providing a strong security layer. 4. **Code Reusability and Maintainability:** Business logic is stored centrally in the database, making it reusable by any application and easier to maintain and change in one place. 5. **Data Integrity:** Complex business rules can be enforced consistently within the database itself."
    },
    {
        "id": 121,
        "question": "Why is database redesign sometimes a necessary process?",
        "answer": "Database redesign is a critical maintenance activity driven by two primary factors: 1. **Correcting Initial Design Flaws:** Mistakes, oversights, or misunderstandings of requirements during the initial design phase are often discovered only after the system is in use. Redesign is needed to fix these issues, such as poor performance due to lack of indexes or data anomalies from insufficient normalization. 2. **Adapting to Evolving Requirements:** Businesses and their processes change over time. New features, regulations, or business models can necessitate changes to the database structure to store new data or support new types of queries. A database must evolve to remain useful and relevant."
    },
    {
        "id": 122,
        "question": "What is the key difference between a correlated subquery and a regular subquery?",
        "answer": "The key difference lies in their execution and dependency: *   **Regular (Non-Correlated) Subquery:** The inner query can be executed independently of the outer query. It is processed once, and its result is passed to the outer query. It is like a constant value within the outer query. *   **Correlated Subquery:** The inner query cannot be executed independently because it references a column from the outer query. It is executed repeatedly, once for each row candidate processed by the outer query. The result of the inner query depends on the specific value of the current row in the outer query, creating a looping mechanism."
    },
    {
        "id": 123,
        "question": "What is a dependency graph used for in database management?",
        "answer": "A dependency graph is a visual tool or diagram used to map the connections and dependencies between various elements within a database system. It helps database administrators and developers understand the potential impact of a proposed change. For example, it can show that a specific table is used by several views, application modules, stored procedures, and reports. Before altering or dropping that table, the graph makes it clear which other components will be affected and need to be reviewed, updated, or tested, thus preventing system failures after deployment."
    },
    {
        "id": 124,
        "question": "What is the correct process for adding a NOT NULL column to an existing table with data?",
        "answer": "You cannot directly add a `NOT NULL` column to a populated table because existing rows would have `NULL` for the new column, violating the constraint. The correct process is: 1. **Add the Column as NULLable:** First, add the column without the `NOT NULL` constraint. `ALTER TABLE table_name ADD column_name data_type;` 2. **Populate the Column:** Update the new column for all existing rows with a valid default value. `UPDATE table_name SET column_name = default_value;` 3. **Add the NOT NULL Constraint:** Finally, alter the column to add the `NOT NULL` constraint now that all rows have a value. `ALTER TABLE table_name ALTER COLUMN column_name SET NOT NULL;`"
    },
    {
        "id": 125,
        "question": "How do you convert a one-to-one relationship to a one-to-many relationship in a database schema?",
        "answer": "Consider two tables, `EMPLOYEE` and `COMPUTER`, in a 1:1 relationship where a computer is assigned to one employee and an employee has one computer. The `COMPUTER` table has a foreign key `EmpNumber` that must be unique. To change this to a 1:N relationship (one employee can have many computers), you simply need to **remove the uniqueness constraint** on the foreign key column (`EmpNumber`) in the `COMPUTER` table. This allows multiple rows in the `COMPUTER` table to have the same `EmpNumber` value, meaning one employee can now be associated with multiple computers. The physical structure of the tables remains the same; only the constraint changes."
    },
    {
        "id": 126,
        "question": "What is the difference between an exclusive lock and a shared lock?",
        "answer": "These locks control how multiple transactions can access the same data item concurrently: *   **Exclusive Lock (X Lock):** Grants a transaction exclusive write access to a data item. While an exclusive lock is held, no other transaction can acquire any type of lock (shared or exclusive) on the same data item. It is used for `UPDATE`, `DELETE`, and `INSERT` operations. *   **Shared Lock (S Lock):** Grants a transaction read-only access to a data item. Multiple transactions can hold shared locks on the same data item simultaneously, allowing for concurrent reads. However, no transaction can acquire an exclusive lock on the data item until all shared locks are released."
    },
    {
        "id": 127,
        "question": "What is the difference between optimistic and pessimistic concurrency control?",
        "answer": "These are two strategies for managing concurrent access to data: *   **Pessimistic Locking:** Assumes conflicts are likely. It prevents conflicts by locking data *before* a transaction begins to use it. Readers block writers and writers block readers. It is used in high-contention environments. *   **Optimistic Locking:** Assumes conflicts are rare. It allows transactions to proceed without locking. Conflicts are detected at the *end* of the transaction when a commit is attempted. If a conflict is detected (e.g., the data was changed by another transaction after it was read), the transaction is rolled back and must be restarted. It is preferred for low-contention environments like web applications, as it provides better scalability."
    },
    {
        "id": 128,
        "question": "What is a deadlock and how is it handled?",
        "answer": "A deadlock is a situation where two or more transactions are permanently blocked because each transaction holds a lock on a resource that the other transactions need to proceed. It's a cyclic wait condition (e.g., Transaction A holds Lock 1 and waits for Lock 2, while Transaction B holds Lock 2 and waits for Lock 1). *   **Prevention:** Systems can use protocols to ensure that deadlocks cannot occur (e.g., requiring transactions to acquire all locks at once). *   **Detection and Resolution:** The DBMS has a deadlock detector that periodically checks the wait-for graph for cycles. When a deadlock is found, the system resolves it by choosing a **victim transaction** and rolling it back, releasing its locks and allowing the other transaction(s) to proceed. The aborted transaction must be restarted by the application."
    },
    {
        "id": 129,
        "question": "What are the primary responsibilities of a Database Administrator (DBA)?",
        "answer": "A DBA's role is multifaceted and crucial for ensuring a database's health, performance, and security. Key responsibilities include: 1. **Database Design & Implementation:** Planning and creating new databases. 2. **Performance Tuning:** Monitoring and optimizing database performance (indexing, query optimization). 3. **Security Management:** Creating users, roles, and managing access permissions. 4. **Backup and Recovery:** Designing and testing robust strategies to prevent data loss. 5. **Availability Management:** Ensuring the database is highly available and online. 6. **Change Management:** Applying patches, upgrades, and managing schema changes. 7. **Capacity Planning:** Forecasting future storage and performance needs."
    },
    {
        "id": 130,
        "question": "What does ACID mean in the context of database transactions?",
        "answer": "ACID is an acronym that describes the four essential properties of a reliable database transaction: *   **Atomicity:** Guarantees that a transaction is treated as a single, indivisible unit of work. It either executes completely ('All') or not at all ('Nothing'). *   **Consistency:** Ensures that a transaction takes the database from one valid state to another, preserving all defined rules and constraints (e.g., foreign keys, unique keys). *   **Isolation:** Ensures that the execution of concurrent transactions leaves the database in the same state as if they were executed sequentially. Transactions are protected from intermediate states of other transactions. *   **Durability:** Guarantees that once a transaction is committed, its changes are permanent and will survive any subsequent system failure."
    },
    {
        "id": 131,
        "question": "What are the common methods for creating an Oracle database?",
        "answer": "There are three primary methods to create an Oracle database: 1. **Using the Database Configuration Assistant (DBCA):** This is a graphical user interface (GUI) tool that guides the DBA through the creation process with easy-to-follow steps. It is the simplest and most common method. 2. **Using Oracle-Managed Scripts:** This involves running prepared SQL scripts provided by Oracle. It offers more control than DBCA but is more complex. 3. **Manual Creation with the CREATE DATABASE Statement:** This is an advanced method where the DBA manually issues SQL commands to create the database. It provides the ultimate level of control but requires deep knowledge of the Oracle architecture and is error-prone."
    },
    {
        "id": 132,
        "question": "What are database sequences and what are potential issues with their use?",
        "answer": "A sequence is a database object that generates a sequence of unique integers, typically used to create artificial primary key values. **Potential Issues:** 1. **Gaps in Sequence Values:** Gaps can occur naturally due to transaction rollbacks, database crashes, or caching. This is usually not a functional problem but can be undesirable for some business requirements (e.g., invoice numbers). 2. **Misuse:** A sequence created for one table might be accidentally used for another, or a developer might insert rows without using the sequence, breaking the consistency of key generation. 3. **Lack of Enforced Relationship:** The sequence itself is independent of the table; the DBMS does not enforce that its values are actually used as the primary key, which is the application's responsibility."
    },
    {
        "id": 133,
        "question": "Under what conditions should you create a database index?",
        "answer": "Indexes should be created strategically to improve query performance. Consider creating an index when: 1. **Frequent Query Filters:** A column is commonly used in the `WHERE` clause for filtering (e.g., `WHERE email = 'x@y.com'`). 2. **Join Conditions:** A column is used frequently to join tables. 3. **Sorting and Grouping:** A column is often used in `ORDER BY` or `GROUP BY` clauses. 4. **Enforcing Uniqueness:** A unique index is required to enforce a primary key or unique constraint. **Caution:** Indexes slow down `INSERT`, `UPDATE`, and `DELETE` operations because the index must be maintained. Therefore, avoid over-indexing, especially on tables with heavy write operations."
    },
    {
        "id": 134,
        "question": "What are the common transaction isolation levels in Oracle?",
        "answer": "Oracle Database primarily supports the following isolation levels: 1. **READ COMMITTED (Default):** Guarantees that a statement will only see data that was committed before the statement began (not the transaction). It prevents dirty reads but allows non-repeatable reads and phantoms. 2. **SERIALIZABLE:** Guarantees that a transaction will see only data that was committed before the transaction began and changes made by the transaction itself. It provides the highest level of isolation, preventing dirty reads, non-repeatable reads, and phantoms. 3. **READ ONLY:** A variant that specifies the transaction cannot perform any DML operations and sees only data committed at the start of the transaction."
    },
    {
        "id": 135,
        "question": "What file types are crucial for Oracle database recovery?",
        "answer": "A successful recovery depends on these files: 1. **Datafiles:** Contain the actual data. Backups of these files are the foundation of restoration. 2. **Control Files:** Essential for mounting and opening the database. They describe the structure of the database, including the location of all datafiles and online redo log files. A backup is critical. 3. **Online Redo Log Files:** Record all changes made to the database. They are used for instance recovery after a crash. 4. **Archived Redo Log Files:** These are copies of filled online redo log files. They are absolutely vital for complete media recovery, allowing you to 'replay' all transactions up to the point of failure."
    },
    {
        "id": 136,
        "question": "What is the difference between a complete and a differential backup in SQL Server?",
        "answer": "These are two backup strategies in a recovery plan: *   **Complete Backup:** A full copy of the entire database. It backs up all data files and enough of the transaction log to allow for recovering that database. It is the foundation for any restore operation but can be large and time-consuming. *   **Differential Backup:** Backs up only the data pages that have changed since the last complete backup. It is smaller and faster to create than a full backup. To restore, you first restore the last complete backup and then restore the last differential backup. This strategy reduces the number of transaction log files needed for recovery compared to using only full backups."
    },
    {
        "id": 137,
        "question": "What are the different transaction isolation levels in SQL Server and their meanings?",
        "answer": "SQL Server defines several isolation levels: 1. **READ UNCOMMITTED:** The lowest level. Allows dirty reads, meaning a transaction may see uncommitted changes from other transactions. No shared locks are taken. 2. **READ COMMITTED (Default):** Prevents dirty reads. A transaction will only see committed data. It uses locking to hold read locks only for the duration of the statement. 3. **REPEATABLE READ:** Prevents dirty reads and non-repeatable reads. Locks are held on all data read by the transaction until it completes. 4. **SERIALIZABLE:** The highest level. Prevents dirty reads, non-repeatable reads, and phantom reads. It uses range locks to prevent other transactions from inserting new rows that would fall into the range of data read by the transaction. 5. **SNAPSHOT:** Provides a transactionally consistent view of the data as it existed at the start of the transaction, using row versioning instead of locking."
    },
    {
        "id": 138,
        "question": "What are the differences between Simple, Full, and Bulk-Logged recovery models in SQL Server?",
        "answer": "The recovery model determines how the transaction log is used and what restore options are available: *   **Simple Recovery Model:** Transaction log space is automatically reused, minimally logging most operations. Point-in-time recovery is *not* supported. You can only restore to the exact time of a full or differential backup. *   **Full Recovery Model:** All transactions are fully logged. This allows for point-in-time recovery up to the last committed transaction before a failure, using the transaction log backups. This is required for maximum data protection. *   **Bulk-Logged Recovery Model:** A special-purpose model that performs minimal logging for certain bulk operations (like BULK INSERT) to improve performance, while otherwise behaving like the Full model. It allows point-in-time recovery unless a bulk operation occurred, in which case the entire log backup containing that operation must be restored."
    },
    {
        "id": 139,
        "question": "What is the difference between a clustered and a nonclustered index in SQL Server?",
        "answer": "This is a fundamental physical storage difference: *   **Clustered Index:** Determines the physical order of data rows in the table. The data rows themselves are stored in the leaf pages of the index. Therefore, a table can have **only one** clustered index. It is typically created on the primary key. Retrieving data via a clustered index is very fast. *   **Nonclustered Index:** Creates a separate structure from the data rows. The leaf pages of a nonclustered index contain pointers to the actual data rows (either the clustered index key or a row identifier if no clustered index exists). A table can have **many** nonclustered indexes. They are used to improve query performance on columns that are not the primary key."
    },
    {
        "id": 140,
        "question": "What types of triggers does SQL Server support?",
        "answer": "SQL Server supports two main types of triggers based on their timing and purpose: 1. **AFTER Triggers (FOR Triggers):** These fire *after* the triggering DML action (`INSERT`, `UPDATE`, `DELETE`) has been processed. They are used for auditing, enforcing business rules, or creating follow-up actions. A table can have multiple AFTER triggers for each action. 2. **INSTEAD OF Triggers:** These fire *in place of* the triggering action. The original DML operation is not performed; instead, the code within the INSTEAD OF trigger is executed. They are often used to allow updates to complex views that would otherwise be non-updatable. A view or table can have at most one INSTEAD OF trigger per triggering action. SQL Server does not have built-in BEFORE triggers."
    },
    {
        "id": 141,
        "question": "How are ODBC, OLE DB, and ADO related to each other?",
        "answer": "These are Microsoft data access technologies that evolved over time: 1. **ODBC (Open Database Connectivity):** The oldest standard. It provides a C-language API specifically for accessing relational databases. 2. **OLE DB (Object Linking and Embedding for Databases):** The successor to ODBC. It is a COM-based specification that provides access to a broader range of data sources, not just relational databases (e.g., spreadsheets, text files, email). ODBC is for relational data; OLE DB is for any data. 3. **ADO (ActiveX Data Objects):** A high-level, easy-to-use API that provides an object-oriented interface to OLE DB. It simplifies data access for programmers in languages like Visual Basic and ASP by wrapping the complexity of OLE DB. The relationship is often visualized as ADO -> OLE DB -> ODBC -> Data Source."
    },
    {
        "id": 142,
        "question": "What are the three types of ODBC data sources?",
        "answer": "An ODBC data source is a stored configuration that defines how to connect to a specific database. The three types are: 1. **User DSN:** The data source information is stored in the Windows registry and is visible only to the user who created it. 2. **System DSN:** The data source information is stored in the Windows registry and is visible to all users on the machine, including Windows services. This is the most common type for server applications. 3. **File DSN:** The data source information is stored in a text file (with a .dsn extension) on the disk. This file can be shared with other users who have the same ODBC driver installed, making it portable."
    },
    {
        "id": 143,
        "question": "What disadvantage of ODBC did OLE DB aim to overcome?",
        "answer": "ODBC's primary disadvantage was its focus solely on **relational data** that could be accessed via SQL. OLE DB was designed to overcome this limitation by providing a universal data access model. Its COM-based architecture allowed it to create 'providers' for any type of data store, whether relational (e.g., SQL Server, Oracle) or non-relational (e.g., spreadsheets, email systems, directory services, text files). This enabled developers to use a more consistent programming model to access a much wider variety of data sources."
    },
    {
        "id": 144,
        "question": "What were the primary design goals of OLE DB?",
        "answer": "The major goals of OLE DB were to: 1. **Provide Universal Data Access:** Create a single, unified interface for accessing data from any source, relational or non-relational. 2. **Component Object Model (COM) Based:** Use the COM standard to define interoperable objects for data access, promoting software reusability. 3. **Increase Flexibility:** Allow data providers to expose functionality in pieces, enabling them to implement only the features they supported. 4. **Avoid Data Movement:** Provide a means to access and manipulate data in place, in its native store, without requiring it to be moved or converted into a different format first."
    },
    {
        "id": 145,
        "question": "In OLE DB, what distinguishes an interface from an implementation?",
        "answer": "This is a core concept of COM, which OLE DB is built upon: *   **Interface:** A contract. It is a defined set of properties and methods that an object must expose. It specifies *what* an object can do, but not *how* it does it. OLE DB defines standardized interfaces. *   **Implementation:** The actual code inside the object that fulfills the contract specified by the interface. It defines *how* the properties and methods work. The implementation is hidden from the user (data consumer). This separation allows a provider (implementation) to change its internal code without breaking any applications that use it, as long as it continues to honor the interface contract."
    },
    {
        "id": 146,
        "question": "Why is XML often considered a better markup language than HTML?",
        "answer": "The key difference is in their purpose: *   **HTML (HyperText Markup Language):** Designed for *presenting* and displaying data in a web browser. It uses a fixed set of tags that describe appearance (e.g., `<b>`, `<i>`). It mixes structure, content, and presentation. *   **XML (eXtensible Markup Language):** Designed for *storing and transporting* data. It is extensible—you define your own tags that describe the meaning of the data (e.g., `<price>`, `<author>`). It provides a clear separation between the structure of the data, the data itself, and its presentation. This makes XML self-describing, portable, and far superior for data exchange between heterogeneous systems."
    },
    {
        "id": 147,
        "question": "What are the two main ways to describe the structure and content of an XML document?",
        "answer": "To define the legal structure, elements, and attributes of an XML document, you use a schema language: 1. **Document Type Definition (DTD):** The older, simpler method. It defines the basic structure with a list of legal elements and attributes. An XML document that conforms to its DTD is 'valid'. 2. **XML Schema Definition (XSD):** A more powerful and modern language written in XML itself. XSD provides much stronger data typing (e.g., integers, dates), supports namespaces, and allows for more complex constraints than DTD. XSD has largely replaced DTD for most serious data exchange applications."
    },
    {
        "id": 148,
        "question": "What is the difference between simple elements and complexType elements in XML Schema?",
        "answer": "In XML Schema (XSD), this distinction defines what content an element can contain: *   **Simple Element:** An element that can contain only text (data value). It cannot contain other elements or attributes. For example: `<first_name>John</first_name>`. *   **complexType Element:** An element that can contain other elements, attributes, and text. It defines a complex structure. For example, a `<customer>` element that contains child elements like `<name>`, `<address>`, and an attribute like `id='123'` must be defined as a complexType."
    },
    {
        "id": 149,
        "question": "What is ADO.NET?",
        "answer": "ADO.NET is the primary data access technology within the Microsoft .NET framework. It is the successor to ADO (ActiveX Data Objects). ADO.NET provides a set of classes for connecting to data sources, executing commands, and retrieving results. Its key innovation is the **disconnected architecture**, centered around the `DataSet` object, which allows applications to work with a local in-memory copy of data, disconnect from the database server, and later reconnect to update the server with changes. It provides high performance and scalability for multi-tier applications."
    },
    {
        "id": 150,
        "question": "What is a DataSet in ADO.NET?",
        "answer": "A `DataSet` is an in-memory, disconnected representation of data. It is a major component of ADO.NET's disconnected architecture. Think of it as a miniature, in-memory database. It can contain: *   Multiple `DataTable` objects (like tables). *   `DataRelation` objects that define relationships between these tables (like foreign keys). *   Constraints (`UniqueConstraint`, `ForeignKeyConstraint`) to enforce data integrity. Because it is disconnected from the data source, a `DataSet` allows an application to work with data locally without maintaining a continuous database connection, which is crucial for web application scalability."
    },
    {
        "id": 151,
        "question": "What are the four JDBC driver types defined by Sun?",
        "answer": "JDBC driver types are categorized by how they implement the connection to the database: 1. **Type 1: JDBC-ODBC Bridge Driver:** Uses an ODBC driver to connect to the database. Requires ODBC to be configured on the client machine. Legacy and not recommended for production. 2. **Type 2: Native-API Driver (Partly Java Driver):** Uses the client-side libraries of the database. Converts JDBC calls into calls to the native API (e.g., OCI for Oracle). Requires native libraries on the client. 3. **Type 3: Network-Protocol Driver (Pure Java Driver for Middleware):** Uses a middleware application server that converts JDBC calls into a database-independent network protocol. The middleware then translates this to the database-specific protocol. 4. **Type 4: Database-Protocol Driver (Pure Java Driver):** Directly converts JDBC calls into the network protocol used by the DBMS. This is a direct-to-database pure Java driver. It is the most common and efficient driver type for most applications (e.g., MySQL Connector/J)."
    },
    {
        "id": 152,
        "question": "What is the difference between a Java Servlet and a Java Applet?",
        "answer": "These are two very different Java technologies: *   **Java Applet:** A small client-side application that runs *inside* a web browser on the user's machine. It is downloaded from a web server and executed by the browser's Java Virtual Machine (JVM). Applets are largely obsolete due to security concerns and lack of browser support. *   **Java Servlet:** A server-side Java program that runs *on* a web server. It extends the capabilities of the server to generate dynamic web content. Servlets receive requests from clients (e.g., web browsers), process them (often involving database access), and return responses (usually HTML). They are a fundamental part of Java web development."
    },
    {
        "id": 153,
        "question": "What is the standard pattern for using JDBC in Java code?",
        "answer": "The fundamental steps for JDBC database access are: 1. **Load and Register the Driver:** `Class.forName('com.mysql.cj.jdbc.Driver');` (Note: JDBC 4.0+ often auto-loads drivers). 2. **Establish a Connection:** `Connection conn = DriverManager.getConnection(url, user, password);` 3. **Create a Statement:** `Statement stmt = conn.createStatement();` (or `PreparedStatement` for parameterized queries). 4. **Execute the Query:** `ResultSet rs = stmt.executeQuery('SELECT * FROM table');` 5. **Process the ResultSet:** `while (rs.next()) { String data = rs.getString('column_name'); }` 6. **Close Resources:** Close the `ResultSet`, `Statement`, and `Connection` objects in a `finally` block or using try-with-resources to avoid memory leaks."
    },
    {
        "id": 154,
        "question": "What is a JavaBean?",
        "answer": "A JavaBean is a reusable software component model for Java. It is a standard class that follows specific conventions: 1. **It has a public no-argument constructor.** 2. **Its properties are accessed through 'getter' and 'setter' methods** following a naming pattern (e.g., `getPropertyName()`, `setPropertyName()`). 3. **It is serializable,** meaning its state can be saved and restored. JavaBeans are primarily used to encapsulate many objects into a single object (the bean), making them easier to work with. They are widely used in Java frameworks for applications like JSP (JavaServer Pages) to represent form data or business objects."
    },
    {
        "id": 155,
        "question": "How does MySQL handle surrogate keys and metadata?",
        "answer": "*   **Surrogate Keys:** MySQL uses the `AUTO_INCREMENT` attribute for a column (typically an `INTEGER` or `BIGINT`) to automatically generate unique surrogate key values. When inserting a new row, you omit this column, and MySQL automatically assigns the next sequential number. *   **Metadata:** MySQL stores its metadata in a special database named `mysql`. This database contains tables that store information about users, privileges, databases, tables, columns, and other system settings. For example, the `user` table stores user accounts and global privileges, while the `tables` table in the `information_schema` database provides information about all tables."
    },
    {
        "id": 156,
        "question": "What is a data mart?",
        "answer": "A data mart is a focused subset of a data warehouse that is tailored to the specific needs of a particular business unit, department, or team (e.g., marketing, sales, finance). It contains a curated collection of data designed to serve a specific group of users with a common analytical need. A data mart can be dependent (derived from an existing enterprise data warehouse) or independent (built directly from operational sources without a data warehouse). It is typically smaller, simpler, and more focused than a full data warehouse, allowing for faster implementation and more targeted analysis."
    },
    {
        "id": 157,
        "question": "What is RFM analysis?",
        "answer": "RFM analysis is a customer segmentation technique used in marketing and business intelligence to rank customers based on their past purchasing behavior. The acronym stands for: *   **Recency (R):** How recently did the customer make a purchase? (Customers who bought more recently are more likely to respond to promotions.) *   **Frequency (F):** How often do they purchase? (Frequent purchasers are more engaged.) *   **Monetary Value (M):** How much money do they spend? (High-spending customers are more valuable.) Customers are scored on each dimension (e.g., on a scale of 1-5, with 5 being best). An RFM score like '5-1-3' represents a customer who bought very recently (5), buys infrequently (1), but spends a good amount when they do (3)."
    },
    {
        "id": 158,
        "question": "What are the core functions of a BI reporting system?",
        "answer": "A Business Intelligence (BI) reporting system has three primary functional areas: 1. **Report Authoring:** Allows users to create reports by connecting to data sources, defining the report structure (queries, groupings, filters), and formatting the layout and style. 2. **Report Management:** Handles the 'what, who, when, and how' of report delivery. It involves defining schedules, user subscriptions, security permissions, and delivery channels (e.g., email, portal). 3. **Report Delivery:** Executes the management plan. It is the engine that generates the report based on the authoring definition and delivers it to the intended recipients at the scheduled time, either by 'pushing' it to them or making it available for them to 'pull' (view on demand)."
    },
    {
        "id": 159,
        "question": "What is OLAP?",
        "answer": "OLAP (Online Analytical Processing) is a category of software technology that allows users to interactively analyze multidimensional data from multiple perspectives. It is a core component of BI systems. Key characteristics include: *   **Multidimensional View:** Data is organized in cubes with dimensions (e.g., Time, Product, Location) and measures (e.g., Sales, Profit). *   **Complex Calculations:** Supports advanced operations like drilling down/up, slicing (selecting one dimension), dicing (selecting sub-cubes), and pivoting (rotating the view). *   **Fast Query Performance:** Pre-aggregates data to provide very quick answers to complex analytical queries. OLAP enables users to gain insights from historical data for strategic planning and decision support."
    },
    {
        "id": 160,
        "question": "What is market basket analysis?",
        "answer": "Market basket analysis is a data mining technique used to discover relationships between items that are frequently purchased together. It is the science behind 'customers who bought this also bought...' recommendations. The analysis is based on calculating: *   **Support:** The frequency with which an itemset appears in all transactions. (How popular is this combination?) *   **Confidence:** The probability that if item A is purchased, item B will also be purchased. (How strong is the rule A -> B?) *   **Lift:** Measures how much more likely item B is to be purchased when item A is purchased, compared to its general probability of being purchased. (Is this association real or random?) It helps retailers with strategies for product placement, cross-selling, and promotions."
    },
    {
        "id": 161,
        "question": "What is the difference between structured and unstructured data?",
        "answer": "This is a fundamental classification of data: *   **Structured Data:** Data that is organized in a predefined format, typically a tabular schema with rows and columns. It is easily stored, queried, and analyzed in relational databases. Examples include numbers, dates, and strings in database tables or CSV files. *   **Unstructured Data:** Data that does not have a predefined data model or is not organized in a predefined manner. It accounts for the vast majority of enterprise data. Examples include text documents, emails, videos, photos, audio files, social media posts, and web pages. Specialized databases (NoSQL) and techniques (NLP, computer vision) are needed to manage and analyze it."
    },
    {
        "id": 162,
        "question": "Why is it important to understand file processing systems even though they are outdated?",
        "answer": "Understanding the limitations of traditional file processing systems is crucial for two reasons: 1. **Legacy Systems:** Many businesses still rely on legacy applications built on file processing systems. DBAs and developers need to understand them to maintain, interface with, or migrate these systems. 2. **Appreciating DBMS Benefits:** Studying the problems of file processing (data redundancy, inconsistency, program-data dependence) provides a deep appreciation for the features and benefits of modern DBMS. It answers the 'why' behind database principles like data independence, data integrity, and non-redundancy, reinforcing their importance in good design."
    },
    {
        "id": 163,
        "question": "What are the major disadvantages of conventional file processing systems?",
        "answer": "File processing systems suffer from several critical drawbacks: 1. **Data Redundancy and Inconsistency:** The same data is often stored in multiple files, leading to duplication, wasted storage, and potential inconsistencies. 2. **Difficulty in Accessing Data:** Writing ad-hoc queries is hard; new programs need to be written for each new task. 3. **Data Isolation:** Data is scattered in various files, making it difficult to get a unified view. 4. **Integrity Problems:** It is hard to apply consistency constraints (e.g., account balance > 0) across multiple files. 5. **Atomicity Problems:** Ensuring a transaction (like a fund transfer) completes entirely or not at all is difficult to program. 6. **Concurrent Access Anomalies:** Uncontrolled multi-user access can lead to incorrect data."
    },
    {
        "id": 164,
        "question": "What are the five categories of database applications based on scope?",
        "answer": "Databases can be categorized by the number of users and the organizational scope they support: 1. **Personal Database:** Designed to support a single user, typically on a personal computer (e.g., a contacts database). 2. **Workgroup Database:** Supports a small team of users (usually fewer than 25). 3. **Department Database:** Supports the major functions of a single department within an organization (e.g., HR, Marketing). 4. **Enterprise Database:** Supports the organization-wide operations and decision-making of an entire company. It spans multiple departments and is often very complex. 5. **Internet/Web Database:** Accessible to anyone via the internet or an intranet/extranet, supporting global user bases (e.g., Amazon.com, Google Search)."
    },
    {
        "id": 165,
        "question": "What is the difference between an intranet and an extranet?",
        "answer": "Both are private networks, but their access differs: *   **Intranet:** A private network that uses internet technologies (like web servers and browsers) to serve the internal needs of an organization. It is accessible only to the organization's employees, members, or others with internal authorization. It is behind a firewall. *   **Extranet:** An extension of an organization's intranet that provides controlled access to external parties, such as partners, vendors, suppliers, or specific customers. It allows for secure collaboration and business-to-business (B2B) transactions over the internet. An extranet is a semi-private network."
    },
    {
        "id": 166,
        "question": "What are the five components of an Information Systems Architecture?",
        "answer": "The Zachman Framework outlines these five fundamental components that must be aligned for a successful system: 1. **Data (What):** The entities and information the business uses and needs. 2. **Function (How):** The business processes and functions that transform the data. 3. **Network (Where):** The geographical distribution of the business and its systems. 4. **People (Who):** The people involved—the organizational units and actors. 5. **Time (When):** The timing and business cycles that trigger events and processes. 6. **Motivation (Why):** The business goals, strategies, and rules that govern the other components. (Note: Some versions list 6, including Motivation.)"
    },
    {
        "id": 167,
        "question": "What is the Systems Development Life Cycle (SDLC)?",
        "answer": "The SDLC is a structured, phased process for planning, creating, testing, and deploying an information system. It provides a framework for managing the complexity of system development. The traditional phases are: 1. **Planning:** Defining the system's scope, goals, and feasibility. 2. **Analysis:** Determining business requirements. 3. **Design:** Designing the system architecture, databases, and interfaces. 4. **Implementation:** Building, testing, and installing the system. 5. **Maintenance:** Fixing issues, making enhancements, and adapting to new requirements. While often depicted as a waterfall, modern approaches use iterative and agile variations of the SDLC."
    },
    {
        "id": 168,
        "question": "What are the two main types of packaged data models?",
        "answer": "Packaged data models are pre-built, generic data models that can be purchased and customized: 1. **Universal Data Models:** Very generic models that represent common business functions found in almost every organization, such as 'Party' (people and organizations), 'Product', 'Order', 'Invoice', and 'Shipment'. They provide a robust starting point. 2. **Industry-Specific Data Models:** Models tailored to the specific needs, terminology, and processes of a particular industry, such as telecommunications, healthcare, insurance, or retail. They incorporate the industry's standard best practices and regulatory requirements."
    },
    {
        "id": 169,
        "question": "Who are the key members of a systems or database development team?",
        "answer": "A successful project requires a team with diverse skills: 1. **Systems Analyst:** Acts as a liaison between stakeholders and the technical team, gathering and translating business requirements. 2. **Database Designer (Data Architect):** Focuses on designing the database structure, including data models, schemas, and integrity constraints. 3. **Application Programmers:** Write the application code that interacts with the database. 4. **Database Administrator (DBA):** Implements and manages the database environment, ensuring performance, security, and availability. 5. **End Users:** The ultimate consumers of the system, who provide crucial input on requirements and test the final product. 6. **Project Manager:** Oversees the project's budget, schedule, and scope."
    },
    {
        "id": 170,
        "question": "What are the key database activities within the SDLC?",
        "answer": "Database development activities run in parallel with the general SDLC phases: 1. **Enterprise Modeling (Planning):** Analyzing the current data processing environment. 2. **Conceptual Data Modeling (Analysis):** Creating an ERD to identify entities, relationships, and attributes based on business requirements. 3. **Logical Database Design (Design):** Transforming the conceptual model into a logical schema (e.g., relational tables), normalizing the design, and defining integrity rules. 4. **Physical Database Design (Design):** Mapping the logical design to a specific DBMS, defining storage structures, indexing, and partitioning for performance. 5. **Database Implementation (Implementation):** Creating the database, loading data, and implementing application programs. 6. **Database Maintenance (Maintenance):** Tuning performance, fixing bugs, and adapting the schema to new requirements."
    },
    {
        "id": 171,
        "question": "What is an Entity-Relationship Diagram (ERD)?",
        "answer": "An Entity-Relationship Diagram (ERD) is a visual graphical representation of the logical structure of a database. It is a conceptual data model that uses standardized symbols to depict: *   **Entities:** Represented as rectangles. These are the 'things' (e.g., Customer, Order). *   **Attributes:** Represented as ovals or within the entity rectangle. These are the properties of entities (e.g., CustomerName, OrderDate). *   **Relationships:** Represented as diamonds or lines connecting entities. These show how entities are associated (e.g., a Customer 'places' an Order). ERDs are a crucial communication tool between stakeholders and database designers in the early stages of the SDLC."
    },
    {
        "id": 172,
        "question": "What are the characteristics of a good data definition?",
        "answer": "A good data definition, often stored in a data dictionary, is clear, precise, and comprehensive. It should include: 1. **A clear, concise name and description** of the data element. 2. **The data type and length** (e.g., VARCHAR(50), DATE). 3. **The allowable values or range** (e.g., 'Y'/'N', 1-100). 4. **Its nullability** (whether it is required or optional). 5. **The source** of the data. 6. **Ownership** (who is responsible for the data). 7. **Examples** of valid data. 8. **Security and privacy classifications.** This ensures everyone in the organization has a shared understanding of the data's meaning and usage rules."
    },
    {
        "id": 173,
        "question": "What do minimum and maximum cardinality represent in a relationship?",
        "answer": "Cardinality defines the numerical attributes of the relationship between two entities. *   **Minimum Cardinality:** Specifies whether the relationship is mandatory or optional. A minimum of 0 means participation is optional for an entity. A minimum of 1 means participation is mandatory. (e.g., An `Order` *must* be placed by a `Customer` -> min cardinality for Customer is 1). *   **Maximum Cardinality:** Specifies the maximum number of entity instances that can be involved in the relationship. This defines the relationship type: 1 (one) or N (many). (e.g., One `Customer` can place many `Orders` -> max cardinality for Order is N). Together, they are often written as (min, max) - e.g., (1, N)."
    },
    {
        "id": 174,
        "question": "What are the best practices for naming relationships in an ERD?",
        "answer": "Relationship names should be meaningful action verbs or verb phrases that accurately describe the nature of the association between entities. Best practices include: 1. **Use a Verb Phrase:** The name should indicate the action (e.g., 'places', 'is assigned to', 'contains'). 2. **Be Specific:** Avoid vague names like 'has' or 'is related to'. Use 'manages', 'submits', 'belongs to'. 3. **Readable in Both Directions:** The relationship should make sense when read from either entity. For example: `Customer` *places* `Order` / `Order` *is placed by* `Customer`. 4. **Use Present Tense:** This makes the model feel current and active."
    },
    {
        "id": 175,
        "question": "Why is it important to model time-dependent data with time stamps?",
        "answer": "Modeling time-dependent data with time stamps (e.g., `StartDate`, `EndDate`, `LastModifiedDate`) is crucial for maintaining a historical record and understanding the state of data over time. Without it, you can only see the current state, losing all history. This is important for: 1. **Historical Reporting and Auditing:** Tracking changes for compliance and understanding past states. 2. **Temporal Queries:** Answering questions like 'What was the price of this product on January 1st?' or 'Who was the manager of this department in 2020?'. 3. **Correcting Errors:** Allowing you to roll back to a previous valid state if an error is made. This concept is central to temporal databases and slowly changing dimensions in data warehousing."
    },
    {
        "id": 176,
        "question": "What is the difference between total and partial specialization in a supertype/subtype relationship?",
        "answer": "This concept defines whether all instances of a supertype must also be an instance of a subtype. *   **Total Specialization ( completeness constraint):** Every instance of the supertype *must* be an instance of at least one subtype. In an ERD, this is represented by a double line connecting the supertype to the circle. (e.g., Every `Vehicle` in the database MUST be either a `Car` or a `Truck`). *   **Partial Specialization:** An instance of the supertype *can* be an instance of a subtype, but it is not mandatory. It is represented by a single line. (e.g., An `Employee` could be a `Manager` or a `Engineer`, but some employees might not be classified into either subtype)."
    },
    {
        "id": 177,
        "question": "What is the difference between an ERD and an EERD?",
        "answer": "An ERD (Entity-Relationship Diagram) represents the basic concepts of entities, attributes, and relationships. An EERD (Enhanced Entity-Relationship Diagram) extends the classical ER model with additional semantic concepts to represent more complex real-world situations. The key enhancements in an EERD are: 1. **Subtyping and Inheritance:** The ability to model supertype/subtype relationships (generalization/specialization hierarchies), where subtypes inherit attributes and relationships from their supertype. 2. **Union (Category):** Modeling a subtype that can inherit from different supertypes. 3. **More Detailed Constraints:** Expressing disjointness and completeness constraints on subtypes more explicitly."
    },
    {
        "id": 178,
        "question": "What is the difference between the disjoint and overlap rules in subtyping?",
        "answer": "These rules constrain whether an instance of a supertype can be an instance of more than one subtype. *   **Disjoint Rule:** An instance of the supertype can be an instance of *only one* of the subtypes. The subtypes are mutually exclusive. (e.g., A `Person` is either a `Male` or a `Female` - represented by a 'd' in the circle on the EERD). *   **Overlap Rule:** An instance of the supertype can be an instance of *more than one* subtype. The subtypes are not mutually exclusive. (e.g., A `Staff` member at a university could be both a `Teacher` and a `Researcher` - represented by an 'o' in the circle)."
    },
    {
        "id": 179,
        "question": "What are the three main types of business rules?",
        "answer": "Business rules are precise statements that define or constrain some aspect of a business. They are categorized as: 1. **Derivations:** Rules that define how knowledge is derived from other knowledge. They are often formulas or calculations. (e.g., `employee_bonus = yearly_sales * 0.05`). 2. **Structural Assertions (Terms and Facts):** Rules that express static structure and definitions. They are the 'nouns' and 'verbs' of the business. (e.g., 'A customer may place many orders', 'An order must have a valid customer'). 3. **Action Assertions (Constraints):** Rules that constrain the actions of the business, often with conditions. (e.g., 'An employee's salary cannot be decreased', 'A loan application must be approved by a manager if the amount is over $10,000')."
    },
    {
        "id": 180,
        "question": "How is a scenario used to define business rules?",
        "answer": "A scenario is a concise, narrative description of a specific sequence of events or actions within a business process. It is used as a tool to discover, validate, and document business rules. By walking through a realistic 'story' (e.g., 'A customer returns a purchased item to a store'), analysts can identify the constraints, derivations, and structural facts that govern that process. The scenario helps to ask the right questions: 'What data is needed?', 'What checks must be performed?', 'What are the possible outcomes?'. The answers to these questions are formalized into the business rules that the database and application must enforce."
    },
    {
        "id": 181,
        "question": "What are the primary objectives of database normalization?",
        "answer": "The core goals of normalization are to: 1. **Minimize Data Redundancy:** Store each logical data item in only one place to conserve space and prevent update anomalies. 2. **Eliminate Anomalies:** Prevent inconsistencies that arise from insertion, deletion, and update operations. 3. **Simplify Data Integrity:** Make it easier to enforce integrity constraints by structuring data logically. 4. **Produce a Stable Database Structure:** Create a design that is less susceptible to changes in requirements and is logically flexible. The process achieves this by organizing data into tables based on their functional dependencies."
    },
    {
        "id": 182,
        "question": "What are the key properties of a well-defined relation in a database?",
        "answer": "A proper relation in the relational model adheres to these properties: 1. **Unique Name:** Every relation (table) must have a distinct name within its schema. 2. **Atomic Values:** Every attribute (column) must contain only atomic (indivisible) values, satisfying 1NF. 3. **Unique Rows:** Every tuple (row) must be unique; no two rows can be identical. 4. **Unordered Rows:** The order of the rows is not significant for data meaning. 5. **Unordered Columns:** The order of the columns is not significant; each column is identified by its name, not its position. 6. **Unique Attribute Names:** Every attribute within a relation must have a unique name."
    },
    {
        "id": 183,
        "question": "What are the steps to convert a relation into Third Normal Form (3NF)?",
        "answer": "Converting a relation to 3NF is a systematic process: 1. **Ensure 1NF:** Verify that the table has no repeating groups or composite attributes; all values are atomic. 2. **Ensure 2NF:** Identify the primary key and verify that all non-prime attributes are fully functionally dependent on the entire key (remove partial dependencies). 3. **Identify Transitive Dependencies:** Find non-prime attributes that are dependent on another non-prime attribute instead of directly on the primary key. 4. **Remove Transitive Dependencies:** For each transitive dependency (e.g., PK -> A -> B), create a new relation. The determinant (A) becomes the primary key of the new relation, and all attributes dependent on it (B) are moved into it. Leave the determinant (A) as a foreign key in the original relation to maintain the link."
    },
    {
        "id": 184,
        "question": "How is a supertype/subtype relationship mapped to physical database tables?",
        "answer": "The most common mapping strategy for supertype/subtype hierarchies is to create a separate table for the supertype and for each subtype: 1. **Supertype Table:** Contains all attributes common to all subtypes. Its primary key is the primary key for the entire hierarchy. 2. **Subtype Tables:** Each subtype table contains two types of columns: a) The primary key of the supertype (which is also the primary key of the subtype table and a foreign key back to the supertype). b) The attributes that are unique to that specific subtype. This design supports the 'is-a' relationship and allows for efficient querying of all instances (via the supertype) or just specific types (via the subtypes). A discriminator attribute (e.g., `EmployeeType`) is often added to the supertype to indicate which subtype table to look in for additional details."
    },
    {
        "id": 185,
        "question": "How would you describe domain constraints in a database?",
        "answer": "Domain constraints are the most fundamental rules that enforce data integrity at the column level. A domain is the set of all possible valid values that an attribute is allowed to contain. Domain constraints ensure that the value for a given attribute must be: 1. **Of the Correct Data Type:** An `Age` column must be an integer. 2. **Within a Defined Set or Range:** An `Age` value must be between 0 and 120. A `Status` value must be in ('Active', 'Inactive', 'Pending'). 3. **A Member of a Distinct List:** Enforced by `CHECK` constraints or user-defined data types. 4. **Consistent with Nullability:** If a column is defined as `NOT NULL`, it must always contain a value. These constraints are checked whenever a value is inserted or updated."
    },
    {
        "id": 186,
        "question": "What are the four key objectives when selecting a data type for an attribute?",
        "answer": "Choosing the right data type is crucial for efficiency and integrity. The objectives are: 1. **Represent All Possible Values:** The data type must be able to represent all valid values for the attribute, both current and future. 2. **Improve Data Integrity:** The data type should inherently prevent invalid data (e.g., a `DATE` type prevents the entry of 'abc' as a date). 3. **Support All Required Data Manipulations:** The data type must allow for the necessary operations (e.g., you can perform arithmetic on `INTEGER` but not on `VARCHAR`). 4. **Minimize Storage Space:** Choose the most space-efficient data type that meets the other objectives (e.g., use `SMALLINT` instead of `INTEGER` if values will never exceed 32,767)."
    },
    {
        "id": 187,
        "question": "What are the four primary types of indexes and their purposes?",
        "answer": "Indexes are categorized based on their structure and uniqueness: 1. **Unique Primary Index:** Determines the physical storage order of the data in a table (the clustered index). There can be only one per table. It must contain unique values. 2. **Nonunique Primary Index:** Used in some database systems where the primary index does not require uniqueness, but still governs storage. 3. **Unique Secondary Index:** A non-clustered index that enforces uniqueness on a column or set of columns (e.g., a unique constraint on an `Email` column). It provides fast access for lookups. 4. **Nonunique Secondary Index:** A non-clustered index that does not enforce uniqueness. It is used to speed up queries on columns that have many duplicate values and are frequently used in `WHERE` clauses or as join conditions."
    },
    {
        "id": 188,
        "question": "What is denormalization and why would a database designer use it?",
        "answer": "Denormalization is the **strategic process of intentionally introducing redundancy into a previously normalized database table.** It is a performance optimization technique that trades off some degree of data redundancy and potential anomaly risk for improved query speed. Designers use it when: 1. **Query Performance is Critical:** When joins between multiple normalized tables are too slow for frequently run reports or queries. 2. **Heavy Read Operations:** In data warehouses or reporting databases where the vast majority of operations are reads (SELECT), and writes (INSERT/UPDATE) are less frequent and controlled. The goal is to reduce the number of table joins needed by pre-consolidating data, which can dramatically speed up complex queries."
    },
    {
        "id": 189,
        "question": "How do the hierarchical and network database models differ?",
        "answer": "These are two pre-relational data models with key differences: *   **Hierarchical Model:** Structures data in a strict tree-like, parent-child hierarchy. Each parent can have many children, but each child can have only one parent. It efficiently represents one-to-many relationships but struggles with many-to-many relationships. Data access is navigational, following predefined paths. *   **Network Model:** An extension of the hierarchical model that allows a child (member) to have multiple parents (owners). This allows it to directly represent more complex relationships, including many-to-many. It is more flexible than the hierarchical model but is also more complex to design and navigate. Both models lack the simplicity and ad-hoc query capability of the relational model."
    },
    {
        "id": 190,
        "question": "What is the difference between horizontal and vertical partitioning?",
        "answer": "Partitioning is the physical process of splitting a large table into smaller, more manageable pieces: *   **Horizontal Partitioning (Sharding):** Splits a table by **rows**. Each partition has the same columns but contains a different subset of the total rows. This is often done based on a range of values (e.g., orders from 2023, orders from 2024) or a hash key. It is useful for distributing data across storage devices and improving query performance on specific data segments. *   **Vertical Partitioning:** Splits a table by **columns**. One partition contains frequently accessed columns, while another contains less frequently accessed or large (BLOB) columns. This is often used to improve I/O performance for common queries that don't need all the table's data."
    },
    {
        "id": 191,
        "question": "What is the difference between a dynamic view and a materialized view?",
        "answer": "This is a key distinction in how views are implemented and updated: *   **Dynamic View (Standard View):** A virtual table that does not store data. Whenever the view is queried, its defining `SELECT` statement is executed against the underlying base tables in real-time. It always returns the most current data but can be slow if the query is complex. *   **Materialized View (Snapshot):** A physical copy of the view's result set is stored as a separate table. The data is persisted to disk. It is extremely fast to query because it avoids recomputing the join and aggregation, but the data can become stale. The view must be periodically **refreshed** to update its stored data with changes from the base tables. This is a trade-off between performance and data currency."
    },
    {
        "id": 192,
        "question": "What techniques can be used to tune the performance of an operational database?",
        "answer": "Database performance tuning involves multiple strategies: 1. **Query Optimization:** Rewriting application SQL to be more efficient (e.g., avoiding `SELECT *`, using joins instead of subqueries). 2. **Indexing:** Adding appropriate indexes on columns used in `WHERE`, `JOIN`, `ORDER BY`, and `GROUP BY` clauses. 3. **Database Design Adjustment:** Considering denormalization for critical, slow queries. 4. **Hardware Optimization:** Adding more RAM (for buffer cache), using faster disks (SSDs), or adding more CPUs. 5. **Configuration Tuning:** Adjusting DBMS configuration parameters (e.g., memory allocation, parallelism settings). 6. **Statistics Maintenance:** Ensuring the DBMS's optimizer has up-to-date statistics on table sizes and data distribution to choose the best execution plans."
    },
    {
        "id": 193,
        "question": "What are the three main categories of SQL commands?",
        "answer": "SQL commands are broadly classified based on their function: 1. **Data Definition Language (DDL):** Commands used to define, alter, and drop the structure of database objects. These commands implicitly commit transactions. Key commands: `CREATE`, `ALTER`, `DROP`, `TRUNCATE`, `RENAME`. 2. **Data Manipulation Language (DML):** Commands used to manipulate data within existing objects. Key commands: `SELECT` (retrieval), `INSERT` (add), `UPDATE` (modify), `DELETE` (remove). 3. **Data Control Language (DCL):** Commands used to control access to the database and its objects. Key commands: `GRANT` (give privileges), `REVOKE` (take away privileges). Some systems also define Transaction Control Language (TCL) with commands like `COMMIT` and `ROLLBACK`."
    },
    {
        "id": 194,
        "question": "What are the key steps to prepare for creating a database table?",
        "answer": "Thorough preparation before executing a `CREATE TABLE` statement is essential: 1. **Define Attributes:** Finalize the list of column names. 2. **Assign Data Types:** Choose the most appropriate data type, length, and precision for each column. 3. **Identify Keys:** Designate the primary key and any foreign keys. 4. **Establish Constraints:** Decide on `NOT NULL`, `UNIQUE`, `CHECK`, and `DEFAULT` constraints for each column. 5. **Plan Indexes:** Identify which columns will need indexes for performance. 6. **Consider Relationships:** Understand how this table relates to others to define foreign keys correctly. 7. **Review Normalization:** Ensure the table design adheres to the desired normal form to avoid redundancy. This planning is often done using a data modeling tool or ER diagram."
    },
    {
        "id": 195,
        "question": "What are some potential disadvantages of a standardized language like SQL?",
        "answer": "While standardization is largely beneficial, it has some drawbacks: 1. **Complexity:** The SQL standard is vast and complex, making full compliance difficult for vendors and mastery difficult for developers. 2. **Vendor Extensions:** DBMS vendors often add proprietary extensions to standard SQL to provide additional functionality. This can hurt portability between different database systems. 3. **Pace of Innovation:** The formal standards process can be slow, sometimes lagging behind the innovative features introduced by individual vendors. 4. **'Lowest Common Denominator' Effect:** To ensure portability, developers might avoid using powerful vendor-specific features, potentially resulting in less efficient or more complex code."
    },
    {
        "id": 196,
        "question": "What is a table join and why is it fundamental to the relational model?",
        "answer": "A join is a fundamental SQL operation that combines rows from two or more tables based on a related column between them. It is the primary mechanism for querying data across multiple tables, which is the core principle of the relational model. By storing data in separate, normalized tables and using joins to reassemble it, the model achieves its goals of minimizing redundancy and maintaining data integrity. The join operation, typically based on primary key-foreign key relationships, allows the database to reconstruct complex entity relationships at query time, providing tremendous flexibility."
    },
    {
        "id": 197,
        "question": "How would you contrast a database trigger with a stored procedure?",
        "answer": "| Feature | Trigger | Stored Procedure |\n| :--- | :--- | :--- |\n| **Invocation** | Automatically fired (executed) by the DBMS in response to a specific DML event (`INSERT`, `UPDATE`, `DELETE`) on a table. | Explicitly called and executed by a user, application, or another procedure. |\n| **Control** | Event-driven; the programmer has no control over when it runs. | Called on demand; the programmer has full control over execution. |\n| **Parameters** | Cannot accept explicit parameters. They use special pseudo-records (e.g., `NEW` and `OLD`) to access the affected row data. | Can accept input, output, and input/output parameters. |\n| **Transaction Context** | Part of the transaction that caused it to fire. | Can contain its own transaction control statements (`COMMIT`, `ROLLBACK`). |\n| **Common Use Case** | Enforcing complex business rules, auditing, maintaining derived data automatically. | Modularizing application logic, performing complex operations, improving performance."
    },
    {
        "id": 198,
        "question": "What is an outer join and when would you use it?",
        "answer": "An outer join is a type of join that returns not only the matching rows between two tables but also the non-matching rows from one or both tables. The result set includes `NULL` values for columns from the table that lacks a matching row. Types include: *   **LEFT OUTER JOIN:** Returns all rows from the left table and the matched rows from the right table. *   **RIGHT OUTER JOIN:** Returns all rows from the right table and the matched rows from the left table. *   **FULL OUTER JOIN:** Returns all rows when there is a match in either the left or right table. You use an outer join when you need a complete list from one table regardless of whether a corresponding record exists in the other table (e.g., list all customers and their orders, including customers who have never placed an order)."
    },
    {
        "id": 199,
        "question": "What is a subquery and how is it used?",
        "answer": "A subquery (or inner query or nested query) is a `SELECT` statement embedded within the `WHERE` or `HAVING` clause of another SQL statement (the outer query). It is used to return a value or set of values that the outer query uses to complete its search condition. Subqueries are powerful for: 1. **Set Membership:** Using `IN` or `NOT IN` (e.g., `SELECT * FROM Products WHERE CategoryID IN (SELECT CategoryID FROM Categories WHERE Name = 'Beverages')`). 2. **Comparisons:** Using operators like `=`, `>`, `ANY`, `ALL`. 3. **Existence Checks:** Using `EXISTS` or `NOT EXISTS` with correlated subqueries. They allow for dynamic, data-driven query conditions that would be impossible to hard-code."
    },
    {
        "id": 200,
        "question": "What is the difference between embedded SQL and dynamic SQL?",
        "answer": "This distinction lies in when the SQL statement is constructed and prepared: *   **Embedded SQL:** SQL statements are **hard-coded** directly within the source code of a host programming language (like C, COBOL, or Java). The statements are static and known at application compile time. They are pre-compiled into an executable access plan. *   **Dynamic SQL:** SQL statements are constructed and **assembled as strings at runtime** within the application. The application can build the statement on the fly based on user input or other conditions. The DBMS must parse, optimize, and compile the statement at runtime, which adds overhead but provides ultimate flexibility for ad-hoc query tools."
    },
    {
        "id": 201,
        "question": "What is the difference between two-tier and three-tier client-server architecture?",
        "answer": "This refers to the logical separation of an application's components: *   **Two-Tier Architecture:** Has only two logical layers: 1) The **Client Tier** (Presentation Layer), which handles the user interface and application logic, and 2) The **Database Server Tier** (Data Layer), which houses the DBMS and database. The client communicates directly with the database. It is simpler but can lead to performance and scalability issues as the number of clients grows. *   **Three-Tier Architecture:** Introduces a middle tier: 1) **Client Tier** (Presentation Layer: UI), 2) **Application Server Tier** (Business Logic Layer: rules, processing), and 3) **Database Server Tier** (Data Layer). The client talks to the application server, which in turn talks to the database. This improves scalability, security, and flexibility, as business logic is centralized and separated from the data and presentation layers."
    },
    {
        "id": 202,
        "question": "How do SQL and QBE differ as database query languages?",
        "answer": "SQL and QBE offer different paradigms for querying a database: *   **SQL (Structured Query Language):** A text-based, declarative language. The user writes a command in a syntax similar to a sentence to specify what data is wanted. It is a standard, powerful, and precise language used by programmers and power users. *   **QBE (Query-by-Example):** A graphical, visual language. The user retrieves data by filling in templates or putting example elements directly into table skeletons on the screen. It is often considered more intuitive for novice users. Many modern GUI database tools (like Microsoft Access's query designer) translate QBE-like actions into SQL commands behind the scenes. SQL is the universal standard, while QBE is a user-friendly interface that generates SQL."
    },
    {
        "id": 203,
        "question": "What is ODBC and what problem does it solve?",
        "answer": "ODBC (Open Database Connectivity) is a standard application programming interface (API) for accessing database management systems (DBMS). It solves the problem of application **portability** and **interoperability**. Before ODBC, applications had to be written to use the specific API of a single DBMS vendor (e.g., Oracle's OCI). To switch databases, the application had to be rewritten. ODBC provides a universal, vendor-neutral interface. An application written to the ODBC standard can connect to any DBMS (Oracle, SQL Server, MySQL, etc.) for which an ODBC driver exists, dramatically reducing development and maintenance costs."
    },
    {
        "id": 204,
        "question": "What is the difference between a 'thin client' and a 'fat client'?",
        "answer": "This classification depends on how much processing logic is handled by the client machine: *   **Fat Client (Thick Client):** A client machine that runs the majority of the application's logic, including data processing, business rules, and the user interface. It requires significant resources on the client PC and often maintains a direct connection to the database. It can function offline but is complex to deploy and update. *   **Thin Client:** A client machine that primarily handles only the **user interface**. The application's core logic and data processing are executed on one or more application servers. It requires minimal resources on the client PC, is easy to deploy and update, and relies on a constant network connection. Web browsers are the ultimate thin clients."
    },
    {
        "id": 205,
        "question": "Why would an Access user learn VBA?",
        "answer": "While Microsoft Access provides a powerful interface for building databases without code, learning VBA (Visual Basic for Applications) unlocks advanced capabilities: 1. **Complex Functionality:** Create custom functions and procedures that go beyond the built-in wizards and expression builder. 2. **Error Handling:** Write robust code that can gracefully handle unexpected errors and user mistakes. 3. **Automation:** Automate complex, multi-step tasks that involve forms, reports, and data manipulation. 4. **Integration:** Interact with other Windows applications (like Excel, Word, Outlook) through OLE Automation. 5. **Performance:** VBA code can execute complex operations faster than a series of macro actions. 6. **User Interaction:** Create sophisticated custom dialog boxes and user forms for data entry and navigation."
    },
    {
        "id": 206,
        "question": "What is middleware in the context of web-database connectivity?",
        "answer": "In web-database integration, middleware is software that acts as a bridge between a web server and a database server. It resides on the web server and facilitates communication between the two. Its primary role is to: 1. **Receive Requests:** Accept requests from a user's web browser sent to the web server. 2. **Process Business Logic:** Execute the application's programming logic. 3. **Interact with the Database:** Generate database queries, connect to the DBMS, pass the queries, and retrieve the results. 4. **Format Results:** Format the database results (e.g., into HTML, JSON, or XML). 5. **Return Response:** Send the formatted response back to the web server, which then delivers it to the user's browser. Examples include ASP.NET, PHP, Java Servlets, and ColdFusion."
    },
    {
        "id": 207,
        "question": "What are JavaScript and VBScript and how were they used?",
        "answer": "JavaScript and VBScript are client-side scripting languages primarily used to add interactivity and dynamic behavior to web pages within a user's browser. *   **JavaScript:** A versatile language developed by Netscape. It became the dominant client-side scripting standard due to its cross-browser support. It is used for tasks like form validation, creating dynamic menus, and interacting with the HTML Document Object Model (DOM). It is not related to Java. *   **VBScript:** A scripting language from Microsoft based on Visual Basic syntax. It was primarily used to script web pages in Microsoft's Internet Explorer browser. It never achieved cross-browser adoption and is now largely obsolete. Both executed code on the client machine, reducing the load on the web server but depending on the browser's capabilities."
    },
    {
        "id": 208,
        "question": "What are Web Services?",
        "answer": "Web Services are a standardized way for different software applications, running on a variety of platforms and frameworks, to communicate and exchange data over a network (typically the internet). They use open standards: *   **XML** to tag the data. *   **SOAP** (or REST) to define the message format. *   **WSDL** to describe the service available. *   **UDDI** to list what services are available. The key idea is **interoperability**. A Java application on a Linux server can call a web service provided by a .NET application on a Windows server to request data or trigger a process, without needing to know the internal details of the other system."
    },
    {
        "id": 209,
        "question": "Can you provide an overview of XML and its role?",
        "answer": "XML (eXtensible Markup Language) is a meta-markup language that provides a flexible, self-describing way to structure and store data. Its key roles are: 1. **Data Transport:** It is the fundamental language for transmitting data between heterogeneous systems, especially in web services. 2. **Data Storage:** It can be used to store configuration files and complex, hierarchical data. 3. **Separation of Concerns:** It strictly separates data content from its presentation, unlike HTML which mixes them. 4. **Interoperability:** As a text-based, platform-independent standard, it enables data exchange between vastly different applications. XML uses custom tags (e.g., `<price>29.99</price>`) that describe the meaning of the data, making it both human and machine-readable."
    },
    {
        "id": 210,
        "question": "What are the primary website security concerns?",
        "answer": "Website security involves protecting all components of a web application from threats. Primary concerns include: 1. **Unauthorized Data Access:** Preventing hackers from accessing sensitive data in the database (e.g., through SQL Injection attacks). 2. **Data Integrity:** Ensuring data cannot be maliciously altered. 3. **Availability:** Protecting against Denial-of-Service (DoS) attacks that make the site unavailable to users. 4. **Authentication & Authorization:** Ensuring users are who they claim to be (authentication) and can only access data and functions they are permitted to (authorization). 5. **Client-Side Attacks:** Protecting users from cross-site scripting (XSS) and other attacks launched from the website. Security must be addressed at every level: network, operating system, web server, application code, and database."
    },
    {
        "id": 211,
        "question": "What is the role of metadata in a three-layer data architecture?",
        "answer": "In a three-layer architecture (e.g., a data warehouse with operational, reconciled, and derived data layers), metadata acts as the essential 'glue' and 'map' for the entire system. Each layer has its own associated metadata: 1. **Operational Metadata:** Describes the data in the operational source systems - its structure, format, and meaning. It is used for extraction and transformation. 2. **Reconciled Data Metadata (Enterprise Data Warehouse):** Describes the integrated, cleansed, and historical data in the central warehouse. It defines the 'single version of the truth' for the enterprise. 3. **Derived Data Metadata (Data Marts):** Describes the data as it is structured in departmental data marts for specific business analysis. It includes definitions of pre-calculated measures, dimensions, and hierarchies used in OLAP and reporting. This metadata is crucial for developers, administrators, and business users to understand and trust the data."
    },
    {
        "id": 212,
        "question": "Why are operational and informational systems typically kept separate?",
        "answer": "Operational systems (OLTP - Online Transaction Processing) and informational systems (OLAP - Online Analytical Processing) are separated because they have fundamentally different and conflicting purposes and characteristics: | Aspect | Operational System (OLTP) | Informational System (OLAP) |\n| :--- | :--- | :--- |\n| **Primary Purpose** | Run the day-to-day business. | Support decision-making and analysis. |\n| **Data Content** | Current, detailed data. | Historical, summarized, and consolidated data. |\n| **Data Model** | Highly normalized for integrity and update speed. | Denormalized (star/snowflake schema) for query speed. |\n| **Access Pattern** | Many small, quick read/write transactions. | Few, but complex and long-running, read-only queries. |\n| **Users** | Clerks, customers, administrators. | Managers, analysts, data scientists. |\n| Separating them prevents analytical queries from slowing down critical transaction processing and allows each system to be optimized for its specific workload."
    },
    {
        "id": 213,
        "question": "What are the defining characteristics of a data warehouse?",
        "answer": "A data warehouse, as defined by Bill Inmon, has four key characteristics: 1. **Subject-Oriented:** Data is organized around major subjects of the enterprise (e.g., customers, products, sales), rather than by specific operational applications. 2. **Integrated:** Data is gathered from various disparate source systems and made consistent. This involves resolving naming conflicts, data type differences, and encoding inconsistencies. 3. **Nonvolatile:** Once data is entered into the warehouse, it is not updated or deleted in the same way operational data is. It is a stable, read-only environment for analysis. Data is loaded and refreshed, not changed. 4. **Time-Variant:** Data is stored to provide a historical perspective. Every record is accurate with respect to some moment in time, enabling analysis of trends and changes over time."
    },
    {
        "id": 214,
        "question": "Why does an 'information gap' often exist in organizations?",
        "answer": "An information gap exists when decision-makers lack the timely, integrated, and relevant information they need. This gap is caused by two main factors: 1. **Data Silos:** Operational systems are typically built independently for specific functions (e.g., sales, inventory, HR). This leads to data being stored in isolated 'silos' with different structures and meanings, making it difficult to get a unified, enterprise-wide view. 2. **Differing Processing Needs:** The primary design goal of operational systems is to support high-volume transaction processing, not complex decision-support queries. Running analytical queries directly on operational systems would degrade their performance, creating a technical barrier to accessing information. Data warehouses and data marts are built specifically to bridge this gap."
    },
    {
        "id": 215,
        "question": "How do a data warehouse and a data mart differ?",
        "answer": "| Characteristic | Data Warehouse | Data Mart |\n| :--- | :--- | :--- |\n| **Scope** | Enterprise-wide. Centralized. | Departmental or functional. Decentralized. |\n| **Subject** | Multiple integrated subjects. | A single, specific subject (e.g., 'Sales', 'Finance'). |\n| **Data Sources** | Many diverse sources from across the organization. | Fewer sources, often a subset of the data warehouse. |\n| **Size** | Very large (TB to PB range). | Smaller (GB to TB range). |\n| **Implementation Time** | Long (months to years). | Shorter (months). |\n| **Design** | Highly normalized (Inmon) or dimensional (Kimball). | Dimensional (star schema). |\n| A data mart can be a **dependent** subset of a data warehouse or an **independent** stand-alone system built directly from operational sources."
    },
    {
        "id": 216,
        "question": "What is the difference between data administration and database administration?",
        "answer": "These are two distinct but related roles: *   **Data Administration (DA):** A **business-oriented** function focused on the management of data as a strategic corporate asset. DA responsibilities are high-level and include: data planning, defining data standards and policies, managing the data dictionary, and resolving data ownership and privacy issues. *   **Database Administration (DBA):** A **technically-oriented** function focused on the physical implementation and maintenance of the database management system (DBMS). DBA responsibilities are hands-on and include: DBMS installation, database design, performance tuning, backup and recovery, security implementation, and troubleshooting. DA is about *what* data is needed and *why*; DBA is about *how* to store and manage it effectively."
    },
    {
        "id": 217,
        "question": "What are key security features provided by a DBMS?",
        "answer": "A robust DBMS provides multiple layers of security features: 1. **Authentication:** Verifying the identity of a user trying to connect to the database (e.g., via username/password, integrated Windows authentication). 2. **Authorization:** Controlling what a user can do through **privileges** (e.g., `SELECT`, `INSERT`) and **roles** (groups of privileges). 3. **Views:** Providing a security mechanism to hide sensitive columns or rows from users. 4. **Encryption:** Scrambling data so it is unreadable without a key. Can be applied to data at rest (on disk) or in transit (over the network). 5. **Auditing:** Tracking and logging database activity to monitor for suspicious behavior and ensure compliance with regulations. 6. **Data Integrity Controls:** Ensuring data is valid through constraints, which is also a security measure."
    },
    {
        "id": 218,
        "question": "What is concurrency control?",
        "answer": "Concurrency control is the set of techniques a DBMS uses to manage simultaneous operations by multiple users or transactions on a database without allowing them to interfere with each other. Its primary goal is to ensure the **isolation** property of transactions, preventing problems like lost updates, dirty reads, and unrepeatable reads. The two main approaches are: 1. **Pessimistic Concurrency Control:** Prevents conflicts by locking data before it is used. This is the most common method. 2. **Optimistic Concurrency Control:** Allows conflicts to occur but detects them at transaction commit time. If a conflict is detected, the transaction is rolled back. It is used in low-contention environments."
    },
    {
        "id": 219,
        "question": "What is database locking?",
        "answer": "Locking is the primary mechanism used for pessimistic concurrency control. A lock is a flag or variable associated with a data item (e.g., a row, a page, a table) that controls how transactions can access it. The basic rules are: *   Before a transaction can read or write a data item, it must first acquire the appropriate lock. *   If a lock is held by another transaction, the requesting transaction must wait. *   Locks are released when the transaction commits or rolls back. The main types are **shared locks** (for reading) and **exclusive locks** (for writing). The lock manager subsystem of the DBMS is responsible for granting and tracking locks."
    },
    {
        "id": 220,
        "question": "What are the key factors affecting database performance?",
        "answer": "Database performance is influenced by factors across the entire system stack: 1. **Database Design:** Poorly designed schemas (lack of normalization, missing indexes, inefficient data types) are a primary cause of performance issues. 2. **Application Design:** Inefficient SQL queries (e.g., `SELECT *`, unnecessary loops, Cartesian products) and poor application logic place unnecessary load on the database. 3. **DBMS Configuration:** Improperly set memory allocation, cache sizes, and parallelism parameters can cripple performance. 4. **Hardware Resources:** Insufficient CPU power, RAM (leading to excessive disk I/O), and slow disk subsystems are common bottlenecks. 5. **Concurrency:** High levels of user contention for the same data can lead to locking and blocking, slowing down all transactions. Tuning requires a holistic approach addressing all these areas."
    },
    {
        "id": 221,
        "question": "What is the difference between a homogeneous and a heterogeneous distributed database?",
        "answer": "This classification depends on the uniformity of the DBMS software across sites: *   **Homogeneous DDBMS:** All sites in the distributed system use the **same** DBMS software (e.g., all run Oracle). The underlying operating system can be different. This is easier to manage, design, and implement because the software is uniform. *   **Heterogeneous DDBMS:** Different sites may use **different** DBMS software (e.g., one site uses Oracle, another uses SQL Server, another uses IMS). This is much more complex. It requires additional middleware (gateways) to translate queries and resolve differences in data models, query languages, and transaction protocols. It is more common in real-world scenarios where different departments have chosen different systems over time."
    },
    {
        "id": 222,
        "question": "What is a distributed database?",
        "answer": "A distributed database is a single logical database that is physically spread across multiple computers (nodes) located in different geographical locations and connected by a network. The key principle is that users can access the data as if it were all stored on their local machine, without needing to know where the data is physically located. The system is managed by a Distributed Database Management System (DDBMS) that provides: 1. **Location Transparency:** Hides the physical location of the data from the user. 2. **Replication Transparency:** Hides the fact that data may be duplicated (replicated) at multiple sites. 3. **Fragmentation Transparency:** Hides the fact that a table may be split (horizontally or vertically) and stored at different sites."
    },
    {
        "id": 223,
        "question": "Explain the difference between horizontal and vertical fragmentation.",
        "answer": "Fragmentation is the technique of breaking a table into smaller pieces (fragments) and distributing them across different sites in a distributed database. *   **Horizontal Fragmentation:** Splits a table by **rows**. Each fragment contains a subset of the table's rows. For example, a `Customer` table could be fragmented so that customers from the East Coast are stored on a server in New York, and customers from the West Coast are stored on a server in California. A predicate (e.g., `Region = 'East'`) defines each fragment. *   **Vertical Fragmentation:** Splits a table by **columns**. Each fragment contains a subset of the table's columns. For example, one fragment could contain `CustomerID`, `Name`, `Address`, while another contains `CustomerID`, `CreditLimit`, `AccountBalance`. The `CustomerID` column is repeated in all fragments to allow reconstruction of the original row."
    },
    {
        "id": 224,
        "question": "What is concurrency transparency in a distributed system?",
        "answer": "Concurrency transparency is a design goal for distributed DBMS where multiple transactions, which may be executing at different sites, are scheduled in such a way that their concurrent execution produces the same final result as if they had been executed one after the other in some serial order (serializability). The user and the application programmer should be unaware of the concurrency and distribution. The DDBMS, specifically the distributed transaction manager, is responsible for ensuring this property across all sites, making the complexity of coordinating distributed locks and schedules invisible to the user."
    },
    {
        "id": 225,
        "question": "What is snapshot replication?",
        "answer": "Snapshot replication is a method of data replication where the entire current state of a published table (or other database object) is copied and distributed to subscribers at a specific point in time. It does not monitor for incremental updates to data. Instead, every time the snapshot is applied, it completely overwrites the previous snapshot at the subscriber. It is best used for: 1. **Data that changes infrequently** (e.g., lookup tables, dimension tables in a data warehouse). 2. **When a large volume of changes occurs** all at once. 3. **When subscribers do not need to have up-to-the-minute data** and can work with data that is refreshed periodically (e.g., nightly, weekly). It is simpler to set up than transactional replication but can place a significant load on the network when transferring large snapshots."
    },
    {
        "id": 226,
        "question": "What is a file-based system and what are its main disadvantages?",
        "answer": "A file-based system is an application program designed to manipulate data files where information is stored in permanent files. Each application program is designed to manipulate specific data files, and new applications are added as needed. The main disadvantages include: 1) Data redundancy - same information kept in several places leading to inconsistency, 2) Data isolation - difficulty retrieving appropriate data stored in various files, 3) Integrity problems - difficulty maintaining data correctness and consistency, 4) Security problems - constraints regarding accessing privileges, and 5) Concurrency access issues - files are locked when opened, preventing multiple users from accessing the same file simultaneously."
    },
    {
        "id": 227,
        "question": "What is the database approach and how does it differ from file-based systems?",
        "answer": "The database approach is a method of managing large amounts of organizational information that was developed to address the difficulties arising from using file-based systems. Unlike file-based systems where each application manages its own data files, the database approach provides a shared collection of related data that supports the activities of a particular organization. Key differences include: concurrency management allowing multiple users access to the same record, reduced data redundancy, improved data integrity, better security controls, and centralized data management."
    },
    {
        "id": 228,
        "question": "What is a Database Management System (DBMS)?",
        "answer": "A Database Management System (DBMS) is a collection of programs that enables users to create and maintain databases and control all access to them. The primary goal of a DBMS is to provide an environment that is both convenient and efficient for users to retrieve and store information. It serves as an interface between the database and users/application programs, ensuring data is organized and accessible while maintaining security and integrity."
    },
    {
        "id": 229,
        "question": "What are the key properties of a database?",
        "answer": "A database has the following properties: 1) It is a representation of some aspect of the real world or a collection of data elements representing real-world information, 2) It is logical, coherent and internally consistent, 3) It is designed, built and populated with data for a specific purpose, 4) Each data item is stored in a field, and 5) A combination of fields makes up a table. A database can contain many tables that are related to each other."
    },
    {
        "id": 230,
        "question": "What are the main characteristics and benefits of a database system?",
        "answer": "The main characteristics and benefits include: 1) Self-describing nature - contains both data and metadata, 2) Insulation between program and data (program-data independence), 3) Support for multiple views of data, 4) Sharing of data and multiuser system support, 5) Control of data redundancy, 6) Data sharing capabilities, 7) Enforcement of integrity constraints, 8) Restriction of unauthorized access, 9) Data independence, 10) Transaction processing capabilities, and 11) Backup and recovery facilities."
    },
    {
        "id": 231,
        "question": "What is data independence and why is it important?",
        "answer": "Data independence is the immunity of user applications to changes made in the definition and organization of data. It is important because it allows changes to the data structure without affecting application programs. There are two types: logical data independence (ability to change logical schema without changing external schema) and physical data independence (immunity of internal model to changes in physical model). This separation ensures that applications continue to work even when database structures are modified."
    },
    {
        "id": 232,
        "question": "What are the different types of data models?",
        "answer": "The main types of data models are: 1) High-level conceptual data models (like Entity Relationship model) that provide concepts close to how people perceive data, 2) Record-based logical data models including relational data models (data as tables/relations), network data models (data as record types with set types), and hierarchical data models (data as hierarchical tree structures)."
    },
    {
        "id": 233,
        "question": "What is data modeling and what is its purpose in database design?",
        "answer": "Data modeling is the first step in the process of database design, sometimes considered a high-level and abstract design phase (conceptual design). The purpose is to describe: the data contained in the database (entities), the relationships between data items, and the constraints on data. It results in a (semi) formal representation of the database structure that is easy to understand and serves as a reference to ensure all user requirements are met before implementation."
    },
    {
        "id": 234,
        "question": "What are the different degrees of data abstraction in database design?",
        "answer": "The degrees of data abstraction include: 1) External models - represent the user's view of the database containing multiple different external views, 2) Conceptual models - provide flexible data-structuring capabilities and present a 'community view' of the entire database, 3) Internal models - closer to physical level representation (relational, network, hierarchical), and 4) Physical models - the physical representation of the database with the lowest level of abstraction dealing with storage details."
    },
    {
        "id": 235,
        "question": "How are database management systems classified?",
        "answer": "DBMS can be classified based on: 1) Data model - relational, hierarchical, network, or object-oriented models, 2) User numbers - single-user or multiuser database systems, and 3) Database distribution - centralized systems (DBMS and database stored at single site), distributed database systems (database distributed across multiple sites), homogeneous distributed systems (same DBMS software), or heterogeneous distributed systems (different DBMS software with common support for data exchange)."
    },
    {
        "id": 236,
        "question": "What is the Relational Data Model and who introduced it?",
        "answer": "The relational data model was introduced by E. F. Codd in 1970. It describes the world as 'a collection of inter-related relations (or tables)' and has provided the basis for research on data theory, numerous database design methodologies, the SQL standard, and almost all modern commercial database management systems."
    },
    {
        "id": 237,
        "question": "What are the key components of the relational data model?",
        "answer": "The key components are: 1) Relation/Table - a subset of the Cartesian product of domains, 2) Tuple/Row - a group of related data values, 3) Attribute/Column/Field - defines the record characteristics, 4) Domain - a set of acceptable values for a column, and 5) Degree - the number of attributes in a table."
    },
    {
        "id": 238,
        "question": "What are the properties of a table in the relational model?",
        "answer": "Properties include: 1) Distinct table name, 2) No duplicate rows, 3) Atomic entries in columns (no repeating groups), 4) Entries from same domain based on data type, 5) No operations combining different data types, 6) Distinct attribute names, 7) Insignificant column sequence, and 8) Insignificant row sequence."
    },
    {
        "id": 239,
        "question": "What is an Entity-Relationship (ER) data model?",
        "answer": "The ER data model is well-suited for database modeling because it is fairly abstract and easy to discuss. It is based on two concepts: entities (tables holding specific information) and relationships (associations between entities). ER models are represented by ER diagrams and are readily translated to relations."
    },
    {
        "id": 240,
        "question": "What are the different types of entities in ER modeling?",
        "answer": "Entity types include: 1) Independent entities (kernels) - backbone of database with primary keys not being foreign keys, 2) Dependent entities - depend on other tables for meaning and connect kernels together, and 3) Characteristic entities - provide more information about another table and represent multivalued attributes."
    },
    {
        "id": 241,
        "question": "What are the different types of attributes in ER modeling?",
        "answer": "Attribute types include: 1) Simple attributes - drawn from atomic value domains (single-valued), 2) Composite attributes - consist of a hierarchy of attributes, 3) Multivalued attributes - have a set of values for each entity, and 4) Derived attributes - contain values calculated from other attributes."
    },
    {
        "id": 242,
        "question": "What are the different types of keys in database systems?",
        "answer": "Key types include: 1) Candidate key - unique and minimal identifier, 2) Primary key - selected candidate key for identifying tuples, 3) Composite key - composed of two or more attributes, 4) Secondary key - used strictly for retrieval, 5) Alternate key - candidate keys not chosen as primary key, and 6) Foreign key - references primary key in another table."
    },
    {
        "id": 243,
        "question": "What are the main types of relationships in ER modeling?",
        "answer": "Relationship types include: 1) One-to-many (1:M) - should be the norm in relational databases, 2) One-to-one (1:1) - should be rare and may indicate entities belong in same table, 3) Many-to-many (M:N) - implemented through composite entities and broken into two 1:M relationships, and 4) Unary/recursive - relationship between occurrences of the same entity set."
    },
    {
        "id": 244,
        "question": "What is relational algebra and what are its fundamental operations?",
        "answer": "Relational algebra is a procedural query language that provides a set of operations to manipulate relations. Fundamental operations include: 1) Selection (σ) - selects rows satisfying a predicate, 2) Projection (π) - selects specific columns, 3) Union (∪) - combines tuples from two relations, 4) Set difference (−) - finds tuples in one relation not in another, 5) Cartesian product (×) - combines all tuples from two relations, and 6) Rename (ρ) - renames relations or attributes."
    },
    {
        "id": 245,
        "question": "What are the additional relational algebra operations derived from fundamental ones?",
        "answer": "Additional operations include: 1) Join (⨝) - combines tuples from two relations based on matching condition (derived from selection and Cartesian product), 2) Natural join - equijoin that automatically matches columns with same names, 3) Outer joins (left, right, full) - preserve tuples with no matching counterparts, 4) Division (÷) - finds all values of one relation that are associated with all values of another, and 5) Intersection (∩) - finds common tuples between two relations."
    },
    {
        "id": 246,
        "question": "What is concurrency control and why is it important in database systems?",
        "answer": "Concurrency control manages simultaneous access to the database by multiple users while maintaining data consistency. It is crucial because without proper control, concurrent transactions can lead to problems like: 1) Lost updates - one transaction overwrites another's changes, 2) Dirty reads - reading uncommitted data, 3) Non-repeatable reads - different values read in same transaction, and 4) Phantom reads - new rows appearing during transaction."
    },
    {
        "id": 247,
        "question": "What are the main concurrency control protocols?",
        "answer": "Main protocols include: 1) Lock-based protocols - use shared and exclusive locks to control access, 2) Two-phase locking (2PL) - growing phase (acquiring locks) and shrinking phase (releasing locks), 3) Timestamp-based protocols - order transactions based on timestamps, 4) Multi-version concurrency control (MVCC) - maintain multiple versions of data items, and 5) Optimistic concurrency control - assume conflicts are rare and check at commit time."
    },
    {
        "id": 248,
        "question": "What are database transactions and what are the ACID properties?",
        "answer": "A transaction is a logical unit of work that contains one or more SQL statements. ACID properties ensure transaction reliability: 1) Atomicity - all or nothing execution, 2) Consistency - preserves database constraints, 3) Isolation - concurrent transactions don't interfere, and 4) Durability - committed changes persist despite failures."
    },
    {
        "id": 249,
        "question": "What is transaction scheduling and what are conflict serializability and view serializability?",
        "answer": "Transaction scheduling determines the order of operation execution. Conflict serializability requires that conflicting operations (read-write, write-read, write-write) of different transactions appear in the same order. View serializability is less strict and allows schedules where transactions see the same data views as some serial execution, even if conflict orders differ."
    },
    {
        "id": 250,
        "question": "What are the different transaction isolation levels in SQL?",
        "answer": "SQL isolation levels include: 1) Read Uncommitted - allows dirty reads, 2) Read Committed - prevents dirty reads but allows non-repeatable reads, 3) Repeatable Read - prevents dirty and non-repeatable reads but allows phantom reads, and 4) Serializable - prevents all concurrency problems but reduces performance. Each level offers different trade-offs between consistency and concurrency."
    },
    {
        "id": 251,
        "question": "What is database recovery and what techniques are used?",
        "answer": "Database recovery restores the database to a consistent state after failures. Techniques include: 1) Log-based recovery - maintains audit trail of changes, 2) Write-ahead logging (WAL) - log records must be written before actual data updates, 3) Checkpoints - periodic saving of database state, 4) Shadow paging - maintains two page tables during transactions, and 5) ARIES algorithm - widely used recovery algorithm that uses log analysis and redo/undo phases."
    },
    {
        "id": 252,
        "question": "What are deadlocks in database systems and how are they handled?",
        "answer": "Deadlocks occur when two or more transactions are waiting for each other to release locks, creating a circular wait. Handling methods include: 1) Prevention - ensuring deadlocks cannot occur through ordering or timeout mechanisms, 2) Avoidance - using resource allocation graphs and banker's algorithm, 3) Detection - using wait-for graphs to identify cycles, and 4) Recovery - aborting one or more transactions to break the deadlock, with victim selection based on factors like transaction age or work completed."
    },
    {
        "id": 253,
        "question": "What is query optimization and what are the main approaches?",
        "answer": "Query optimization is the process of selecting the most efficient execution strategy for a query. Main approaches include: 1) Rule-based optimization - uses heuristic rules to transform queries, 2) Cost-based optimization - estimates costs of different execution plans using statistics, 3) Semantic optimization - uses constraints and semantics to simplify queries, and 4) Physical optimization - considers storage structures and access methods. The optimizer considers factors like join order, index usage, and access methods."
    },
    {
        "id": 254,
        "question": "What are database indices and what types exist?",
        "answer": "Indices are data structures that improve data retrieval speed. Types include: 1) B-tree indices - balanced tree structure for range queries, 2) Hash indices - for equality queries using hash functions, 3) Bitmap indices - for columns with few distinct values, 4) Clustered indices - determine physical storage order of data, 5) Non-clustered indices - separate structure from data storage, and 6) Composite indices - on multiple columns. Indices trade off query performance against update overhead and storage space."
    },
    {
        "id": 255,
        "question": "What is normalization and what are the normal forms?",
        "answer": "Normalization is the process of organizing data to reduce redundancy and improve data integrity. The normal forms are: 1) 1NF - eliminate repeating groups, ensure atomic values, 2) 2NF - remove partial dependencies (all non-key attributes fully dependent on PK), 3) 3NF - remove transitive dependencies (no non-key attribute dependent on another non-key attribute), 4) BCNF - every determinant is a candidate key, 5) 4NF - remove multi-valued dependencies, and 6) 5NF - remove join dependencies. Each normal form addresses specific types of redundancy and update anomalies."
    },
    {
        "id": 256,
        "question": "What is a JOIN in SQL and why is it important?",
        "answer": "A JOIN is an SQL operation that combines rows from two or more tables based on a related column. It's fundamental to relational databases because it allows you to retrieve data from multiple normalized tables in a single query, reconstructing the relationships defined by foreign keys."
    },
    {
        "id": 257,
        "question": "Explain the difference between INNER JOIN and OUTER JOIN.",
        "answer": "INNER JOIN returns only rows that have matching values in both tables. OUTER JOIN returns all rows from one table and matching rows from the other, with NULLs where no match exists. OUTER JOIN has three types: LEFT (all from left table), RIGHT (all from right table), and FULL (all from both tables)."
    },
    {
        "id": 258,
        "question": "What is a LEFT JOIN and when would you use it?",
        "answer": "LEFT JOIN returns all records from the left table and matching records from the right table. If no match exists, NULL values are returned for right table columns. Use it when you need all records from the primary table regardless of whether they have related records, like listing all customers and their orders, including customers with no orders."
    },
    {
        "id": 259,
        "question": "Explain the concept of a self-join with an example.",
        "answer": "A self-join is when a table is joined with itself. It's useful for querying hierarchical data stored in a single table. For example, in an EMPLOYEE table with EmployeeID and ManagerID columns, you can self-join to get each employee's manager name: SELECT e1.Name as Employee, e2.Name as Manager FROM Employee e1 LEFT JOIN Employee e2 ON e1.ManagerID = e2.EmployeeID."
    },
    {
        "id": 260,
        "question": "What is a CROSS JOIN and when might it be useful?",
        "answer": "CROSS JOIN returns the Cartesian product of two tables - every row from the first table combined with every row from the second. It's useful for generating combinations, like matching all products with all sizes for an inventory matrix, or creating test data. Be careful as it can produce very large result sets."
    },
    {
        "id": 261,
        "question": "Explain NATURAL JOIN and its potential pitfalls.",
        "answer": "NATURAL JOIN automatically joins tables based on columns with the same name. While convenient, it's risky because it silently uses all common columns, which might not be intended for joining. If table schemas change, NATURAL JOIN behavior changes unexpectedly. It's better to use explicit JOIN with ON clause for clarity and maintainability."
    },
    {
        "id": 262,
        "question": "What is the difference between USING and ON in JOIN clauses?",
        "answer": "USING is used when join columns have identical names and you want to join on specific columns: JOIN ON table1.col = table2.col. USING(col) automatically removes the duplicate column from the result. ON is more flexible, allowing any join condition, including non-equality conditions, and both join columns appear in the result."
    },
    {
        "id": 263,
        "question": "How does a FULL OUTER JOIN differ from LEFT and RIGHT JOIN?",
        "answer": "FULL OUTER JOIN returns all rows from both tables, matching where possible. LEFT JOIN returns all rows from left table plus matches from right. RIGHT JOIN returns all rows from right table plus matches from left. FULL OUTER JOIN is like combining LEFT and RIGHT JOINs - it shows all records from both tables with NULLs where no match exists."
    },
    {
        "id": 264,
        "question": "What is a theta join in relational algebra?",
        "answer": "A theta join is a join that combines tables based on a general condition (theta) that may include operators other than equality (=), such as <, >, ≤, ≥, or ≠. It's the most general form of join. When the condition uses only equality, it's specifically called an equijoin."
    },
    {
        "id": 265,
        "question": "Explain the performance implications of using JOINs in SQL.",
        "answer": "JOINs can be expensive operations as they may require scanning large tables and creating temporary result sets. Performance depends on factors like proper indexing, join order, table sizes, and data distribution. Nested loop joins work well for small tables, hash joins for large unsorted data, and merge joins for sorted data. Proper indexing on join columns is crucial for performance."
    },
    {
        "id": 266,
        "question": "Write a SQL query to find employees who have never placed an order.",
        "answer": "SELECT e.EmployeeID, e.Name FROM Employees e LEFT JOIN Orders o ON e.EmployeeID = o.EmployeeID WHERE o.OrderID IS NULL; This uses LEFT JOIN to include all employees and filters for those with no matching orders."
    },
    {
        "id": 267,
        "question": "How do you join more than two tables in SQL?",
        "answer": "You can chain JOIN operations: SELECT * FROM Table1 t1 JOIN Table2 t2 ON t1.id = t2.t1_id JOIN Table3 t3 ON t2.id = t3.t2_id. The order of joins affects performance but not the final result set in most databases. Each join builds on the result of previous joins."
    },
    {
        "id": 268,
        "question": "What is an equijoin?",
        "answer": "An equijoin is a join that uses only equality comparisons (=) in the join condition. It's the most common type of join. For example: SELECT * FROM Employees e JOIN Departments d ON e.DeptID = d.DeptID. All INNER, LEFT, RIGHT, and FULL joins can be equijoins if they use equality conditions."
    },
    {
        "id": 269,
        "question": "Explain the concept of a non-equijoin with an example.",
        "answer": "A non-equijoin uses conditions other than equality, like <, >, BETWEEN, etc. Example: Find salary grades for employees: SELECT e.Name, e.Salary, g.Grade FROM Employees e JOIN SalaryGrades g ON e.Salary BETWEEN g.MinSalary AND g.MaxSalary. This matches each employee to their appropriate salary grade range."
    },
    {
        "id": 270,
        "question": "What happens if you omit the JOIN condition?",
        "answer": "Omitting the JOIN condition (or using CROSS JOIN explicitly) produces a Cartesian product, where every row from the first table is paired with every row from the second. If Table1 has 100 rows and Table2 has 1000 rows, the result will have 100,000 rows. This is rarely intended and can crash applications if tables are large."
    },
    {
        "id": 271,
        "question": "How do you join a table with itself to find duplicate records?",
        "answer": "SELECT DISTINCT a.* FROM Table a JOIN Table b ON a.id != b.id AND a.column1 = b.column1 AND a.column2 = b.column2; This self-join finds rows with identical values in specified columns but different IDs, indicating potential duplicates."
    },
    {
        "id": 272,
        "question": "What is the difference between JOIN and UNION?",
        "answer": "JOIN combines columns from different tables based on relationships, horizontally extending the result set. UNION combines rows from similar tables, vertically stacking results. JOIN requires a related column; UNION requires compatible column structures. JOIN can return more columns; UNION returns same number of columns."
    },
    {
        "id": 273,
        "question": "Explain how a hash join works internally.",
        "answer": "A hash join builds a hash table in memory for the smaller table (build phase), using join columns as hash keys. It then scans the larger table (probe phase), hashes its join columns, and looks up matches in the hash table. Hash joins are efficient for large, unsorted tables and equijoins."
    },
    {
        "id": 274,
        "question": "When would you use a CROSS APPLY vs JOIN in SQL Server?",
        "answer": "CROSS APPLY is like a join that can use columns from the left table as parameters in a table-valued function on the right. It's useful when the right side is a function or subquery that depends on the left side. OUTER APPLY is the equivalent of LEFT JOIN. Regular JOIN can't parameterize the right side based on left table values."
    },
    {
        "id": 275,
        "question": "What is a semi-join and how is it implemented in SQL?",
        "answer": "A semi-join returns rows from the first table that have at least one match in the second table, without returning any columns from the second. In SQL, it's implemented using EXISTS or IN. For example: SELECT * FROM Customers c WHERE EXISTS (SELECT 1 FROM Orders o WHERE o.CustomerID = c.CustomerID)."
    },
    {
        "id": 276,
        "question": "Explain anti-join with an example.",
        "answer": "An anti-join returns rows from the first table that have no matches in the second table. In SQL, it's implemented using NOT EXISTS or NOT IN. Example: SELECT * FROM Products p WHERE NOT EXISTS (SELECT 1 FROM OrderItems oi WHERE oi.ProductID = p.ProductID) finds products never ordered."
    },
    {
        "id": 277,
        "question": "How do indexes affect JOIN performance?",
        "answer": "Indexes on join columns dramatically improve JOIN performance by allowing rapid lookups instead of full table scans. Without indexes, the database may need to scan both tables fully for each join operation. For best performance, index foreign key columns and columns frequently used in JOIN conditions."
    },
    {
        "id": 278,
        "question": "What is a natural join and why is it generally avoided?",
        "answer": "A natural join automatically joins tables on columns with the same name. It's avoided because: 1) It's implicit - you can't control which columns are used for joining, 2) Schema changes can silently break queries, 3) It can unintentionally join on wrong columns if table designs change, 4) It reduces query readability and maintainability."
    },
    {
        "id": 279,
        "question": "Write a query using LEFT JOIN to find departments with no employees.",
        "answer": "SELECT d.* FROM Departments d LEFT JOIN Employees e ON d.DeptID = e.DeptID WHERE e.EmployeeID IS NULL; This returns all departments, then filters to those with no matching employees (where EmployeeID is NULL from the right table)."
    },
    {
        "id": 280,
        "question": "Explain the difference between a merge join and a hash join.",
        "answer": "Merge join requires both inputs sorted on join columns and works by simultaneously iterating through both sorted lists. It's efficient for large datasets that are already sorted. Hash join builds a hash table for one input and probes with the other. Merge join is good for range conditions; hash join excels at equijoins on unsorted data."
    },
    {
        "id": 281,
        "question": "What is a lateral join in PostgreSQL?",
        "answer": "A LATERAL join allows a subquery in the FROM clause to reference columns from preceding tables in the same FROM clause. It's like a correlated subquery but can return multiple columns and rows. Example: SELECT * FROM Employees e, LATERAL (SELECT * FROM Orders o WHERE o.EmployeeID = e.EmployeeID ORDER BY o.Date DESC LIMIT 1) AS last_order."
    },
    {
        "id": 282,
        "question": "How do you handle NULL values in JOIN conditions?",
        "answer": "NULL values never match anything in JOIN conditions because NULL = NULL evaluates to NULL, not TRUE. To include NULLs, you need special handling: use IS NULL in the condition, or use COALESCE to replace NULLs with a sentinel value. For example: ON a.col = b.col OR (a.col IS NULL AND b.col IS NULL)."
    },
    {
        "id": 283,
        "question": "What is the difference between a straight join and a nested loop join?",
        "answer": "A straight join is a join that processes tables in the order specified, without reordering by the optimizer. A nested loop join is an implementation method that iterates through each row of one table and scans the other for matches. Straight join is a directive; nested loop is an execution strategy."
    },
    {
        "id": 284,
        "question": "Explain how to optimize queries with multiple JOINs.",
        "answer": "To optimize multi-join queries: 1) Join smaller tables first, 2) Ensure proper indexes on all join columns, 3) Filter rows as early as possible with WHERE clauses, 4) Consider join order, 5) Use appropriate join types, 6) Avoid joining unnecessary tables, 7) Analyze execution plans to identify bottlenecks."
    },
    {
        "id": 285,
        "question": "Write a query using FULL OUTER JOIN to find mismatched records between two tables.",
        "answer": "SELECT COALESCE(a.id, b.id) as id, a.value as value_a, b.value as value_b FROM TableA a FULL OUTER JOIN TableB b ON a.id = b.id WHERE a.id IS NULL OR b.id IS NULL; This finds records that exist in one table but not the other."
    },
    {
        "id": 316,
        "question": "What are aggregate functions in SQL?",
        "answer": "Aggregate functions perform calculations on multiple rows and return a single result. Common ones: COUNT() - counts rows, SUM() - totals numeric values, AVG() - calculates average, MAX() - finds maximum, MIN() - finds minimum. They're often used with GROUP BY to create summary reports."
    },
    {
        "id": 317,
        "question": "Explain the COUNT function with examples.",
        "answer": "COUNT(*) counts all rows including NULLs. COUNT(column) counts non-NULL values in column. COUNT(DISTINCT column) counts unique non-NULL values. Examples: SELECT COUNT(*) FROM Employees; SELECT COUNT(ManagerID) FROM Employees; SELECT COUNT(DISTINCT DepartmentID) FROM Employees;"
    },
    {
        "id": 318,
        "question": "What is the purpose of the GROUP BY clause?",
        "answer": "GROUP BY groups rows with same values in specified columns, allowing aggregate functions to be applied per group. Example: SELECT DepartmentID, COUNT(*) as EmpCount, AVG(Salary) as AvgSalary FROM Employees GROUP BY DepartmentID; This gives one row per department with employee count and average salary."
    },
    {
        "id": 319,
        "question": "Explain the HAVING clause and how it differs from WHERE.",
        "answer": "HAVING filters groups after aggregation, while WHERE filters rows before grouping. HAVING can use aggregate functions; WHERE cannot. Example: SELECT DepartmentID, AVG(Salary) as AvgSalary FROM Employees GROUP BY DepartmentID HAVING AVG(Salary) > 50000; This filters departments after calculating average."
    },
    {
        "id": 320,
        "question": "How does SUM function handle NULL values?",
        "answer": "SUM ignores NULL values - they don't affect the total. If all values in a group are NULL, SUM returns NULL (or 0 in some databases like SQL Server). It's important to consider this when calculating percentages or averages that might be affected by NULLs."
    },
    {
        "id": 321,
        "question": "What is the difference between COUNT(*) and COUNT(column)?",
        "answer": "COUNT(*) counts all rows regardless of NULLs. COUNT(column) counts only non-NULL values in that column. If column has NULLs, COUNT(column) returns a smaller number. Example: If Employees table has 100 rows but 10 have NULL ManagerID, COUNT(*) = 100, COUNT(ManagerID) = 90."
    },
    {
        "id": 322,
        "question": "Explain the AVG function and how it handles NULLs.",
        "answer": "AVG calculates the arithmetic mean of non-NULL values. It ignores NULLs, so average is sum of non-NULL values divided by count of non-NULL values. Example: Values (10, 20, NULL, 30) give AVG = (10+20+30)/3 = 20, not 15. Use COALESCE to replace NULLs if different behavior needed."
    },
    {
        "id": 323,
        "question": "How do you use multiple aggregate functions in one query?",
        "answer": "SELECT DepartmentID, COUNT(*) as EmpCount, SUM(Salary) as TotalSalary, AVG(Salary) as AvgSalary, MIN(Salary) as MinSalary, MAX(Salary) as MaxSalary FROM Employees GROUP BY DepartmentID; Multiple aggregates can be combined, each calculated independently from the same group."
    },
    {
        "id": 324,
        "question": "What is the ROLLUP extension in GROUP BY?",
        "answer": "ROLLUP creates subtotals and grand totals. Example: SELECT DepartmentID, JobTitle, SUM(Salary) FROM Employees GROUP BY ROLLUP(DepartmentID, JobTitle); Produces totals per department, per job title within department, and grand total. NULLs in result indicate subtotal rows."
    },
    {
        "id": 325,
        "question": "Explain the CUBE extension in GROUP BY.",
        "answer": "CUBE generates all possible subtotal combinations. SELECT DepartmentID, JobTitle, SUM(Salary) FROM Employees GROUP BY CUBE(DepartmentID, JobTitle); Produces totals for all combinations: per department, per job title, per department+title, and grand total. More comprehensive than ROLLUP but more computationally expensive."
    },
    {
        "id": 326,
        "question": "What is GROUPING SETS and when would you use it?",
        "answer": "GROUPING SETS allows specifying exactly which groupings you want. Example: SELECT DepartmentID, JobTitle, SUM(Salary) FROM Employees GROUP BY GROUPING SETS ((DepartmentID), (JobTitle), ()); This gives only department totals, job title totals, and grand total, avoiding unnecessary combinations from ROLLUP or CUBE."
    },
    {
        "id": 327,
        "question": "How do you filter aggregated results using HAVING?",
        "answer": "SELECT DepartmentID, AVG(Salary) as AvgSalary FROM Employees GROUP BY DepartmentID HAVING AVG(Salary) > 60000 AND COUNT(*) > 5; HAVING can use multiple aggregate conditions, and can include non-aggregate columns that appear in GROUP BY."
    },
    {
        "id": 328,
        "question": "Explain the difference between WHERE and HAVING with an example.",
        "answer": "WHERE filters before grouping: SELECT DepartmentID, AVG(Salary) FROM Employees WHERE HireDate > '2020-01-01' GROUP BY DepartmentID; Only recently hired employees are considered. HAVING filters after grouping: SELECT DepartmentID, AVG(Salary) FROM Employees GROUP BY DepartmentID HAVING AVG(Salary) > 50000; All employees considered, then departments filtered."
    },
    {
        "id": 329,
        "question": "What is the purpose of the DISTINCT keyword with aggregate functions?",
        "answer": "DISTINCT ensures aggregate functions consider only unique values. SELECT COUNT(DISTINCT DepartmentID) FROM Employees; counts unique departments. SELECT SUM(DISTINCT Salary) FROM Employees; sums unique salary values (rarely useful). Useful for counting distinct categories or preventing duplicate counting."
    },
    {
        "id": 330,
        "question": "How do you calculate percentages using aggregate functions?",
        "answer": "SELECT DepartmentID, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM Employees) as Percentage FROM Employees GROUP BY DepartmentID; Or using window functions: SELECT DepartmentID, COUNT(*) * 100.0 / SUM(COUNT(*)) OVER() as Percentage FROM Employees GROUP BY DepartmentID;"
    },
    {
        "id": 331,
        "question": "Explain how to use aggregate functions with CASE statements.",
        "answer": "SELECT DepartmentID, COUNT(*) as Total, SUM(CASE WHEN Gender = 'M' THEN 1 ELSE 0 END) as MaleCount, SUM(CASE WHEN Gender = 'F' THEN 1 ELSE 0 END) as FemaleCount FROM Employees GROUP BY DepartmentID; This creates conditional aggregates, like pivot tables."
    },
    {
        "id": 332,
        "question": "What is the difference between SUM and COUNT?",
        "answer": "SUM adds numeric values, COUNT counts rows or values. SUM(column) returns total of column values; COUNT(column) returns number of non-NULL entries. SUM can overflow with large numbers; COUNT doesn't have this issue. SUM only works on numeric data; COUNT works on any data type."
    },
    {
        "id": 333,
        "question": "How do you find the top N per group using aggregates?",
        "answer": "SELECT DepartmentID, EmployeeID, Salary FROM ( SELECT DepartmentID, EmployeeID, Salary, ROW_NUMBER() OVER (PARTITION BY DepartmentID ORDER BY Salary DESC) as rn FROM Employees ) t WHERE rn <= 3; This gives top 3 highest-paid employees per department using window functions."
    },
    {
        "id": 334,
        "question": "Explain the use of aggregate functions in window functions.",
        "answer": "SELECT EmployeeID, DepartmentID, Salary, AVG(Salary) OVER (PARTITION BY DepartmentID) as DeptAvg, Salary - AVG(Salary) OVER (PARTITION BY DepartmentID) as DiffFromAvg FROM Employees; Window aggregates calculate values without collapsing rows, showing each row alongside group statistics."
    },
    {
        "id": 335,
        "question": "What is the MEDIAN function and how do you calculate it?",
        "answer": "Not all databases have built-in MEDIAN. PostgreSQL: PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY Salary). SQL Server: PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY Salary) OVER(). MySQL: Set variables or use subqueries with row numbering. Median is the middle value when data is sorted."
    },
    {
        "id": 336,
        "question": "How do you calculate running totals with aggregate functions?",
        "answer": "SELECT OrderID, OrderDate, Amount, SUM(Amount) OVER (ORDER BY OrderDate) as RunningTotal FROM Orders; Window functions with ORDER BY create running totals. For running totals per group: SUM(Amount) OVER (PARTITION BY CustomerID ORDER BY OrderDate) as CustomerRunningTotal."
    },
    {
        "id": 337,
        "question": "What is the difference between COUNT(1) and COUNT(*)?",
        "answer": "No practical difference - both count all rows. COUNT(1) counts occurrences of the constant 1 for each row; COUNT(*) counts rows. Modern optimizers treat them identically. Some developers prefer COUNT(*) as clearer intent. Both ignore NULLs only when counting specific columns."
    },
    {
        "id": 338,
        "question": "Explain how to use GROUP BY with multiple columns.",
        "answer": "SELECT DepartmentID, JobTitle, COUNT(*) as EmpCount FROM Employees GROUP BY DepartmentID, JobTitle ORDER BY DepartmentID, JobTitle; Groups by combinations of department and job title, showing how many employees have each specific role in each department."
    },
    {
        "id": 339,
        "question": "How do you handle NULLs in GROUP BY?",
        "answer": "NULLs form their own group in GROUP BY. All NULL values are grouped together. Example: SELECT ManagerID, COUNT(*) FROM Employees GROUP BY ManagerID; This includes a group for employees with NULL ManagerID. Use COALESCE to replace NULLs with a label: COALESCE(ManagerID, 'No Manager') as Manager."
    },
    {
        "id": 340,
        "question": "What is the purpose of the FILTER clause in PostgreSQL aggregates?",
        "answer": "FILTER allows conditional aggregation without CASE. Example: SELECT DepartmentID, COUNT(*) as Total, COUNT(*) FILTER (WHERE Salary > 70000) as HighEarners FROM Employees GROUP BY DepartmentID; More readable than CASE for multiple conditions, and can improve performance."
    },
    {
        "id": 341,
        "question": "How do you calculate weighted averages using aggregate functions?",
        "answer": "SELECT SUM(Score * Weight) / SUM(Weight) as WeightedAvg FROM Grades WHERE StudentID = 123; Weighted average multiplies each value by its weight, sums products, and divides by total weight. Can also be done per group: SELECT StudentID, SUM(Score * Weight) / SUM(Weight) FROM Grades GROUP BY StudentID;"
    },
    {
        "id": 342,
        "question": "Explain the concept of mode (most frequent value) with aggregates.",
        "answer": "SELECT Salary, COUNT(*) as freq FROM Employees GROUP BY Salary ORDER BY freq DESC LIMIT 1; This finds the most common salary. PostgreSQL has MODE() function: SELECT MODE() WITHIN GROUP (ORDER BY Salary) FROM Employees; Mode identifies the most frequently occurring value."
    },
    {
        "id": 343,
        "question": "What is the difference between MIN/MAX on dates vs numbers?",
        "answer": "MIN/MAX work the same way: MIN finds earliest/smallest, MAX finds latest/largest. For dates: MIN(OrderDate) finds earliest order, MAX(OrderDate) finds latest. For numbers: MIN(Salary) finds lowest salary, MAX(Salary) finds highest. The function behavior is consistent across data types."
    },
    {
        "id": 344,
        "question": "How do you create a pivot table using aggregate functions?",
        "answer": "SELECT DepartmentID, SUM(CASE WHEN Year = 2020 THEN Amount END) as Y2020, SUM(CASE WHEN Year = 2021 THEN Amount END) as Y2021, SUM(CASE WHEN Year = 2022 THEN Amount END) as Y2022 FROM Sales GROUP BY DepartmentID; This pivots year values into columns with aggregated amounts."
    },
    {
        "id": 345,
        "question": "Explain the use of aggregate functions in subqueries.",
        "answer": "SELECT EmployeeID, Salary FROM Employees WHERE Salary > (SELECT AVG(Salary) FROM Employees); Subquery calculates average, outer query finds employees above it. Also: SELECT d.DepartmentName, (SELECT AVG(Salary) FROM Employees e WHERE e.DepartmentID = d.DepartmentID) as AvgSalary FROM Departments d;"
    },
    {
        "id": 346,
        "question": "What is an index in a database and why is it important?",
        "answer": "An index is a data structure that improves the speed of data retrieval operations on a table. Like a book index, it provides quick access to rows without scanning the entire table. Indexes are crucial for performance but add overhead on INSERT, UPDATE, and DELETE operations because they must be maintained."
    },
    {
        "id": 347,
        "question": "Explain the difference between clustered and non-clustered indexes.",
        "answer": "Clustered index determines the physical order of data in a table. A table can have only one clustered index. Data rows are stored at the leaf level. Non-clustered index is a separate structure containing index key values and pointers to data rows. A table can have many non-clustered indexes. Clustered is faster for range queries."
    },
    {
        "id": 348,
        "question": "What is a B-tree index and how does it work?",
        "answer": "B-tree (balanced tree) is the most common index type. It maintains sorted data for efficient insertion, deletion, and search operations. The tree has root, branch, and leaf nodes. Searches traverse from root to leaf in O(log n) time. B-trees are self-balancing, maintaining performance even with many operations."
    },
    {
        "id": 349,
        "question": "Explain hash indexes and when to use them.",
        "answer": "Hash indexes use a hash function to map keys to bucket locations. They're extremely fast for exact-match equality queries (WHERE col = value) but useless for range queries (>, <, BETWEEN). Hash indexes work best when all queries are equality lookups and data is relatively static."
    },
    {
        "id": 350,
        "question": "What is a composite index and how does column order matter?",
        "answer": "A composite index includes multiple columns. Column order matters significantly - the index is sorted by first column, then second, etc. It supports queries using leftmost columns. For index on (LastName, FirstName), it helps queries on LastName alone or both, but not on FirstName alone. Choose order based on query patterns."
    },
    {
        "id": 351,
        "question": "What are the trade-offs of using indexes?",
        "answer": "Indexes speed up SELECT queries but slow down INSERT, UPDATE, DELETE operations because indexes must be maintained. They consume disk space and memory. Over-indexing can hurt performance. Each index is a trade-off between query performance and write overhead. Strategic indexing based on actual query patterns is essential."
    },
    {
        "id": 352,
        "question": "Explain bitmap indexes and their use cases.",
        "answer": "Bitmap indexes store index keys as bit arrays (bits per distinct value). Each bit represents a row. Excellent for columns with low cardinality (few distinct values) like gender or status. Efficient for complex Boolean operations (AND, OR) but poor for high-cardinality columns. Common in data warehousing."
    },
    {
        "id": 353,
        "question": "What is a covering index?",
        "answer": "A covering index contains all columns needed by a query, eliminating the need to access the table data. If index includes SELECT, WHERE, and JOIN columns, the database can satisfy the query entirely from the index. This dramatically improves performance. Example: Index on (DepartmentID, Salary, Name) covers query selecting these columns for a department."
    },
    {
        "id": 354,
        "question": "How do you decide which columns to index?",
        "answer": "Index columns used in: WHERE clauses, JOIN conditions, ORDER BY, GROUP BY. Index columns with high selectivity (many unique values). Consider query frequency and importance. Avoid indexing columns with few distinct values unless using bitmap indexes. Monitor slow queries and add indexes based on actual usage patterns."
    },
    {
        "id": 355,
        "question": "What is index fragmentation and how do you fix it?",
        "answer": "Index fragmentation occurs as data is inserted, updated, and deleted, causing pages to become disorganized with wasted space. Fragmented indexes degrade performance. Fix with REBUILD or REORGANIZE (SQL Server), REINDEX (PostgreSQL), OPTIMIZE TABLE (MySQL). Regular maintenance schedules prevent fragmentation buildup."
    },
    {
        "id": 356,
        "question": "Explain unique indexes and their purpose.",
        "answer": "Unique indexes enforce uniqueness on column values, preventing duplicates. They automatically created for PRIMARY KEY and UNIQUE constraints. Unique indexes also improve query performance like regular indexes. CREATE UNIQUE INDEX idx_email ON Users(Email); ensures no duplicate emails and speeds up email lookups."
    },
    {
        "id": 357,
        "question": "What is a full-text index and when would you use it?",
        "answer": "Full-text indexes enable complex text searches on character-based columns. They support word matching, phrase searching, and relevance ranking. Use for document search, article content, product descriptions. Unlike LIKE '%text%', full-text searches are efficient and support linguistic features like stemming and stop words."
    },
    {
        "id": 358,
        "question": "How do indexes affect INSERT performance?",
        "answer": "Each INSERT requires updating every index on the table. More indexes = slower inserts. For a table with N indexes, an INSERT becomes N times more work (approximately). On high-volume insert tables, minimize indexes or use strategies like dropping indexes before bulk loads and rebuilding afterward."
    },
    {
        "id": 359,
        "question": "Explain the concept of index selectivity.",
        "answer": "Selectivity measures how many rows match a typical value. High selectivity (many unique values) makes indexes more effective. Low selectivity (few unique values) means index might not help much. Example: Index on Email (high selectivity) is very useful; index on Gender (low selectivity) is less useful except with other columns."
    },
    {
        "id": 360,
        "question": "What is a filtered index in SQL Server?",
        "answer": "A filtered index indexes only rows meeting a condition, reducing size and maintenance overhead. Example: CREATE INDEX idx_active_customers ON Customers(Email) WHERE Status = 'Active'; Only active customers are indexed, saving space and making queries on active customers faster. Useful for partial data access patterns."
    },
    {
        "id": 361,
        "question": "How do you create an index on an expression or function?",
        "answer": "Function-based indexes index the result of an expression. PostgreSQL: CREATE INDEX idx_lower_email ON Users(LOWER(email)); MySQL: CREATE INDEX idx_lower_email ON Users((LOWER(email))); Oracle: CREATE INDEX idx_lower_email ON Users(LOWER(email)); Allows efficient searches on transformed values like WHERE LOWER(email) = 'user@example.com'."
    },
    {
        "id": 362,
        "question": "What is the difference between a primary key and a unique index?",
        "answer": "Primary key is a special constraint that combines unique index with NOT NULL and identifies each row uniquely. A table can have only one primary key. Unique index enforces uniqueness but allows one NULL (in most databases). Multiple unique indexes are allowed. Primary keys are often clustered by default."
    },
    {
        "id": 363,
        "question": "Explain index intersection and how databases use multiple indexes.",
        "answer": "Index intersection uses multiple indexes to satisfy a query, then combines results. For query WHERE Gender='M' AND Status='Active', database might use Gender index and Status index, then intersect the row IDs. Useful when no single index covers all conditions but can be less efficient than a composite index."
    },
    {
        "id": 364,
        "question": "How do NULL values behave in indexes?",
        "answer": "Behavior varies by database. In most, NULLs are included in indexes (except unique indexes where they're treated as distinct). B-tree indexes typically store NULLs together (all at beginning or end). Queries with IS NULL can benefit from indexes if NULLs are included. Some databases allow NULLS FIRST/LAST in index definition."
    },
    {
        "id": 365,
        "question": "What is a spatial index and when is it used?",
        "answer": "Spatial indexes optimize queries on geometric data types (points, lines, polygons). Used in GIS applications for location-based queries like . Common in PostgreSQL with PostGIS, MySQL with spatial extensions, SQL Server with geography/geometry types. Use R-tree or similar structures."
    },
    {
        "id": 366,
        "question": "Explain index-only scans and their benefits.",
        "answer": "Index-only scans occur when all required columns are in the index, avoiding table access. The database reads only the index, which is much smaller than the table. Benefits: dramatically faster queries, reduced I/O, less buffer pool usage. Example: Index on (DepartmentID, Salary) covers query SELECT DepartmentID, Salary FROM Employees WHERE DepartmentID = 10."
    },
    {
        "id": 367,
        "question": "How do you monitor index usage and find unused indexes?",
        "answer": "Databases provide views for index usage: pg_stat_user_indexes (PostgreSQL), sys.dm_db_index_usage_stats (SQL Server), performance_schema.table_io_waits_summary_by_index_usage (MySQL). Find indexes with few or no seeks/scans, consider dropping them to reduce maintenance overhead. Regular monitoring identifies optimization opportunities."
    },
    {
        "id": 368,
        "question": "What is the impact of indexing on ORDER BY queries?",
        "answer": "Indexes can eliminate sorting operations for ORDER BY if the index order matches the ORDER BY clause. For ORDER BY LastName, FirstName, an index on (LastName, FirstName) allows database to return rows in sorted order directly from index. This is especially valuable for large result sets."
    },
    {
        "id": 369,
        "question": "Explain partial indexes in PostgreSQL.",
        "answer": "Partial indexes index only rows satisfying a condition, similar to filtered indexes. Example: CREATE INDEX idx_recent_orders ON Orders(OrderDate) WHERE OrderDate > '2023-01-01'; Useful when queries typically target a subset of data. Smaller index size, less maintenance, faster for relevant queries."
    },
    {
        "id": 370,
        "question": "How do you create a descending index?",
        "answer": "CREATE INDEX idx_date_desc ON Orders(OrderDate DESC); Useful for queries sorting by column descending. Some databases allow mixed directions: CREATE INDEX idx_name ON Employees(LastName ASC, FirstName DESC). Helps queries with mixed sort directions without explicit sorting."
    },
    {
        "id": 371,
        "question": "What is the difference between a hash index and a B-tree index?",
        "answer": "Hash indexes use hash tables for O(1) lookups but only support equality (=) operations. B-tree indexes support equality, range (<, >, BETWEEN), and pattern matching (LIKE 'prefix%'). Hash indexes are smaller and faster for exact matches but useless for sorting or ranges. B-trees are more versatile."
    },
    {
        "id": 372,
        "question": "Explain the concept of index include columns (SQL Server).",
        "answer": "INCLUDE adds non-key columns to leaf level of index without affecting index sort order. CREATE INDEX idx_dept ON Employees(DepartmentID) INCLUDE (FirstName, LastName, Salary); The index is sorted by DepartmentID but includes other columns at leaf level, enabling covering queries without making index wider at upper levels."
    },
    {
        "id": 373,
        "question": "How do indexes affect JOIN performance?",
        "answer": "Indexes on join columns dramatically improve JOIN performance. For nested loop joins, the inner table needs an index on join column for quick lookups. For hash joins, indexes on build input can help. Without indexes, JOINs may require full table scans and temporary tables, causing poor performance."
    },
    {
        "id": 374,
        "question": "What is a redundant index and why avoid it?",
        "answer": "A redundant index is an index that's a prefix of another index. Example: Index A on (col1), Index B on (col1, col2). Index A is redundant because Index B can serve queries using just col1. Redundant indexes waste space and add maintenance overhead without performance benefit."
    },
    {
        "id": 375,
        "question": "How do you estimate index size before creating it?",
        "answer": "Index size depends on number of rows, key column sizes, and index type. Rough estimate: size ≈ rows × (key_size + pointer_size) + overhead. For clustered indexes, include data row size. Many databases provide tools: SQL Server sp_spaceused, PostgreSQL pg_total_relation_size, MySQL information_schema. Test on representative data."
    },
    {
        "id": 376,
        "question": "What are transaction isolation levels in SQL?",
        "answer": "Isolation levels define how transactions interact with each other, controlling phenomena like dirty reads, non-repeatable reads, and phantom reads. SQL defines four levels: READ UNCOMMITTED (lowest isolation), READ COMMITTED, REPEATABLE READ, and SERIALIZABLE (highest). Higher levels provide more consistency but reduce concurrency."
    },
    {
        "id": 377,
        "question": "Explain the READ UNCOMMITTED isolation level.",
        "answer": "READ UNCOMMITTED is the lowest isolation level. It allows dirty reads, meaning a transaction can see uncommitted changes from other transactions. It offers highest concurrency but lowest consistency. Useful for approximate data where absolute accuracy isn't critical, like rough counts or reports. In SQL Server, it's equivalent to NOLOCK hint."
    },
    {
        "id": 378,
        "question": "What is READ COMMITTED isolation level?",
        "answer": "READ COMMITTED prevents dirty reads by ensuring transactions only see committed data. It's the default in many databases (SQL Server, PostgreSQL, Oracle). However, it allows non-repeatable reads - same query in a transaction might get different results if another transaction commits changes. Balances consistency and concurrency."
    },
    {
        "id": 379,
        "question": "Explain REPEATABLE READ isolation level.",
        "answer": "REPEATABLE READ prevents dirty reads and non-repeatable reads. If a transaction reads a row twice, it sees the same data both times. However, it may still allow phantom reads - new rows inserted by other transactions can appear in subsequent queries. Prevents modification of existing data but not insertion of new data affecting result sets."
    },
    {
        "id": 380,
        "question": "What is SERIALIZABLE isolation level?",
        "answer": "SERIALIZABLE is the highest isolation level, preventing dirty reads, non-repeatable reads, and phantom reads. It ensures transactions execute as if they were serial (one after another), even though they're concurrent. Achieved through locking or multiversion concurrency control. Highest consistency but lowest concurrency and performance."
    },
    {
        "id": 381,
        "question": "What is a dirty read and which isolation levels prevent it?",
        "answer": "A dirty read occurs when a transaction reads data written by another uncommitted transaction. If the writing transaction rolls back, the reading transaction has seen invalid data. Dirty reads are prevented by READ COMMITTED and higher isolation levels. Only READ UNCOMMITTED allows dirty reads."
    },
    {
        "id": 382,
        "question": "Explain non-repeatable reads and how to prevent them.",
        "answer": "A non-repeatable read occurs when a transaction reads the same row twice and gets different values because another transaction modified and committed the row between reads. Prevented by REPEATABLE READ and SERIALIZABLE. Example: Transaction T1 reads salary=50000, T2 updates to 55000 and commits, T1 reads again and sees 55000."
    },
    {
        "id": 383,
        "question": "What are phantom reads and which isolation levels prevent them?",
        "answer": "Phantom reads occur when a transaction runs a query twice and sees new rows inserted by another transaction between executions. Unlike non-repeatable reads (existing rows changed), phantoms are new rows appearing. Prevented only by SERIALIZABLE. Example: T1 queries employees in department, T2 inserts new employee in same department, T1 query returns different count."
    },
    {
        "id": 384,
        "question": "How does MVCC (Multi-Version Concurrency Control) relate to isolation levels?",
        "answer": "MVCC implements isolation by keeping multiple versions of data rows. Readers see a snapshot of data as of a point in time, without blocking writers. PostgreSQL uses MVCC to implement READ COMMITTED and REPEATABLE READ. Oracle uses MVCC for its default isolation. MVCC provides consistency without many read locks, improving concurrency."
    },
    {
        "id": 385,
        "question": "What is snapshot isolation in SQL Server?",
        "answer": "Snapshot isolation uses row versioning to provide a transactionally consistent view of data as of the start of the transaction. It avoids read locks and prevents dirty reads, non-repeatable reads, and phantom reads. Similar to SERIALIZABLE but with less blocking. Enabled with ALLOW_SNAPSHOT_ISOLATION database option."
    },
    {
        "id": 386,
        "question": "Explain the difference between optimistic and pessimistic locking.",
        "answer": "Pessimistic locking assumes conflicts are likely and locks data when read, preventing other transactions from modifying it. Optimistic locking assumes conflicts are rare, detects them at commit time, and rolls back if conflicts occurred. Isolation levels relate to pessimistic approaches; optimistic concurrency is often application-managed."
    },
    {
        "id": 387,
        "question": "How do you set isolation level in SQL?",
        "answer": "SET TRANSACTION ISOLATION LEVEL SERIALIZABLE; Then BEGIN TRANSACTION; Or database-specific: In SQL Server, you can use table hints: SELECT * FROM Employees WITH (NOLOCK). In PostgreSQL: SET TRANSACTION ISOLATION LEVEL REPEATABLE READ; In MySQL: SET SESSION TRANSACTION ISOLATION LEVEL READ COMMITTED;"
    },
    {
        "id": 388,
        "question": "What is the default isolation level in different databases?",
        "answer": "SQL Server: READ COMMITTED (with locking or row versioning depending on database setting). PostgreSQL: READ COMMITTED. MySQL (InnoDB): REPEATABLE READ. Oracle: READ COMMITTED (implemented with MVCC). DB2: Cursor Stability (similar to READ COMMITTED). Understanding defaults helps predict transaction behavior."
    },
    {
        "id": 389,
        "question": "Explain lost update problem and how isolation prevents it.",
        "answer": "Lost update occurs when two transactions read the same data, both modify it, and the second write overwrites the first without incorporating its changes. Example: T1 and T2 both read inventory=10, T1 sets to 9, T2 sets to 8 (losing T1's update). Prevented by higher isolation levels or explicit locking (SELECT FOR UPDATE)."
    },
    {
        "id": 390,
        "question": "What is the difference between READ COMMITTED and REPEATABLE READ in PostgreSQL?",
        "answer": "PostgreSQL implements both using MVCC. In READ COMMITTED, each query sees a snapshot of data committed before the query started. Different queries in same transaction may see different snapshots. In REPEATABLE READ, the entire transaction sees a single snapshot from transaction start, preventing non-repeatable reads."
    },
    {
        "id": 391,
        "question": "How does isolation level affect deadlocks?",
        "answer": "Higher isolation levels (REPEATABLE READ, SERIALIZABLE) typically use more locks, increasing deadlock probability. READ UNCOMMITTED uses fewest locks, minimizing deadlocks but risking data consistency. READ COMMITTED balances both. Choosing appropriate level reduces unnecessary locking while maintaining required consistency."
    },
    {
        "id": 392,
        "question": "What is serializable snapshot isolation in SQL Server?",
        "answer": "SERIALIZABLE isolation can be implemented with optimistic concurrency using snapshot isolation. It provides true serializability by detecting conflicts that would break serializability, even those not caught by snapshot isolation. It combines performance benefits of row versioning with correctness guarantees of SERIALIZABLE."
    },
    {
        "id": 393,
        "question": "Explain the concept of predicate locking in SERIALIZABLE isolation.",
        "answer": "Predicate locking locks the logical conditions of a query, not just specific rows. If a query has WHERE department = 'Sales', SERIALIZABLE may lock the concept of 'Sales department' to prevent phantoms - new Sales rows can't be inserted. Implemented via index locking or range locks in different databases."
    },
    {
        "id": 394,
        "question": "How do you choose the right isolation level for an application?",
        "answer": "Consider: consistency requirements, concurrency needs, performance targets, and business rules. Financial transactions often need SERIALIZABLE. Reporting can use READ UNCOMMITTED. Web applications often use READ COMMITTED for balance. Test with realistic workloads. Higher isolation isn't always better - it reduces concurrency and can cause blocking."
    },
    {
        "id": 395,
        "question": "What is the difference between READ UNCOMMITTED and READ COMMITTED?",
        "answer": "READ UNCOMMITTED allows dirty reads (seeing uncommitted data), uses no read locks, highest concurrency but lowest consistency. READ COMMITTED prevents dirty reads, ensures only committed data is seen, uses shared locks or row versioning, balances consistency and concurrency. Most applications prefer READ COMMITTED over READ UNCOMMITTED."
    },
    {
        "id": 396,
        "question": "Explain how Oracle implements READ COMMITTED isolation.",
        "answer": "Oracle uses MVCC with statement-level read consistency. Each query sees a snapshot of data as of the moment the query begins (not transaction start). Readers don't block writers, writers don't block readers. Undo data maintains previous versions. This provides consistent, non-blocking reads without locking."
    },
    {
        "id": 397,
        "question": "What is the NOLOCK hint in SQL Server and its risks?",
        "answer": "NOLOCK hint applies READ UNCOMMITTED to a specific table, allowing dirty reads. It improves concurrency by avoiding shared locks but risks: reading uncommitted data that might roll back, missing rows, seeing rows twice, or data movement during scan. Use cautiously, typically for approximate reporting on low-contention data."
    },
    {
        "id": 398,
        "question": "How does REPEATABLE READ prevent non-repeatable reads?",
        "answer": "REPEATABLE READ holds shared locks on all rows read until transaction ends, preventing other transactions from modifying them. Or in MVCC databases, it uses a transaction snapshot that remains consistent. This ensures the same row returns same values if read multiple times within transaction, regardless of other committed changes."
    },
    {
        "id": 399,
        "question": "What are write skew anomalies and which isolation level prevents them?",
        "answer": "Write skew occurs when two transactions read overlapping data, make disjoint updates based on what they read, but the combination violates a constraint. Example: Two doctors both on-call, both update themselves off-call thinking the other remains, leaving no doctor on-call. Only SERIALIZABLE prevents write skew in most databases."
    },
    {
        "id": 400,
        "question": "Explain the difference between statement-level and transaction-level consistency.",
        "answer": "Statement-level consistency ensures each statement sees a consistent snapshot (when statement starts). READ COMMITTED in PostgreSQL/Oracle. Transaction-level consistency ensures entire transaction sees consistent snapshot (when transaction starts). REPEATABLE READ/SERIALIZABLE in PostgreSQL, SERIALIZABLE in Oracle. Affects non-repeatable reads and phantoms."
    },
    {
        "id": 401,
        "question": "How does isolation level affect backup consistency?",
        "answer": "Database backups need consistent views. Higher isolation levels help create transactionally consistent backups. SERIALIZABLE backup ensures all data reflects a single point. READ COMMITTED backup might have inconsistencies across tables. Backup tools often use specific isolation levels or snapshot technologies for consistency."
    },
    {
        "id": 402,
        "question": "What is the difference between ANSI isolation levels and practical implementations?",
        "answer": "ANSI defines four levels based on prohibited phenomena. Databases implement them differently: PostgreSQL uses MVCC, Oracle uses undo segments, SQL Server uses locking or row versioning. Some implement additional levels (snapshot isolation). Phenomena prevented may differ - some REPEATABLE READ implementations prevent phantoms, some don't."
    },
    {
        "id": 403,
        "question": "Explain how to test isolation level behavior in your database.",
        "answer": "Open two database sessions. Session 1: BEGIN TRANSACTION, UPDATE a row (don't commit). Session 2: Try to read that row with different isolation levels (SET TRANSACTION ISOLATION LEVEL ...; SELECT ...). Observe if you see the uncommitted change. Test with INSERT to check phantom reads. Document behavior for your specific database."
    },
    {
        "id": 404,
        "question": "What is the relationship between isolation levels and ACID?",
        "answer": "Isolation is the 'I' in ACID. Higher isolation levels provide stronger isolation, making transactions behave more like serial executions. Lower levels relax isolation for performance, potentially violating ACID if not carefully managed. Choosing isolation level involves trading off between consistency (ACID compliance) and concurrency."
    },
    {
        "id": 405,
        "question": "How do distributed transactions affect isolation levels?",
        "answer": "Distributed transactions across multiple databases complicate isolation. Global serializability is expensive. Two-phase commit helps atomicity but isolation across resources is harder. Applications may need weaker isolation or compensate for anomalies. Some systems use saga patterns instead of distributed transactions for better scalability."
    },
    {
        "id": 406,
        "question": "What are database locks and why are they needed?",
        "answer": "Database locks are mechanisms to control concurrent access to data, ensuring transaction isolation and data consistency. They prevent problems like lost updates, dirty reads, and inconsistent analysis. Locks can be on rows, pages, tables, or databases. The lock manager tracks which transactions hold which locks on which resources."
    },
    {
        "id": 407,
        "question": "Explain shared locks and exclusive locks.",
        "answer": "Shared locks (S locks) allow multiple transactions to read a resource but prevent modifications. Multiple shared locks can coexist. Exclusive locks (X locks) allow a transaction to modify a resource and prevent any other transaction from reading or modifying it. Only one exclusive lock can exist on a resource at a time."
    },
    {
        "id": 408,
        "question": "What is two-phase locking (2PL) protocol?",
        "answer": "2PL ensures serializability by dividing transaction into two phases: growing phase (acquiring locks) and shrinking phase (releasing locks). Once a transaction releases a lock, it cannot acquire new ones. Strict 2PL holds all locks until commit/abort, preventing cascading aborts. Rigorous 2PL holds all locks until transaction end."
    },
    {
        "id": 409,
        "question": "Explain intention locks and their purpose.",
        "answer": "Intention locks indicate a transaction intends to acquire locks at a finer granularity. Types: IS (intention shared), IX (intention exclusive), SIX (shared + intention exclusive). They sit at higher levels (table) while finer locks are at row level. This allows efficient lock checking without scanning all rows."
    },
    {
        "id": 410,
        "question": "What is lock escalation and when does it occur?",
        "answer": "Lock escalation converts many fine-grained locks (rows) into fewer coarse-grained locks (table) to reduce overhead. Occurs when a transaction acquires many row locks, exceeding a threshold. While reducing lock manager overhead, it reduces concurrency because whole table is locked. Can be disabled or threshold adjusted in some databases."
    },
    {
        "id": 411,
        "question": "How do you detect and resolve blocking in SQL Server?",
        "answer": "Use dynamic management views: sys.dm_exec_requests, sys.dm_tran_locks, sys.dm_os_waiting_tasks. Identify blocking chain with sp_who2 or Activity Monitor. Resolve by optimizing queries, adding indexes, shortening transactions, or killing blocking process (last resort). Consider snapshot isolation to reduce blocking."
    },
    {
        "id": 412,
        "question": "What is update lock and when is it used?",
        "answer": "Update lock (U lock) is a hybrid lock used during the initial read phase of an update operation. It's compatible with shared locks but not other update locks. Prevents deadlocks where two transactions might hold shared locks and both try to upgrade to exclusive. Transaction acquires U lock, then escalates to X lock when ready to modify."
    },
    {
        "id": 413,
        "question": "Explain lock compatibility matrix.",
        "answer": "Lock compatibility determines which locks can coexist: Shared (S) compatible with S and IS; Exclusive (X) incompatible with all; Update (U) compatible with S, incompatible with U and X; Intention Shared (IS) compatible with all except X; Intention Exclusive (IX) compatible with IS, IX, incompatible with S, U, X; SIX compatible only with IS."
    },
    {
        "id": 414,
        "question": "What is the difference between optimistic and pessimistic locking?",
        "answer": "Pessimistic locking assumes conflicts are likely and locks data when read (SELECT FOR UPDATE). Prevents conflicts but reduces concurrency. Optimistic locking assumes conflicts are rare, checks at update time using version numbers or timestamps. If data changed, transaction rolls back and retries. Better for read-heavy, low-contention workloads."
    },
    {
        "id": 415,
        "question": "How does MVCC reduce locking needs?",
        "answer": "MVCC (Multi-Version Concurrency Control) maintains multiple versions of data rows. Readers see old versions without blocking writers, and writers don't block readers. This eliminates need for many read locks. Used in PostgreSQL, Oracle, MySQL (InnoDB). MVCC dramatically improves concurrency in mixed read-write workloads."
    },
    {
        "id": 416,
        "question": "What is SELECT FOR UPDATE and when would you use it?",
        "answer": "SELECT FOR UPDATE acquires exclusive locks on selected rows, preventing other transactions from modifying them until your transaction ends. Used when you need to read data, then update it based on what you read, ensuring no other transaction changes it in between. Example: reading account balance before withdrawal."
    },
    {
        "id": 417,
        "question": "Explain lock timeouts and how to set them.",
        "answer": "Lock timeout specifies how long a transaction waits for a lock before giving up. Prevents indefinite waiting. In SQL Server: SET LOCK_TIMEOUT 5000 (milliseconds); In PostgreSQL: SET statement_timeout = '5s'; In MySQL: innodb_lock_wait_timeout variable. When timeout occurs, transaction typically rolls back."
    },
    {
        "id": 418,
        "question": "What is a schema stability lock?",
        "answer": "Schema stability lock (Sch-S) prevents modifications to table schema while queries are running. Allows concurrent queries but blocks DDL operations. SQL Server uses it. When you run SELECT, Sch-S lock acquired. ALTER TABLE needs Sch-M (schema modification) lock, which waits for Sch-S locks to release, ensuring schema changes don't corrupt running queries."
    },
    {
        "id": 419,
        "question": "How do you monitor locks in PostgreSQL?",
        "answer": "Use pg_locks system view: SELECT * FROM pg_locks; Also pg_blocking_pids() to find blockers. pg_stat_activity shows current queries. For real-time monitoring, pg_activity tool. Log slow queries and check lock waits. Enable log_lock_waits parameter to record long waits. Extensions like pg_stat_statements help identify problematic queries."
    },
    {
        "id": 420,
        "question": "What is a deadlock and how are they resolved?",
        "answer": "A deadlock occurs when two or more transactions hold locks the other needs, creating a circular wait. Databases detect deadlocks using wait-for graphs. When detected, a deadlock victim is chosen (usually transaction with least work) and rolled back, releasing its locks so others can proceed. Applications should retry deadlock victims."
    },
    {
        "id": 421,
        "question": "Explain lock granularity (row, page, table).",
        "answer": "Lock granularity determines the size of locked resource. Row locks: finest granularity, highest concurrency, most overhead. Page locks: lock entire data page (multiple rows), balance concurrency and overhead. Table locks: coarsest granularity, lowest concurrency, least overhead. Databases may escalate from fine to coarse granularity automatically."
    },
    {
        "id": 422,
        "question": "What is key-range locking and how does it prevent phantoms?",
        "answer": "Key-range locking locks ranges of index keys, preventing phantom inserts. Used in SERIALIZABLE isolation. For query with WHERE department = 'Sales', range lock on 'Sales' index entries prevents new Sales rows from being inserted. Implemented in SQL Server, MySQL (InnoDB) with next-key locking combining record lock and gap lock."
    },
    {
        "id": 423,
        "question": "How do you avoid deadlocks in application design?",
        "answer": "Access resources in consistent order across transactions (always update accounts in AccountID order). Keep transactions short. Use lower isolation levels when appropriate. Consider optimistic concurrency. Ensure proper indexing to reduce lock ranges. Use bound connections. Implement deadlock retry logic. Monitor and analyze deadlock graphs."
    },
    {
        "id": 424,
        "question": "What is a spinlock in database internals?",
        "answer": "Spinlocks are lightweight synchronization primitives used internally by database engines for very short critical sections. Instead of yielding CPU, threads spin (loop) waiting for lock, avoiding expensive context switches. Used for protecting in-memory structures like buffer pool lists. Effective only for very short waits on multi-CPU systems."
    },
    {
        "id": 425,
        "question": "Explain latch vs lock in database terminology.",
        "answer": "Latches are lightweight, short-duration synchronization mechanisms protecting in-memory structures (like buffer pool pages). They don't participate in transaction semantics and aren't tracked for deadlock detection. Locks are heavier, transaction-aware, protect logical data (rows, tables), participate in deadlock detection, and are held for transaction duration."
    },
    {
        "id": 426,
        "question": "What is the NOWAIT option with locks?",
        "answer": "NOWAIT specifies that a transaction should not wait for locks - if the resource is already locked, the statement immediately fails with an error. Example: SELECT * FROM Employees WITH (NOWAIT) (SQL Server); SELECT * FROM Employees FOR UPDATE NOWAIT (Oracle/PostgreSQL). Useful when you can't afford to wait and want immediate feedback."
    },
    {
        "id": 427,
        "question": "How does lock partitioning improve performance?",
        "answer": "Lock partitioning splits lock manager structures across multiple CPU cores to reduce contention. Each partition handles locks for a subset of resources. Used in SQL Server and other high-end databases. Prevents lock manager from becoming a scalability bottleneck on large multi-core systems with high concurrency."
    },
    {
        "id": 428,
        "question": "What is a transaction isolation level's relationship to locking?",
        "answer": "Higher isolation levels use more restrictive locking: READ UNCOMMITTED uses no read locks; READ COMMITTED uses short-duration read locks or row versioning; REPEATABLE READ holds read locks until transaction end; SERIALIZABLE adds range locks or predicate locking. Locking strategy chosen by database based on isolation level setting."
    },
    {
        "id": 429,
        "question": "Explain bulk update locks in SQL Server.",
        "answer": "Bulk update locks (BU) are used during bulk import operations (BCP, BULK INSERT). They allow multiple bulk operations to load data concurrently while preventing other processes from accessing the table. Less restrictive than exclusive locks but more restrictive than shared locks. Optimizes bulk load performance while maintaining some concurrency."
    },
    {
        "id": 430,
        "question": "What is lock starvation and how is it prevented?",
        "answer": "Lock starvation occurs when a transaction repeatedly can't acquire needed locks because higher-priority transactions keep getting them first. Databases prevent starvation using fair lock queues (FIFO), lock timeouts, or promotion mechanisms. Some use adaptive locking where lock manager detects starvation and temporarily boosts waiting transaction's priority."
    },
    {
        "id": 431,
        "question": "How do you interpret a deadlock graph?",
        "answer": "Deadlock graph shows transactions involved, resources they hold, and resources they're waiting for. Each node represents transaction with process ID, isolation level, and SQL statements. Edges show wait relationships. Victim node is marked. Analyze to see which locks caused deadlock, then redesign code to access objects in consistent order or reduce lock duration."
    },
    {
        "id": 432,
        "question": "What is predicate locking in PostgreSQL?",
        "answer": "PostgreSQL uses predicate locks for SERIALIZABLE isolation to detect serialization anomalies. It locks logical predicates (like WHERE conditions), not just physical rows. Implemented using SIREAD locks that track which tuples were read. Detects conflicts that could break serializability, even without phantoms. Enables true serializability without extensive physical locking."
    },
    {
        "id": 433,
        "question": "Explain lock hints in SQL Server.",
        "answer": "Lock hints override default locking behavior per table: NOLOCK (read uncommitted), HOLDLOCK (hold shared locks until end), UPDLOCK (use update locks), XLOCK (exclusive locks), TABLOCK (table-level lock), PAGLOCK (page-level lock), ROWLOCK (row-level lock). Example: SELECT * FROM Employees WITH (TABLOCK, HOLDLOCK). Use carefully as they can cause concurrency issues."
    },
    {
        "id": 434,
        "question": "How do you configure lock settings in MySQL InnoDB?",
        "answer": "InnoDB variables: innodb_lock_wait_timeout (seconds to wait before rollback), innodb_deadlock_detect (enable/disable deadlock detection), innodb_autoinc_lock_mode (how AUTO_INCREMENT locks). Monitor with SHOW ENGINE INNODB STATUS. Use information_schema.INNODB_TRX and PERFORMANCE_SCHEMA tables for lock analysis."
    },
    {
        "id": 435,
        "question": "What is the difference between lock and semaphore in databases?",
        "answer": "Locks are database-level synchronization for transactional data access, with deadlock detection, compatibility matrices, and transaction semantics. Semaphores are OS-level synchronization for internal resources like memory, with simpler counting semantics. Locks protect logical data; semaphores protect physical resources like buffer pool slots or I/O queues."
    },
    {
        "id": 436,
        "question": "What is a distributed database system?",
        "answer": "A distributed database is a collection of multiple interconnected databases stored on different computers, appearing as a single logical database to users. Data can be fragmented, replicated, and distributed across sites. The distributed DBMS manages transparency (location, fragmentation, replication), distributed query processing, and distributed transaction management."
    },
    {
        "id": 437,
        "question": "Explain the CAP theorem in distributed databases.",
        "answer": "CAP theorem states that a distributed system can only guarantee two of three properties: Consistency (all nodes see same data simultaneously), Availability (every request receives response, even if some nodes fail), and Partition tolerance (system continues operating despite network failures). Most distributed databases choose AP (availability + partition tolerance) or CP (consistency + partition tolerance)."
    },
    {
        "id": 438,
        "question": "What is two-phase commit (2PC) protocol?",
        "answer": "2PC ensures atomic commitment of distributed transactions across multiple nodes. Phase 1 (Prepare): coordinator asks all participants to prepare (vote yes/no). Phase 2 (Commit/Abort): if all vote yes, coordinator tells all to commit; otherwise, tells all to abort. Ensures all nodes agree on outcome but has blocking issues if coordinator fails."
    },
    {
        "id": 439,
        "question": "Explain data fragmentation in distributed databases.",
        "answer": "Fragmentation splits data across sites: Horizontal fragmentation (rows) - different rows at different sites (e.g., East region customers in NY, West in LA). Vertical fragmentation (columns) - different columns at different sites (e.g., personal info in HR, salary in Finance). Mixed fragmentation combines both. Fragmentation aims to store data near where it's most used."
    },
    {
        "id": 440,
        "question": "What is data replication and what are its benefits?",
        "answer": "Replication copies data to multiple sites. Benefits: improved availability (data accessible if some sites down), faster read access (local copies), load distribution, and fault tolerance. Types: synchronous (all copies updated simultaneously, strong consistency) vs asynchronous (eventual consistency, better performance). Replication strategies include master-slave and multi-master."
    },
    {
        "id": 441,
        "question": "Explain the difference between homogeneous and heterogeneous distributed databases.",
        "answer": "Homogeneous: all sites use same DBMS software (e.g., all Oracle). Easier to manage, same query language, uniform protocols. Heterogeneous: sites may run different DBMS (Oracle, MySQL, MongoDB). Requires gateways for translation, more complex query processing, and may not support all transactions. Common in real-world when different departments chose different systems."
    },
    {
        "id": 442,
        "question": "What is the difference between synchronous and asynchronous replication?",
        "answer": "Synchronous replication updates all copies before transaction commits, ensuring strong consistency but higher latency and availability risk (if any replica down, transaction fails). Asynchronous replication commits transaction on primary, then propagates changes to replicas later. Better performance and availability but risk of reading stale data. Trade-off between consistency and performance."
    },
    {
        "id": 443,
        "question": "Explain distributed query processing challenges.",
        "answer": "Challenges include: data localization (finding where data resides), fragmentation transparency, query optimization across sites (considering network costs vs local processing), joining tables from different sites (data must be transferred), and handling heterogeneous schemas. Optimizers choose execution strategies minimizing data transfer, often using semi-joins to reduce communication."
    },
    {
        "id": 444,
        "question": "What is the difference between replication and duplication?",
        "answer": "Replication is intentional copying of data across sites for performance/availability, managed by DBMS with synchronization protocols. Duplication is accidental, uncontrolled redundancy causing consistency problems. Replication is planned with consistency guarantees; duplication is waste. In distributed DB design, replication is controlled; in poor design, duplication occurs unintentionally."
    },
    {
        "id": 445,
        "question": "Explain the concept of location transparency.",
        "answer": "Location transparency means users don't need to know where data is physically stored. Queries reference logical names (e.g., Customers table) without specifying site (NY server). The distributed DBMS maps logical names to physical locations, routes queries appropriately, and combines results. Essential for distributed database usability."
    },
    {
        "id": 446,
        "question": "What is the difference between distributed database and parallel database?",
        "answer": "Distributed databases: geographically separated, autonomous sites, network-connected, often heterogeneous, designed for local autonomy and sharing. Parallel databases: tightly coupled processors in same location, shared resources, homogeneous, designed for high performance on large queries through parallel execution. Different goals: distribution for autonomy, parallelism for speed."
    },
    {
        "id": 447,
        "question": "Explain the two-phase commit (2PC) failure scenarios.",
        "answer": "2PC failure scenarios: Coordinator failure before sending prepare - all abort. Participant failure during prepare - coordinator aborts after timeout. Coordinator failure after all prepare but before commit - participants blocked (in doubt). Coordinator recovery must query participants. Three-phase commit (3PC) addresses blocking but more complex. In practice, 2PC with logging and recovery is common."
    },
    {
        "id": 448,
        "question": "What is eventual consistency in distributed databases?",
        "answer": "Eventual consistency means that if no new updates are made, all replicas will eventually become consistent. Reads may see stale data temporarily, but convergence is guaranteed. Used in many NoSQL databases (Cassandra, DynamoDB) and asynchronous replication. Provides high availability and partition tolerance at cost of temporary inconsistency. BASE (Basically Available, Soft state, Eventual consistency) vs ACID."
    },
    {
        "id": 449,
        "question": "Explain the concept of distributed deadlock detection.",
        "answer": "Distributed deadlock detection is complex because no site has complete information. Approaches: centralized (one site collects all lock info), hierarchical (regional detectors), or distributed (sites exchange wait-for graphs). Phantom deadlocks possible due to communication delays. Algorithms like edge chasing (probes circulate wait-for graphs) detect cycles without central coordinator."
    },
    {
        "id": 450,
        "question": "What is the difference between shared-nothing and shared-disk architecture?",
        "answer": "Shared-nothing: each node has own CPU, memory, disk. Data partitioned across nodes. Scales well, high availability, but repartitioning complex. Used in many distributed databases. Shared-disk: all nodes share same disks via SAN. Easier management, but cache coherency overhead, single point of failure at disk level. Oracle RAC uses shared-disk. Shared-nothing more common in modern distributed systems."
    },
    {
        "id": 451,
        "question": "Explain the concept of fragment independence.",
        "answer": "Fragment independence means applications can operate on logical tables without knowing how they're fragmented. The DDBMS maps logical operations to appropriate fragments, combines results, and handles any necessary transformations. Changes to fragmentation (re-fragmentation) don't affect applications. This is a key transparency feature."
    },
    {
        "id": 452,
        "question": "What is the difference between vertical and horizontal fragmentation?",
        "answer": "Vertical fragmentation splits table by columns, with each fragment containing subset of columns plus primary key. Use for distributing columns used by different applications. Horizontal fragmentation splits by rows, typically by value ranges or hash. Use for distributing data geographically or by customer segment. Mixed fragmentation combines both."
    },
    {
        "id": 453,
        "question": "Explain the PACELC theorem extension of CAP.",
        "answer": "PACELC extends CAP: if Partition (P) occurs, trade-off between Availability (A) and Consistency (C); Else (no partition), trade-off between Latency (L) and Consistency (C). Recognizes that even without network partitions, systems face trade-offs: replicating for low latency may sacrifice consistency. Provides more complete framework for distributed system design."
    },
    {
        "id": 454,
        "question": "What is the difference between master-slave and multi-master replication?",
        "answer": "Master-slave: one primary node accepts writes, propagates to read-only replicas. Simple, avoids conflicts, but write bottleneck. Multi-master: multiple nodes accept writes, changes propagate between them. Higher write availability but conflict resolution needed (last-write-wins, application merge). Used in active-active configurations for high availability."
    },
    {
        "id": 455,
        "question": "Explain the concept of distributed transaction coordinator.",
        "answer": "Transaction coordinator (TC) manages distributed transactions across sites. Responsibilities: generating global transaction IDs, coordinating 2PC, logging decisions for recovery, communicating with local transaction managers at each site. TC ensures atomic commitment even with failures. Can be centralized (single TC) or distributed (multiple TCs cooperating)."
    },
    {
        "id": 456,
        "question": "What is the difference between strong consistency and weak consistency?",
        "answer": "Strong consistency (linearizability) ensures all nodes see same data order, reads see latest write. Required for ACID, achieved via synchronous replication/quorum. Weak consistency allows temporary inconsistencies for performance. Includes eventual consistency, causal consistency, read-your-writes. Trade-off between consistency guarantees and system performance/availability."
    },
    {
        "id": 457,
        "question": "Explain the concept of data shipping vs query shipping.",
        "answer": "Data shipping moves data to where query executes (traditional approach). Query shipping moves query to where data resides (pushdown). In distributed databases, optimizing queries often involves deciding which operations to ship (projections, selections) vs moving data. Modern systems use hybrid: push filtering/aggregation to data nodes, move only results."
    },
    {
        "id": 458,
        "question": "What is the difference between global deadlock and local deadlock?",
        "answer": "Local deadlock involves resources on single node, detected by local lock manager. Global deadlock involves resources across multiple nodes, requiring distributed detection. Example: T1 on Node A holds lock on resource R1 (Node A), waits for R2 (Node B); T2 on Node B holds R2, waits for R1. No single node sees full cycle."
    },
    {
        "id": 459,
        "question": "Explain the concept of timestamp ordering in distributed databases.",
        "answer": "Timestamp ordering assigns unique timestamps to transactions globally (using site ID + local timestamp). Schedules transactions based on timestamps to ensure serializability. Each data item tracks read and write timestamps. Transaction aborts if it attempts operation out of order. Provides deadlock-free concurrency control without locking."
    },
    {
        "id": 460,
        "question": "What is the difference between distributed database and federated database?",
        "answer": "Distributed database is a single logical database physically distributed, with tight integration and centralized control. Federated database integrates multiple autonomous databases with less central control. Federated systems preserve local autonomy, may have schema mapping, and provide looser coupling. Federated query processor translates between local and global schemas."
    },
    {
        "id": 461,
        "question": "Explain the concept of partial failure in distributed systems.",
        "answer": "Partial failure means some nodes fail while others continue working. Distributed databases must handle this gracefully: transactions on failed nodes may block or abort, but system continues servicing other requests. Requires failure detection, recovery protocols, and replica management. Contrast with centralized systems where failure stops everything."
    },
    {
        "id": 462,
        "question": "What is the difference between distributed query optimization and centralized optimization?",
        "answer": "Centralized optimization considers CPU, I/O costs. Distributed adds network communication costs, which often dominate. Distributed optimizers consider: data transfer strategies (whole table vs semi-join), site selection for operations, parallelism opportunities, and fragment pruning. May use two-phase optimization: global optimization across sites, then local optimization at each site."
    },
    {
        "id": 463,
        "question": "Explain the concept of network partition in distributed databases.",
        "answer": "Network partition occurs when communication fails between nodes, splitting system into subgroups. During partition, CAP systems must choose: continue operating (availability) risking inconsistency, or stop some operations (consistency). After partition heals, systems must reconcile divergent data. Handling partitions is fundamental distributed system challenge."
    },
    {
        "id": 464,
        "question": "What is the difference between gossip protocol and consensus protocol?",
        "answer": "Gossip protocols propagate information epidemically: nodes periodically exchange data with random peers, eventually all nodes learn. Used for failure detection, membership management. Consensus protocols (Paxos, Raft) get nodes to agree on a value despite failures, used for leader election, atomic commits. Gossip is eventual, consensus is deterministic but more expensive."
    },
    {
        "id": 465,
        "question": "Explain the concept of vector clocks in distributed systems.",
        "answer": "Vector clocks capture causality in distributed systems. Each node maintains vector of counters (one per node). When node does internal event, increments own counter. When sending message, includes vector. Receiver updates its vector as max(own, received) +1. Detects concurrent updates: if vectors are incomparable (neither ≤ other), updates are concurrent, may need conflict resolution."
    },
    {
        "id": 466,
        "question": "What is NoSQL and why did it emerge?",
        "answer": "NoSQL (Not Only SQL) databases emerged to handle limitations of relational databases for modern applications: massive scale (web-scale data), flexible schemas (semi-structured data), high velocity (real-time updates), and horizontal scaling. They sacrifice ACID transactions and strong consistency for scalability, availability, and performance. Types: document, key-value, column-family, graph."
    },
    {
        "id": 467,
        "question": "Explain the different types of NoSQL databases.",
        "answer": "Four main types: 1) Document stores (MongoDB, Couchbase) - store JSON/BSON documents, schema-flexible. 2) Key-value stores (Redis, DynamoDB) - simple key-value pairs, extremely fast. 3) Column-family stores (Cassandra, HBase) - store data in columns families, optimized for wide-table analytics. 4) Graph databases (Neo4j, Amazon Neptune) - store nodes and edges for connected data."
    },
    {
        "id": 468,
        "question": "What is MongoDB and what are its key features?",
        "answer": "MongoDB is a document-oriented NoSQL database storing data in BSON format (binary JSON). Key features: schema flexibility (documents can have different fields), horizontal scaling via sharding, rich query language, secondary indexes, aggregation pipeline, replication with automatic failover, and geospatial queries. Good for content management, real-time analytics, catalogs."
    },
    {
        "id": 469,
        "question": "Explain Cassandra's data model and architecture.",
        "answer": "Cassandra is a column-family store with distributed, masterless architecture. Data model: keyspace (database) → tables → rows with columns. Partition key determines data distribution, clustering columns sort within partition. All nodes equal (no master), data partitioned and replicated across cluster. Tunable consistency (any to all). Optimized for write-heavy workloads, time-series data."
    },
    {
        "id": 470,
        "question": "What is Redis and what is it commonly used for?",
        "answer": "Redis is an in-memory key-value store with optional persistence. Supports rich data structures: strings, hashes, lists, sets, sorted sets, bitmaps, streams. Used for caching, session storage, real-time analytics, message brokering (Pub/Sub), rate limiting, and leaderboards. Extremely low latency (<ms). Can persist to disk but primarily in-memory for performance."
    },
    {
        "id": 471,
        "question": "Explain the concept of document store with an example.",
        "answer": "Document stores store semi-structured data as documents (JSON, XML, BSON). Each document is self-describing with its own schema.  Unlike relational tables, fields can vary per document, arrays nest naturally, relationships embedded or referenced."
    },
    {
        "id": 472,
        "question": "What is the difference between SQL and NoSQL databases?",
        "answer": "SQL: relational model, fixed schemas, ACID transactions, strong consistency, vertical scaling, SQL language. NoSQL: various models (document, key-value), flexible schemas, BASE (Basically Available, Soft state, Eventual consistency), horizontal scaling, query APIs vary. SQL for complex queries, transactions, structured data. NoSQL for scalability, semi-structured data, high velocity."
    },
    {
        "id": 473,
        "question": "Explain the concept of BASE in NoSQL databases.",
        "answer": "BASE (Basically Available, Soft state, Eventual consistency) is the NoSQL alternative to ACID. Basically Available: system guarantees availability. Soft state: state may change over time without input (due to eventual consistency). Eventual consistency: system will become consistent over time if no new updates. Sacrifices immediate consistency for availability and partition tolerance."
    },
    {
        "id": 474,
        "question": "What is Neo4j and what is it used for?",
        "answer": "Neo4j is a graph database storing data as nodes (entities), relationships (connections), and properties (attributes). Uses Cypher query language for pattern matching. Optimized for connected data queries like social networks (friends of friends), recommendation engines, fraud detection (finding suspicious patterns), and network analysis. Traverses relationships in real-time regardless of depth."
    },
    {
        "id": 475,
        "question": "Explain sharding in MongoDB.",
        "answer": "Sharding distributes data across multiple machines horizontally. MongoDB uses shard key to partition data into chunks distributed across shards. Config servers store metadata, mongos routers direct queries. Enables horizontal scaling beyond single node limits. Choose shard key carefully to avoid hotspots and ensure even distribution. Rebalancing occurs automatically as data grows."
    },
    {
        "id": 476,
        "question": "What is the difference between replication and sharding?",
        "answer": "Replication copies same data to multiple nodes for redundancy and read scaling (master-slave, multi-master). Sharding partitions different data across nodes for write scaling and large datasets (each node holds subset). Replication provides high availability; sharding provides scalability beyond single node. Often combined: each shard is a replica set for fault tolerance."
    },
    {
        "id": 477,
        "question": "Explain eventual consistency in Cassandra.",
        "answer": "Cassandra offers tunable consistency. With eventual consistency, writes return after updating some replicas (e.g., 1 of 3). Reads may see stale data until all replicas updated. Repairs happen via read repairs and anti-entropy (Merkle trees). Choose consistency level per operation: ONE, QUORUM, ALL. Eventual consistency provides lowest latency but application must handle stale reads."
    },
    {
        "id": 478,
        "question": "What is the CAP theorem and how do NoSQL databases choose?",
        "answer": "CAP: Consistency, Availability, Partition tolerance - choose two. NoSQL databases often choose AP (Cassandra, DynamoDB) - available during partitions but eventually consistent. Some choose CP (HBase, MongoDB with majority writes) - consistent but may be unavailable during partitions. Trade-off choice depends on application needs: banking needs CP, social media often AP."
    },
    {
        "id": 479,
        "question": "Explain the concept of key-value store with examples.",
        "answer": "Key-value stores are simplest NoSQL: data stored as key-value pairs, like hash table. Keys unique, values can be strings, JSON, binary. Examples: Redis (in-memory with data structures), DynamoDB (managed, scalable), Riak. Extremely fast for lookups by key. Use cases: caching, session storage, user profiles, shopping carts. Limited query capability - no complex joins or filters."
    },
    {
        "id": 480,
        "question": "What is the difference between MongoDB and Cassandra?",
        "answer": "MongoDB: document store, rich query language, secondary indexes, aggregation, master-slave replication (primary replica set), strong consistency options. Better for complex queries, flexible schemas. Cassandra: column-family store, CQL (SQL-like), masterless (all nodes equal), tunable consistency, optimized for write throughput. Better for time-series, high-volume writes, linear scalability."
    },
    {
        "id": 481,
        "question": "Explain column-family stores with HBase example.",
        "answer": "Column-family stores (HBase, Bigtable) store data in column families containing multiple columns. Tables are sparse - rows can have any columns. HBase: data sorted by row key, column families stored together on disk. Cells versioned by timestamp. Good for time-series data (multiple versions), wide tables with many attributes, sparse data. Used in Hadoop ecosystem for analytics."
    },
    {
        "id": 482,
        "question": "What is DynamoDB and its key features?",
        "answer": "DynamoDB is AWS's managed NoSQL key-value/document database. Features: fully managed, automatic scaling, single-digit millisecond performance, ACID transactions (limited), DAX (in-memory cache), streams for change capture, global tables (multi-region replication), on-demand/ provisioned capacity. Data model: tables with items (rows) and attributes. Supports both key-value and document patterns."
    },
    {
        "id": 483,
        "question": "Explain the concept of materialized views in Cassandra.",
        "answer": "Materialized views in Cassandra automatically maintain denormalized views of base table data for different query patterns. When base table updated, view updates automatically. Example: base table by user_id, materialized view by email for lookups. Trade-off: write amplification (each write updates base + views) but faster reads. Alternative to manual denormalization."
    },
    {
        "id": 484,
        "question": "What is the difference between RDBMS and NoSQL consistency models?",
        "answer": "RDBMS typically use ACID with strong consistency (serializable, repeatable read). All nodes see same data immediately after commit. NoSQL often use eventual consistency with BASE - data may be temporarily inconsistent but converges. Some NoSQL offer tunable consistency (Cassandra) or strong consistency options (MongoDB majority writes). Trade-off between consistency and performance."
    },
    {
        "id": 485,
        "question": "Explain graph database concepts with Neo4j examples.",
        "answer": "Graph databases store: Nodes (entities: Person, Company), Relationships (edges: WORKS_FOR, FRIEND_WITH), Properties (attributes: name, since). Neo4j Cypher: MATCH (p:Person)-[:WORKS_FOR]->(c:Company {name: 'Google'}) RETURN p. Optimized for relationship traversal, path finding, pattern matching. Use cases: social networks, fraud detection (find circular patterns), recommendation engines, network analysis."
    },
    {
        "id": 486,
        "question": "What is the difference between embedded and referenced documents in MongoDB?",
        "answer": "Embedded documents store related data inside parent document (denormalization). Good for one-to-one or one-to-few relationships, improves read performance (single query). Referenced documents store IDs linking to other documents (normalization). Better for many-to-many, large documents, or when data shared across parents. Trade-off: embedding faster reads but data duplication; references avoid duplication but need joins ($lookup)."
    },
    {
        "id": 487,
        "question": "Explain the concept of consistency levels in Cassandra.",
        "answer": "Cassandra consistency levels per operation: ONE (1 replica responds), QUORUM (majority of replicas), ALL (all replicas), LOCAL_QUORUM (majority in local DC), EACH_QUORUM (majority in each DC), ANY (at least one, may be hinted handoff). Write/read consistency can differ. Choose based on trade-off: higher consistency = slower but accurate; lower = faster but possibly stale."
    },
    {
        "id": 488,
        "question": "What is the difference between OLTP and NoSQL for web applications?",
        "answer": "Traditional OLTP (relational) handles structured transactions with ACID guarantees, complex joins, normalized data. NoSQL handles semi-structured data, horizontal scaling, high velocity, simpler queries. Web apps often combine: relational for orders/payments (need ACID), document for product catalogs (flexible schema), key-value for sessions (fast access), graph for recommendations (relationships). Polyglot persistence."
    },
    {
        "id": 489,
        "question": "Explain MongoDB aggregation pipeline.",
        "answer": "Aggregation pipeline processes documents through stages: $match (filter), $group (group by), $project (reshape), $sort (order), $lookup (join with another collection), $unwind (flatten arrays), $bucket (histograms)."
    },
    {
        "id": 490,
        "question": "What is the difference between CQL and SQL?",
        "answer": "Cassandra Query Language (CQL) resembles SQL but with differences: no joins (denormalization instead), no GROUP BY (use aggregates), no subqueries, WHERE clause only on partition keys or indexed columns, limited aggregation. Tables designed around query patterns, not normalization. CQL supports primary key, clustering columns, collections (sets, lists, maps), and counters."
    },
    {
        "id": 491,
        "question": "Explain the concept of partition key in Cassandra.",
        "answer": "Partition key determines which node stores a row. All rows with same partition key stored together, ensuring fast access. Clustering columns sort within partition. Choose partition key to distribute data evenly (avoid hotspots) and match query patterns. Example: PRIMARY KEY ((user_id), timestamp) - user_id partitions, timestamp clusters. Queries must include partition key for efficiency."
    },
    {
        "id": 492,
        "question": "What is the difference between Redis and Memcached?",
        "answer": "Both in-memory key-value caches. Redis advantages: rich data structures (lists, sets, hashes), persistence options (RDB snapshots, AOF logs), replication, Lua scripting, transactions, Pub/Sub. Memcached simpler: only strings, multi-threaded (better for many cores), no persistence, simpler eviction. Redis for caching + features; Memcached for pure caching at massive scale."
    },
    {
        "id": 493,
        "question": "Explain the concept of secondary indexes in MongoDB.",
        "answer": "MongoDB secondary indexes improve query performance on non-_id fields. Types: single field, compound, multikey (array fields), text, geospatial, hashed, wildcard. Indexes use B-tree structure. Can be unique, sparse, partial. Create: db.collection.createIndex({field: 1}) (ascending) or -1 (descending). Choose indexes based on query patterns; each index adds write overhead."
    },
    {
        "id": 494,
        "question": "What is the difference between relational model and document model?",
        "answer": "Relational: data normalized across tables, fixed schema, ACID transactions, joins for relationships, SQL. Document: denormalized nested documents, flexible schema, BASE, embedded relationships, query API. Example: Order in relational splits to Orders, OrderItems, Products. In document, order document embeds items with product details. Document model simpler for hierarchical data but may duplicate data."
    },
    {
        "id": 495,
        "question": "Explain the concept of change streams in MongoDB.",
        "answer": "Change streams allow applications to subscribe to real-time data changes on collections, databases, or entire deployments. Returns ordered stream of change events (insert, update, replace, delete). Useful for: real-time analytics, cache invalidation, cross-system synchronization, event-driven architectures. Based on oplog, requires replica set. Provides resume tokens for fault tolerance."
    }
]